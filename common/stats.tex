%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%======================================
\chapter{Statistics}
\label{chp:stats}
%======================================
%=======================================
\section{Expectation operator}
\index{expectation operator}
%=======================================
In a \structe{probability space} $\ps$, all probability information
is contained in the \fncte{measure} $\psp$ (or equivalently in the pdf or cdf
defined in terms of $\psp$).
Often times this information is overwhelming and a simpler statistic,
which does not offer so much information, is sufficient.
Some of the most common statistics can be conveniently expressed in terms
of the \hie{expectation operator} $\pE$.
%---------------------------------------
\begin{definition}
\label{def:pE}
%---------------------------------------
Let $\ps$ be a \structe{probability space} and
$\rvX$ a \fncte{random variable} on $\ps$ with
\fncte{probability density function} $\ppx$.
\defboxt{ 
  The \opd{expectation operator} $\pEx$ on $\rvX$ is defined as
  \\\indentx$\ds\pEx\rvX \eqd \int_{x\in\F} x \ppx(x) \dx$. 
}
\end{definition}

We already said that a \fncte{random variable} $\rvX$ is neither random nor a variable,
but is rather a function of an underlying process that does appear to be random.
However, because it is a function of a process that does appear random,
the \fncte{random variable} $\rvX$ also appears to be random.
That is, if we don't know the outcome of of the underlying experimental
process, then we also don't know for sure what $\rvX$ is, and so $\rvX$ does
indeed appear to be random.
However, eventhough $\rvX$ appears to be random,
the expected value $\pEx\rvX$  of $\rvX$ is {\bf not random}.
Rather it is a fixed value (like $0$ or $7.9$ or $-2.6$).

On the other hand, eventhough $\pE\rvX$ is {\bf not random},
note that $\pE(X|Y)$ {\bf is random}.
This is because $\pE(X|Y)$ is a function of $\rvY$.
That is, once we know that $\rvY$ equals some fixed value $y$
(like $0$ or $2.7$ or $-5.1$) then $\pE(X|Y=y)$ is also fixed.
However, if we don't know the value of $\rvY$,
then $\rvY$ is still a \fncte{random variable} and the expression $\pE(X|Y)$
is also random (a function of \fncte{random variable} $\rvY$).

Two common statistics that are conveniently expressed in terms of the
expectation operator are the \hie{mean} and \hie{variance}.
The mean is an indicator of the ``middle" of a probability distribution and the
variance is an indicator of the ``spread".
%---------------------------------------
\begin{definition}
\label{def:Mx}
\index{mean}
\index{variance}
%---------------------------------------
Let $\ps$ be a \structe{probability space} and $\rvX:\pso\to\R$ a \fncte{random variable}.
The {\bf mean} $\pmeanx$ and {\bf variance} $\pVar(X)$ of $\rvX$ are
\defbox{\begin{array}{rcl}
  \pmeanx  &\eqd& \pEx\rvX \\
  \pVar(X) &\eqd& \pEx\left[(X-\pEx\rvX)^2 \right]
\end{array}}
\end{definition}

The next theorem gives some useful relations for simple statistics.
%---------------------------------------
\begin{theorem}
\label{thm:pE}
%---------------------------------------
Let $\ps$ be a \structe{probability space} with \fncte{random variable} $\rvX:\pso\to\R$
and let $a\in\R$.
\thmbox{\begin{array}{>{\ds}rc>{\ds}lM}
  \pEx(aX+b)  &=& \brs{a\pEx\rvX} + b    & (\prope{linear}) \\
  \pVar(aX)   &=& a^2\pVar(X) \\
  \pVar(X)    &=& \pEx\rvX^2 - (\pEx\rvX)^2
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \pEx(aX)
    &\eqd \int_{x\in\R}\brs{ax+b} \ppx(x)  \dx
  \\&=    a \int_{x\in\R}x \ppx(x)  \dx + b\int_{x\in\R}\ppx(x)  \dx
  \\&= \brs{a\pEx\rvX} + b\brs{1}
  \\&= \brs{a\pEx\rvX} + b
\\
\\
  \pVar(X)
    &\eqd \pEx\left[(X-\pEx\rvX)^2\right]
  \\&=    \pEx\left[\rvX^2-2X\pEx\rvX + (\pEx\rvX)^2 \right]
  \\&=    \pEx[\rvX^2]  - \pEx 2X\pEx\rvX  + \pEx (\pEx\rvX)^2
  \\&=    \pEx[\rvX^2] - 2(\pEx\rvX)[\pEx\rvX] + (\pEx\rvX)^2
  \\&=    \pEx\rvX^2  - (\pEx\rvX)^2
\\
\\
  \pVar(aX)
    &=    \pEx(aX)^2  - [\pEx(aX)]^2
  \\&=    \pEx(a^2X^2)  - [a\pEx\rvX]^2
  \\&=    a^2 \pEx\rvX^2  - a^2[\pEx\rvX]^2
  \\&=    a^2\left[ \pEx\rvX^2  - (\pEx\rvX)^2 \right]
  \\&\eqd a^2 \pVar(X)
\end{align*}
\end{proof}

\begin{figure}[ht]
\setlength{\unitlength}{0.3mm}%
\begin{center}%
\begin{picture}(200,110)(-50,-10)%
  \color{axis}%
    \put(  0,  0){\line(1, 0){120}}%
    \put(  0,  0){\line(0, 1){100}}%
    \qbezier[20](0,60)(30,60)(60,60)%
    \qbezier[20](60,0)(60,30)(60,60)%
  \color{blue}%
    \qbezier(33,100)(85,0)(110,100)%
    \put( 60, 60){\circle*{5}}%
  \color{red}%
    \put( 20,100){\line(1,-1){80}}%
  \color{label}%
  \put(125,  0){\makebox(0,0)[l]{$x$}}%
  \put( -5, 60){\makebox(0,0)[r]{$\ff(\pE\rvX)$}}%
  \put( 60, -5){\makebox(0,0)[t]{$\pE\rvX$}}%
  \put(130,60){\vector(-1,0){32}}%
  \put(130,40){\vector(-1,0){47}}%
  \put(135,60){\makebox(0,0)[l]{$\ff(x)$ (convex function)}}%
  \put(135,40){\makebox(0,0)[l]{$mx+c$ (support line)}}%
\end{picture}
\end{center}
\caption{
  Jensen's inequality
  \label{fig:jensen}
  }
\end{figure}

\fncte{Jensen's inequality} is an extremely useful application of \prope{convex}ity \xref{def:convex} to the
\ope{expectation} operator.
Jensen's inequality is stated in \pref{thm:jensen} (next)
and illustrated in \prefpp{fig:jensen}.
%--------------------------------------
\begin{theorem}[\thmd{Jensen's inequality}]
\footnote{
  \citerp{cover}{25},
  \citerpp{jensen1906}{179}{180}
  }
\label{thm:jensen}
%--------------------------------------
Let $\ff$ be a \prope{convex} function and $\rvX$ be a \fncte{random variable}. Then
\thmbox{
  \ff \mbox{ is \prope{convex}} \implies \ff(\pE\rvX) \le \pE\ff(X).
  }
\end{theorem}
\begin{proof}
Let $mx+c$ be a ``support line" under $\ff(x)$ such that
\[
  \begin{array}{rcll}
    mx+c &<& \ff(x) & \mbox{for } x\ne \pE\rvX \\
    mx+c &=& \ff(x) & \mbox{for } x=\pE\rvX.
  \end{array}
\]
Then
\begin{align*}
  \ff(\pE\rvX)
    &=   m[\pE\rvX] + c
  \\&=   \pE[mX + c]
  \\&\le \pE\ff(X)
\end{align*}
\end{proof}

%=======================================
\section{Upper bounds on probability}
%=======================================
%---------------------------------------
\begin{theorem}[Markov's inequality]
\index{Markov's inequality}
\index{theorems!Markov's inequality}
\citetbl{
  \citerp{ross}{395}
  }
%---------------------------------------
Let $\rvX:\Omega\to[0,\infty)$ be a non-negative valued \fncte{random variable} and
$a\in(0,\infty)$. Then
\thmbox{ \psp\setn{\rvX\ge a} \le \frac{1}{a} \pE\rvX }
\end{theorem}
\begin{proof}
\begin{align*}
  I &\eqd \left\{ \begin{array}{l@{\hspace{4ex}\mbox{for}\hspace{4ex}}l}
    1 &\rvX\ge a \\
    0 &\rvX < a
    \end{array}\right.
\\
  aI &\le\rvX           \\
   I &\le \frac{1}{a}\rvX \\
   \pE I &\le \pE\left(\frac{1}{a}\rvX\right) \\
\\
   \psp\setn{\rvX\ge a}
     &= 1\cdot\psp\setn{\rvX\ge a} + 0\cdot\psp\setn{\rvX<a}
   \\&= \pE I
   \\&\le \pE\left(\frac{1}{a}\rvX \right)
   \\&=   \frac{1}{a}\pE\rvX
\end{align*}
\end{proof}


%---------------------------------------
\begin{theorem}[Chebyshev's inequality]
\index{Chebyshev's inequality}
\index{theorems!Chebyshev's inequality}
\citetbl{
  \citerp{ross}{396}
  }
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} with mean $\mu$ and variance $\sigma^2$.
\thmbox{ \psp\setn{\abs{\rvX-\mu}\ge a} \le \frac{\sigma^2}{a^2}}
\end{theorem}
\begin{proof}
\begin{align*}
  \psp\setn{\abs{\rvX-\mu} \ge a}
    &=   \psp\setn{ (\rvX-\mu)^2 \ge a^2}
  \\&\le \frac{1}{a^2} \pE(X-\mu)^2 
    && \text{by \thme{Markov's inequality}}
  \\&=   \frac{\sigma^2}{a^2}
\end{align*}
\end{proof}

%=======================================
\section{Functions of one random variable}
%=======================================
\begin{figure}\color{figcolor}
\setlength{\unitlength}{0.4mm}
\begin{center}
\begin{footnotesize}
\begin{picture}(250,150)(-100,-20)
  \put(-100,   0){\line(1,0){200}}
  \put(   0, -20){\line(0,1){120}}
  {\color{red}
    \qbezier(-100,100)(0,-100)(100,100)
    \put( 100, 105){\makebox(0,0)[b]{$y=\ff(x)$}}
    }
  \qbezier[8](-40,0)(-40,8)(-40,16)
  \qbezier[8](40,0)(40,8)(40,16)
  \qbezier[28](-80,0)(-80,32)(-80,64)
  \qbezier[28](80,0)(80,32)(80,64)
  \qbezier[64](-80,64)(0,64)(80,64)
  \qbezier[40](-40,16)(0,16)(40,16)
  \put(   0, 110){\makebox(0,0)[r]{$y$}}
  \put( 110,   0){\makebox(0,0)[r]{$x$}}
  \put(  -5,  64){\makebox(0,0)[r]{$y+h$}}
  \put(  -5,  16){\makebox(0,0)[r]{$y$}}
  \put( -40,  -5){\makebox(0,0)[t]{$x_1=\ffi{y}$}}
  \put(  40,  -5){\makebox(0,0)[t]{$x_2=\ffi{y}$}}
  \put( -80,  -5){\makebox(0,0)[rt]{$\ffi{y+h}$}}
  \put(  80,  -5){\makebox(0,0)[lt]{$\ffi{y+h}$}}
  \put(-100,  40){\makebox(0,0)[r]{$\left.\frac{\dy}{\dx}\right|_{x=x_1}$}}
  \put( 100,  40){\makebox(0,0)[l]{$\left.\frac{\dy}{\dx}\right|_{x=x_2}$}}
  \put(  95,  40){\vector(-1,0){35}}
  \put(-100,  20){\makebox(0,0)[r]{$x_1 + \frac{1}{\ffp{x_1}}h$}}
  \put( 100,  20){\makebox(0,0)[l]{$x_2 + \Delta y\left.\frac{1}{\dy/\dx}\right|_{x=x_2} = x_1 + \frac{1}{\ffp{x_2}}h$}}
  \put( -95,  15){\vector( 1,-1){15}}
  \put(  95,  15){\vector(-1,-1){15}}
  \put(  40,  16){\line(5,6){40}}   %straight line
  \put( -40,  16){\line(-5,6){40}}   %straight line
  %{\color{green}\qbezier(40,16)(60,40)(80,64)}     %straight line
\end{picture}
\end{footnotesize}
\end{center}
\caption{
  $\rvY\eqd\ff(\rvX)$
  \label{fig:Y=f(X)}
  }
\end{figure}

%---------------------------------------
\begin{theorem}
\label{thm:Y=f(X)}
\citetbl{
  \citerp{papoulis}{93}, 
  \citerp{proakis}{30}
  }
%---------------------------------------
Let $\rvY=\ff(X)$ and $\set{x_n}{n=1,2,\ldots,N}$ be the roots of the equation
$y=\ff(x)$.
\thmbox{
  \ppy(y) = \sum_{n=1}^{\xN} \frac{\ppx(x_n)}{|\ff'(x_n)|}
}
\end{theorem}
\begin{proof}
Let the range of $\rvX$ be partitioned into $\xN$ partitions
$\set{A_n}{x_n\in\setA_n, n=1,2,\ldots,\xN}$ and $h\to0$.
\begin{align*}
  \ppy(y)h
    &= \psp\setn{y \le\rvY < y+h}
  \\&= \psp\setn{y \le \ff(X) < y+h}
  \\&= \sum_{n=1}^{\xN} \pPa{y \le \ff(X) < y+h}{\rvX\in\setA_n}
  \\&= \sum_{n=1}^{\xN} \pPc{\ffi{y} \le\rvX < \ffi{y+h}}{\rvX\in\setA_n}\psp\setn{\rvX\in\setA_n}
  \\&= \left\{\begin{array}{l@{\hspace{4ex}\mbox{for}\hspace{4ex}}l}
         \sum_{n=1}^{\xN} \pPc{x_n     \le\rvX < x_n + \frac{1}{\ffp{x_n}}h}{\rvX\in\setA_n}\psp\setn{\rvX\in\setA_n} & \ffp{x_n}<0 \\
         \sum_{n=1}^{\xN} \pPc{x_n + \frac{1}{\ffp{x_n}}h \le\rvX < x_n}{\rvX\in\setA_n}\psp\setn{\rvX\in\setA_n}     & \ffp{x_n}\ge0
       \end{array}\right.
  \\&= \sum_{n=1}^{\xN} \pPa{x_n     \le\rvX < x_n + \frac{1}{|\ffp{x_n}|}h}{\rvX\in\setA_n}
  \\&= \sum_{n=1}^{\xN} \psp\setn{x_n     \le\rvX < x_n + \frac{1}{|\ffp{x_n}|}h}
  \\&= \sum_{n=1}^{\xN} h \frac{1}{|\ffp{x_n}|} \ppx(x_n)
  \\&= h \sum_{n=1}^{\xN} \frac{\ppx(x_n)}{|\ffp{x_n}|}
\end{align*}
\end{proof}

%---------------------------------------
\begin{proposition}
\citetbl{
  \citerp{papoulis}{95},
  \citerp{proakis}{29}
  }
%---------------------------------------
Let $a,b\in\R, a\ne 0$ and $\rvY=a\rvX+b$. Then
\propbox{
  \ppy(y) =
    \frac{1}{|a|} \ppx\left(\frac{y-b}{a}\right)
  }
\end{proposition}
\begin{proof}
\begin{align*}
  \ppy(y)h
    &=  \psp\setn{y\le\rvY < y+h}
  \\&=  \psp\setn{y\le a\rvX+b < y+h}
  \\&=  \psp\setn{y-b\le aX < y-b+h}
  \\&=  \left\{\begin{array}{ll}
          \psp\setn{\frac{y-b}{a}\le\rvX < \frac{y-b}{a}+\frac{1}{a}h} &: a>0 \\
          \psp\setn{\frac{y-b}{a}\ge\rvX > \frac{y-b}{a}+\frac{1}{a}h} &: a<0
        \end{array}\right.
  \\&=  \left\{\begin{array}{ll}
          \psp\setn{\frac{y-b}{a}\le\rvX < \frac{y-b}{a}+\frac{1}{|a|}h} &: a>0 \\
          \psp\setn{\frac{y-b}{a}-\frac{1}{|a|}h <\rvX \le \frac{y-b}{a}} &: a<0
        \end{array}\right.
  \\&=  \frac{1}{|a|}h \ppx\left(\frac{y-b}{a}\right)
\\\implies
\\
  \ppy(y)
    &=  \frac{1}{|a|} \ppx\left(\frac{y-b}{a}\right)
\end{align*}

The theorem can also be proved using \prefpp{thm:Y=f(X)}.
The only root of $y=ax+b$ is $x_1=\frac{y-b}{a}$.
\begin{align*}
  \ppy(y)
    &= \sum_{n=1}^{\xN} \frac{\ppx(x_n)}{|\ff'(x_n)|}
  \\&= \frac{\ppx(x_1)}{|\ff'(x_1)|}
  \\&= \frac{\ppx(x_1)}{|a|}
  \\&= \frac{1}{|a|}\ppx\left(\frac{y-b}{a}\right)
\end{align*}
\end{proof}

\begin{figure}\color{figcolor}
\setlength{\unitlength}{0.3mm}
\begin{center}
\begin{footnotesize}
\begin{picture}(350,200)(-100,-20)
  \put( -20,   0){\line(1,0){220}}
  \put(   0, -20){\line(0,1){120}}
  \put(   0, 110){\makebox(0,0)[r]{$y$}}
  \put( 210,   0){\makebox(0,0)[r]{$x$}}
  {\color{red}
%    \qbezier(10,140)(60,10)(160,10)
    \qbezier(20,150)(30,30)(150,20)
    \put( 35, 105){\makebox(0,0)[l]{$y=\frac{1}{x}$}}
    }
  \put(40,80){\line(1,-1){40}} %straight line
  \qbezier[28](0,80)(20,80)(40,80)
  \qbezier[50](40,0)(40,40)(40,80)
  \qbezier[40](0,40)(40,40)(80,40)
  \qbezier[20](80,0)(80,20)(80,40)
  \put(  -5,  80){\makebox(0,0)[r]{$y+h$}}
  \put(  -5,  40){\makebox(0,0)[r]{$y$}}
  \put(  40,  -5){\makebox(0,0)[t]{$\frac{1}{y+h}$}}
  \put(  80,  -5){\makebox(0,0)[t]{$\frac{1}{y}$}}
  \put( 100,  60){\makebox(0,0)[l]{$\frac{\Delta y}{\Delta x}=\left.\frac{\dy}{\dx}\right|_{x=\frac{1}{y}}=-y^2$}}
  \put(  95,  60){\vector(-1,0){35}}
  \put( 100,  40){\makebox(0,0)[l]{$\frac{1}{y}+\frac{\Delta x}{\Delta y}\Delta y = \frac{1}{y} - \frac{1}{y^2}h$}}
  \put(  95,  40){\vector(-3,-2){55}}
\end{picture}
\end{footnotesize}
\end{center}
\caption{
  $\rvY=\frac{1}{\rvX}$
  \label{fig:Y=1/X}
  }
\end{figure}
%---------------------------------------
\begin{proposition}
\citetbl{
  \citerp{papoulis}{94}
  }
%---------------------------------------
Let $\rvY=\frac{1}{\rvX}$. Then
\propbox{
  \ppy(y) = \left\{\begin{array}{lM}
     %                                             & for $y<0$ \\
     %0                                            & for $y=0$ \\
     \frac{1}{y^2} \ppx\left( \frac{1}{y} \right) & for $y>0$
  \end{array}\right.
  }
\end{proposition}
\begin{proof}
Let $h\to0$.
First we show a useful relation for $\frac{1}{y+h}$.
This relation is illustrated in \prefpp{fig:Y=1/X}.
\begin{align*}
  \frac{1}{y+h}
    &=    y_1 + \frac{1}{m} \Delta y
  \\&=    \frac{1}{y} + \left.\frac{1}{dy/dx}\right|_{x=1/y} h
  \\&=    \frac{1}{y} - \left.x^2\right|_{x=1/y} h
  \\&=    \frac{1}{y} - \frac{1}{y^2} h
\end{align*}

Now we prove the theorem using the above relation.
\begin{align*}
  \ppy(y)h
    &=    \psp\setn{y\le\rvY < y+h}
  \\&=    \psp\setn{y\le \frac{1}{\rvX} < y+h}
  \\&=    \psp\setn{\frac{1}{y}\ge\rvX > \frac{1}{y+h}}
  \\&=    \psp\setn{\frac{1}{y}\ge\rvX > \frac{1}{y}-\frac{1}{y^2}h}
  \\&=    \psp\setn{\frac{1}{y}-\frac{1}{y^2}h < \rvX \le \frac{1}{y} }
  \\&=    \frac{1}{y^2}h \ppx\left( \frac{1}{y} \right)
\\\implies
\\
  \ppy(y)
    &=    \frac{1}{y^2} \ppx\left( \frac{1}{y} \right)
\end{align*}

The theorem can also be proved using \prefpp{thm:Y=f(X)}.
\begin{align*}
  x_1 &=& \frac{1}{y} \\
  \ff'(x) &=& -\frac{1}{x^2}    \\
  \ppy(y)
    &= \sum_{n=1}^{\xN} \frac{\ppx(x_n)}{|\ff'(x_n)|}
  \\&= \frac{\ppx(x_1)}{|\ff'(x_1)|}
  \\&= \frac{\ppx(1/y)}{|\ff'(1/y)|}
  \\&= \frac{\ppx(1/y)}{y^2}
  \\&= \frac{1}{y^2} \ppx\left( \frac{1}{y} \right)
\end{align*}
\end{proof}





\begin{figure}\color{figcolor}
\setlength{\unitlength}{0.3mm}
\begin{center}
\begin{footnotesize}
\begin{picture}(250,150)(-100,-20)
  \put(-100,   0){\line(1,0){200}}
  \put(   0, -20){\line(0,1){120}}
  {\color{red}
    \qbezier(-100,100)(0,-100)(100,100)
    \put( 100, 105){\makebox(0,0)[b]{$y=x^2$}}
    }
  \qbezier[8](-40,0)(-40,8)(-40,16)
  \qbezier[8](40,0)(40,8)(40,16)
  \qbezier[28](-80,0)(-80,32)(-80,64)
  \qbezier[28](80,0)(80,32)(80,64)
  \qbezier[64](-80,64)(0,64)(80,64)
  \qbezier[40](-40,16)(0,16)(40,16)
  \put(   0, 110){\makebox(0,0)[r]{$y$}}
  \put( 110,   0){\makebox(0,0)[r]{$x$}}
  \put(  -5,  64){\makebox(0,0)[r]{$y+h$}}
  \put(  -5,  16){\makebox(0,0)[r]{$y$}}
  \put( -40,  -5){\makebox(0,0)[t]{$-\sqrt{y}$}}
  \put(  40,  -5){\makebox(0,0)[t]{$\sqrt{y}$}}
  \put( -80,  -5){\makebox(0,0)[t]{$-\sqrt{y+h}$}}
  \put(  80,  -5){\makebox(0,0)[t]{$\sqrt{y+h}$}}
% \put(  80,  -5){\makebox(0,0)[t]{$\sqrt{y+h}\approx \sqrt{y} + \frac{1}{2\sqrt{y}}h$}}
  \put( 100,  40){\makebox(0,0)[l]{$\frac{\Delta y}{\Delta x}=\left.\frac{\dy}{\dx}\right|_{x=\sqrt{y}}=2\sqrt{y}$}}
  \put(  95,  40){\vector(-1,0){35}}
  \put(-100,  20){\makebox(0,0)[r]{$\sqrt{y}+\frac{\Delta x}{\Delta y}\Delta y = \sqrt{y} - \frac{1}{2\sqrt{y}}h$}}
  \put( 100,  20){\makebox(0,0)[l]{$\sqrt{y}+\frac{\Delta x}{\Delta y}\Delta y = \sqrt{y} + \frac{1}{2\sqrt{y}}h$}}
  \put( -95,  15){\vector( 1,-1){15}}
  \put(  95,  15){\vector(-1,-1){15}}
  \put(  40,  16){\line(5,6){40}}   %straight line
  \put( -40,  16){\line(-5,6){40}}   %straight line
  %{\color{green}\qbezier(40,16)(60,40)(80,64)}     %straight line
\end{picture}
\end{footnotesize}
\end{center}
\caption{
  $\rvY=\rvX^2$
  \label{fig:YX2}
  }
\end{figure}
%---------------------------------------
\begin{proposition}
\citetbl{
  \citerp{papoulis}{95},
  \citerp{proakis}{29}
  }
\label{prop:YX2}
%---------------------------------------
Let $\rvY=\rvX^2$. Then
\propbox{
  \ppy(y) = \brbl{\begin{array}{lM}
     0                  & for $a<0$ \\
     \text{undefined}   & for $a=0$ \\
     \left.\left.\frac{1}{2\sqrt{y}} \right[\ppx(-\sqrt{y}) + \ppx( \sqrt{y}) \right]
                        & for $a>0$ 
  \end{array}}
  }
\end{proposition}
\begin{proof}
Let $h\to0$.
First we show a useful relation for $\sqrt{y+h}$.
This relation is illustrated in \prefpp{fig:YX2}.
\begin{align*}
  \sqrt{y+h}
    &=    y_1 + \frac{1}{m} \Delta y
  \\&=    \sqrt{y} + \left.\frac{1}{dy/dx}\right|_{x=\sqrt{y}} h
  \\&=    \sqrt{y} + \left.\frac{1}{2x}\right|_{x=\sqrt{y}} h
  \\&=    \sqrt{y} + \frac{1}{2\sqrt{y}} h
\end{align*}

Now we prove the theorem using the above relation.
\begin{align*}
  \ppy(y)h
    &= \psp\setn{y\le\rvY < y+h}
  \\&= \psp\setn{y\le\rvX^2 < y+h}
  \\&= \psp\setn{(y\le\rvX^2 < y+h) \land (X<0)}+ \psp\setn{(y\le\rvX^2 < y+h) \land (X\ge0)}
  \\&= \psp\setn{y\le\rvX^2 < y+h |\rvX<0}\psp\setn{\rvX<0} + \psp\setn{y\le\rvX^2 < y+h |\rvX\ge0}\psp\setn{\rvX\ge0}
  \\&= \psp\setn{-\sqrt{y}\le\rvX < -\sqrt{y+h} |\rvX<0  } \psp\setn{\rvX<0} +
       \psp\setn{+\sqrt{y}\le\rvX < +\sqrt{y+h} |\rvX\ge0} \psp\setn{\rvX\ge0}
  \\&= \psp\setn{-\sqrt{y}\le\rvX < -\left(\sqrt{y}+\frac{1}{2\sqrt{y}}h\right) \land\rvX<0   } +
       \psp\setn{+\sqrt{y}\le\rvX <        \sqrt{y}+\frac{1}{2\sqrt{y}}h        \land\rvX\ge0 }
  \\&= \psp\setn{-\sqrt{y}\le\rvX < -\left(\sqrt{y}+\frac{1}{2\sqrt{y}}h\right)  } +
       \psp\setn{+\sqrt{y}\le\rvX <        \sqrt{y}+\frac{1}{2\sqrt{y}}h         }
  \\&= \frac{1}{2\sqrt{y}}h\ppx(-\sqrt{y})  +
       \frac{1}{2\sqrt{y}}h\ppx( \sqrt{y})
\\\implies
  \ppy(y)
    &=  \left.\left.\frac{1}{2\sqrt{y}}\right[
        \ppx(-\sqrt{y}) + \ppx( \sqrt{y}) \right]
\end{align*}

The theorem can also be proved using \prefpp{thm:Y=f(X)}.
\begin{align*}
  x_1 &= -\sqrt{y} \\
  x_2 &= +\sqrt{y} \\
  \ff'(x) &= 2x    \\
  \ppy(y)
    &= \sum_{n=1}^{\xN} \frac{\ppx(x_n)}{|\ff'(x_n)|}
  \\&= \frac{\ppx(x_1)}{|\ff'(x_1)|} + \frac{\ppx(x_2)}{|\ff'(x_2)|}
  \\&= \frac{\ppx(-\sqrt{y})}{|\ff'(-\sqrt{y})|} + \frac{\ppx(\sqrt{y})}{|\ff'(\sqrt{y})|}
  \\&= \frac{\ppx(-\sqrt{y})}{2\sqrt{y}} + \frac{\ppx(\sqrt{y})}{2\sqrt{y}}
  \\&=    \left.\left.\frac{1}{2\sqrt{y}}\right[
          \ppx(-\sqrt{y}) + \ppx( \sqrt{y}) \right]
\end{align*}
\end{proof}




\begin{figure}\color{figcolor}
\setlength{\unitlength}{0.15mm}
\begin{center}
\begin{footnotesize}
\begin{picture}(1000,220)(-500,-100)
  \put(-500,   0){\line(1,0){1000}}
  \put(   0, -100){\line(0,1){220}}
  \multiput(-400,0)(200,0){5}{
    {\color{red}
      \qbezier(0,0)(25,25)(50,37)
      \qbezier(50,37)(75,50)(95,100)
      \qbezier(0,0)(-25,-25)(-50,-37)
      \qbezier(-50,-37)(-75,-50)(-95,-100)
      }
    \qbezier[14](77,0)(77,32)(77,64)
    }
  \put( 100, 105){\makebox(0,0)[b]{$z=\atan\theta$}}
  \put(-400,-5){\makebox(0,0)[t]{$-2\pi$}}
  \put(-200,-5){\makebox(0,0)[t]{$- \pi$}}
  \put( 200,-5){\makebox(0,0)[t]{$  \pi$}}
  \put( 400,-5){\makebox(0,0)[t]{$ 2\pi$}}

  \put(-323,-5){\makebox(0,0)[t]{$\theta_{-2}$}}
  \put(-123,-5){\makebox(0,0)[t]{$\theta_{-1}$}}
  \put(  77,-5){\makebox(0,0)[t]{$\theta_{ 0}$}}
  \put( 277,-5){\makebox(0,0)[t]{$\theta_{ 1}$}}
  \put( 477,-5){\makebox(0,0)[t]{$\theta_{ 2}$}}

  \qbezier[130](-315,64)(77,64)(477,64)
  \put(   0, 110){\makebox(0,0)[r]{$z$}}
  \put( 520,   0){\makebox(0,0)[l]{$\theta$}}
\end{picture}
\end{footnotesize}
\end{center}
\caption{
  $\rvZ=\tan\Theta$
  \label{fig:Z=tan0}
  }
\end{figure}
%---------------------------------------
\begin{proposition}
\citetbl{
  \citerpp{papoulis}{99}{100}
  }
%---------------------------------------
Let $\rvZ=\tan\Theta$. Then
\propbox{
  \ppz(z) = \frac{1}{1+z^2}  \sum_{n\in\Z} \ppth(\atan(z)+n\pi)
  }
\end{proposition}
\begin{proof}
Let $z=\frac{y}{x}$ and $x^2 + y^2 = r^2$.
\begin{align*}
  \cos^2\atan z
    &=& \cos^2\theta
     =  \frac{x^2}{r^2}
     =  \frac{x^2}{x^2+y^2}
     =  \frac{\frac{x^2}{x^2}}{\frac{x^2}{x^2}+\frac{y^2}{x^2}}
     =  \frac{1}{1+z^2}
\end{align*}
Let $h\to0$.
\begin{align*}
  \atan{z+h}
    &=    y_1 + \frac{1}{m} \Delta y
  \\&=    \atan{z} + \left.\frac{1}{\dz/\dth}\right|_{\theta=\atan{z}} h
  \\&=    \atan{z} + \left.\frac{1}{\sec^2\theta}\right|_{\theta=\atan{z}} h
  \\&=    \atan{z} + \left.\cos^2\theta\right|_{\theta=\atan{z}} h
  \\&=    \atan{z} + h\cos^2\atan{z}
  \\&=    \atan{z} + h\frac{1}{1+z^2}
\end{align*}

Now we prove the theorem using the above relation.
\begin{align*}
  \ppz(z)h
    &=    \psp\setn{z\le Z < z+h}
  \\&=    \psp\setn{z\le \tan\Theta < z+h}
  \\&=    \sum_{n\in\Z}\psp\setn{z\le \tan\Theta < z+h \land
                 \pi\left(n-\frac{1}{2}\right) \le \Theta < \pi\left(n+\frac{1}{2}\right)
                 }
  \\&=    \sum_{n\in\Z}\psp\setn{z\le \tan\Theta < z+h \left|
                 \pi\left(n-\frac{1}{2}\right) \le \Theta < \pi\left(n+\frac{1}{2}\right)
                 \right.}
                 \psp\setn{\pi\left(n-\frac{1}{2}\right) \le \Theta \pi\left(n+\frac{1}{2}\right)}
  \\&=    \sum_{n\in\Z}\psp\setn{\atan{z}+n\pi \le \Theta < \atan(z+h)+n\pi \left|
                 \pi\left(n-\frac{1}{2}\right) \le \Theta < \pi\left(n+\frac{1}{2}\right)
                 \right.}
                 \psp\setn{\pi\left(n-\frac{1}{2}\right) \le \Theta \pi\left(n+\frac{1}{2}\right)}
  \\&=    \sum_{n\in\Z}\psp\setn{\atan{z}+n\pi \le \Theta < \atan(z+h)+n\pi \land
                 \pi\left(n-\frac{1}{2}\right) \le \Theta < \pi\left(n+\frac{1}{2}\right)
                 }
  \\&=    \sum_{n\in\Z}\psp\setn{\atan{z}+n\pi \le \Theta < \atan(z+h)+n\pi }
  \\&=    \sum_{n\in\Z}\psp\setn{\atan{z}+n\pi \le \Theta < +\atan(z)+n\pi + h\frac{1}{1+z^2} }
  \\&=    h\frac{1}{1+z^2} \sum_{n\in\Z}\ppth(\atan{z}+n\pi)
\\\implies
  \ppz(z) &=  \frac{1}{1+z^2} \sum_{n\in\Z}\ppth(\atan{z}+n\pi)
\end{align*}

The theorem can also be proved using \prefpp{thm:Y=f(X)}.
\begin{align*}
  \theta_n &=& \atan{z}+n\pi \\
  \ff'(\theta) &=& \sec^2 \theta    \\
  \ppz(z)
    &= \sum_{n=1}^{\xN} \frac{\ppth(\theta_n)}{|\ff'(\theta_n)|}
  \\&= \sum_{n\in\Z}\frac{\ppth(\atan{z}+n\pi)}{|\ff'(\atan{z}+n\pi)|}
  \\&= \sum_{n\in\Z}\frac{\ppth(\atan{z}+n\pi)}{|\sec^2(\atan{z}+n\pi)|}
  \\&= \sum_{n\in\Z}\cos^2(\atan{z}+n\pi)  \ppth(\atan{z}+n\pi)
  \\&= \cos^2(\atan{z}) \sum_{n\in\Z} \ppth(\atan{z}+n\pi)
  \\&= \frac{1}{1+z^2}  \sum_{n\in\Z}\ppth(\atan{z}+n\pi)
\end{align*}
\end{proof}

%=======================================
\section{Joint and conditional probability spaces}
%=======================================
As described in \prefpp{def:ps},
every \structe{probability space} $\ps$ contains a probability \fncte{measure} $\psp:\pse\to[0,1]$.
This probability \fncte{measure} has some basic properties as described in
\pref{thm:P} (next).
%---------------------------------------
\begin{theorem}
\label{thm:P}
%---------------------------------------
Let $\ps$ be a \structe{probability space},
and $\set{B_n}{n=1,2,\ldots,N}$ be a partition of a set $\setB$.
\thmbox{\begin{array}{rc>{\ds}l@{\qquad}l}
  \psp(\setB)  &=&  \sum_{n=1}^{\xN} \psp(\setB_n)    & \forall \setB\in\pse\\
  \psp(\setA\setB) &=&  \sum_{n=1}^{\xN} \psp(\setA\setB_n)   & \forall \setA,\setB\in\pse
\end{array}}
\end{theorem}
\begin{proof}
This is because $\psp$ is a \fncte{measure} and by \prefpp{def:measure}.
\end{proof}



Sometimes the problem of finding the expected value of a \fncte{random variable} $\rvX$
can be simplified by ``conditioning $\rvX$ on $\rvY$".
%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s. Then
\thmbox{\pEx{\rvX} = \pEy\pEx[x|y](X|Y) }
\end{theorem}
\begin{proof}
\begin{align*}
   \pEy\pEx[x|y](\rvX|\rvY)
     &\eqd \pEy \left[ \int_{x\in\R}x \pp(X=x|Y) \dx \right]
   \\&\eqd \int_{y\in\R} \left[\int_{x\in\R}x \pp(x|Y=y) \dx \right] \pp(y) \dy
   \\&=    \int_{y\in\R} \int_{x\in\R}x \pp(x|y)\pp(y) \dx   \dy
   \\&=    \int_{x\in\R}x \int_{y\in\R} \pp(x,y) \dy   \dx
   \\&=    \int_{x\in\R}x \pp(x) \dx
   \\&\eqd \pEx\rvX
\end{align*}
\end{proof}

When possible, we like to generalize any given mathematical structure
to a more general mathematical structure and then take advantage of
the properties of that more general structure.
Such a generalization can be done with \fncte{random variable}s.
Random variables can be viewed as vectors in a vector space.
Furthermore, the expectation of the product of two \fncte{random variable}s
(e.g. $\pE(\rvX\rvY)$)
can be viewed as an innerproduct in an \structe{inner product space}.
Since we have an inner product space,
we can then immediately use all the properties of
\structe{inner product space}s, normed spaces, vector spaces, metric spaces,
and topological spaces.

%---------------------------------------
\begin{theorem}
\citetbl{
  \citerpp{moon2000}{105}{106}
  }
\label{thm:prb_vspace}
%---------------------------------------
Let $R$ be a ring,
$\ps$ be a \structe{probability space}, $\pE$ the expectation operator, and
$\spV=\set{\rvX}{\rvX:\pso\to R}$ be the set of all random vectors
in \structe{probability space} $\ps$.
\thmbox{\begin{array}{FlM}
  (1). & \spV\eqd\set{\rvX}{\rvX:\pso\to R}        & is a \structe{vector space}. \\
  (2). & \inprod{\rvX}{\rvY}\eqd\pE(\rvX\rvY^\ast) & is an \fncte{inner product}. \\
  (3). & \norm{\rvX}\eqd\sqrt{\pE(\rvX\rvX^\ast)}  & is a \fncte{norm}. \\
  (4). & \opair{\spV}{\inprodn}                    & is an \structe{inner product space}.
\end{array}}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Proof that $V$ is a vector space:
    \[\begin{array}{lll@{\hs{1cm}}D}
   1) & \forall \rvX,\rvY, Z\in V
      & (\rvX+\rvY)+ Z = \rvX+(\rvY+ Z)
      & \text{($+$ is associative)}
      \\
   2) & \forall \rvX,\rvY\in V
      & \rvX+\rvY = \rvY+\rvX
      & \text{($+$ is commutative)}
      \\
   3) & \exists  0 \in V \st \forall \rvX\in V
      & \rvX+ 0 = \rvX
      & \text{($+$ identity)}
      \\
   4) & \forall \rvX \in V \exists \rvY\in V \st
      & \rvX+\rvY =  0
      & \text{($+$ inverse)}
      \\
   5) & \forall \alpha\in S \text{ and } \rvX,\rvY\in V
      & \alpha\cdot(\rvX+\rvY) = (\alpha \cdot\rvX)+(\alpha\cdot\rvY)
      & \text{($\cdot$ distributes over $+$)}
      \\
   6) & \forall \alpha,\beta\in S \text{ and } \rvX\in V
      & (\alpha+\beta)\cdot\rvX = (\alpha\cdot \rvX)+(\beta\cdot \rvX)
      & \text{($\cdot$ pseudo-distributes over $+$)}
      \\
   7) & \forall \alpha,\beta\in S \text{ and } \rvX\in V
      & \alpha(\beta\cdot\rvX) = (\alpha\cdot\beta)\cdot\rvX
      & \text{($\cdot$ associates with $\cdot$)}
      \\
   8) & \forall \rvX\in V
      & 1\cdot \rvX = \rvX
      & \text{($\cdot$ identity)}
\end{array}\]

  \item Proof that $\inprod{\rvX}{\rvY}\eqd\pE(\rvX\rvY^\ast)$ is an inner product.
  \[\begin{array}{llllD}
   1) &  \pE(\rvX\rvX^\ast) &\ge 0
      &  \forall \rvX\in V
      &  \text{(non-negative)}
      \\
   2) &  \pE(\rvX\rvX^\ast) &= 0 \iff \rvX=0
      &  \forall \rvX\in V
      &  \text{(non-degenerate)}
      \\
   3) &  \pE(\alpha\rvX\rvY^\ast)    &= \alpha\pE(\rvX\rvY^\ast)
      &  \forall \rvX,\rvY\in V,\;\forall\alpha\in\C
      &  \text{(homogeneous)}
      \\
   4) &  \pE[(X+\rvY)Z^\ast] &= \pE(XZ^\ast) + \pE(YZ^\ast)
      &  \forall \rvX,\rvY, Z\in V
      &  \text{(additive)}
      \\
   5) &  \pE(\rvX\rvY^\ast) &= \pE(\rvY\rvX^\ast)
      &  \forall \rvX,\rvY\in V
      &  \text{(conjugate symmetric)}.
  \end{array}\]

  \item Proof that $\norm{\rvX}\eqd\sqrt{\pE(\rvX\rvX^\ast)}$ is a norm:
    This norm is simply induced by the above innerproduct.
  \item Proof that $\opair{\spV}{\inprodn}$ is an \structe{inner product space}:
    Because $V$ is a vector space and $\inprodn$ is
    an innerproduct, $\opair{\spV}{\inprodn}$ is an \structe{inner product space}.
\end{enumerate}
\end{proof}




The next theorem gives some results that follow directly from vector space
properties:
%---------------------------------------
\begin{theorem}
\index{Generalized triangle inequality}
\index{Cauchy-Schwartz inequality}
\index{Parallelogram Law}
%---------------------------------------
Let $\ps$ be a \structe{probability space} with expectation functional $\pE$.
\thmbox{\begin{array}{l >{\ds}r c >{\ds}l D}
  1. & \sqrt{\pE\left(\sum_{n=1}^{\xN}\rvX_n\right)}
     &\le& \sum_{n=1}^{\xN} \pE(X_nX_n^\ast)
     & (\thme{Generalized triangle inequality})
     \\
  2. & \left| \pE(\rvX\rvY^\ast)\right|^2
     &\le& \pE(\rvX\rvX^\ast)\:\pE(YY^\ast)
     & (\thme{Cauchy-Schwartz inequality})
     \\
  3. & 2\pE(\rvX\rvX^\ast) + 2\pE(YY^\ast)
     &=& \pE[(X+\rvY)(X+\rvY)^\ast] + \pE[(X-Y)(X-Y)^\ast]
     &   (\thme{Parallelogram Law})
  \end{array}}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item $\opair{\clFor}{\pE(\rvx,\rvy)}$ is an \structe{inner product space}. Proof: \prefpp{thm:prb_vspace}.

  \item Because it is an \structe{inner product space}, the other properties follow:
        \\\indentx\begin{tabular}{llll}
          1. & Generalized triangle inequality:
             & \pref{thm:norm_tri}
             & \prefpo{thm:norm_tri}
             \\
          2. & Cauchy-Schwartz inequality:
             & \pref{thm:cs}
             & \prefpo{thm:cs}
             \\
          3. & Parallelogram Law:
             & \pref{thm:parallelogram}
             & \prefpo{thm:parallelogram}
        \end{tabular}
\end{enumerate}
\end{proof}


%=======================================
%\section{Multiple \fncte{random variable}s}
%=======================================
%---------------------------------------
\begin{theorem}
\mbox{}\\
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s.
\thmbox{ 
  \brb{\begin{array}{FMD}
    (1).&$\rvX$ and $\rvY$ are \prope{independent} & and\\
    (2).&$\rvZ\eqd\rvX+\rvY$
  \end{array}}
  \implies
  \brb{\ppz(z) = \ppx(z)\conv\ppy(z)}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \ppz(z)
    &\eqd \lim_{h\to0}\frac{1}{h} \psp\setn{z \le Z < z+h }
  \\&=    \lim_{h\to0}\frac{1}{h} \psp\setn{z \le\rvX+\rvY < z+h }
  \\&=    \lim_{h\to0}\frac{1}{h} \int_{y\in\R} \psp\setn{(z \le\rvX+y < z+h) \land (y\le\rvY<y+h) } \dy
  \\&=    \lim_{h\to0}\frac{1}{h} \int_{y\in\R} \psp\setn{(z-y \le\rvX < z-y+h) | (y\le\rvY<y+h) }\psp\setn{y\le\rvY<y+h} \dy
  \\&=    \lim_{h\to0}\frac{1}{h} \int_{y\in\R} \psp\setn{(z-y \le\rvX < z-y+h) |\rvY=y }\psp\setn{y\le\rvY<y+h} \dy
  \\&\eqd \int_{y\in\R} \ppx(z-y) \ppy(y)  \dy
  \\&=    \ppx(z) \conv \ppy(z)
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}
\label{thm:x1x2->y1y2}
%---------------------------------------
Let
\begin{liste}
  \item $\rvX_1$ and $\rvX_2$ be \fncte{random variable}s with joint distribution
        $\ppx[\rvX_1,\rvX_2](x_1,x_2)$
  \item $\rvY_1=\ff_1(x_1,x_2)$ and $\rvY_2=\ff_2(x_1,x_2)$
\end{liste}
Then the joint distribution of $\rvY_1$ and $\rvY_2$ is
\thmbox{
  \ppx[\rvY_1,\rvY_2](y_1,y_2)
    = \frac{\ppx[\rvX_1,\rvX_2](x_1,x_2)}{|J(x_1,x_2)|}
    = \frac{\ppx[\rvX_1,\rvX_2](x_1,x_2)}{
        \left|\begin{array}{cc}
          \pderiv{\ff_1}{x_1} & \pderiv{\ff_1}{x_2}   \\
          \pderiv{\ff_2}{x_1} & \pderiv{\ff_2}{x_2}
        \end{array}\right|
        }
    = \frac{\ppx[\rvX_1,\rvX_2](x_1,x_2)}{
        \pderiv{\ff_1}{x_1}\pderiv{\ff_2}{x_2} -
        \pderiv{\ff_1}{x_2}\pderiv{\ff_2}{x_1}
        }
  }
\end{theorem}

%---------------------------------------
\begin{proposition}
\label{prop:XY->RT}
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s with joint distribution
$\ppxy(x,y)$ and
\[\R^2 \eqd\rvX^2 +\rvY^2 \hspace{10ex} \Theta \eqd \atan\frac{\rvY}{\rvX}. \]
Then
\propbox{
  \ppx[R,\Theta](r,\theta)
    =  r\;\ppxy(r\cos\theta,r\sin\theta)
  }
\end{proposition}
\begin{proof}
\begin{align*}
  \ppx[R,\Theta](r,\theta)
    &= \frac{\ppxy(x,y)}{|J(x,y)|}
     = \frac{\ppxy(x,y)}{
       \left|\begin{array}{cc}
         \pderiv{R}{x}      & \pderiv{R}{y}   \\
         \pderiv{\theta}{x} & \pderiv{\theta}{y}
       \end{array}\right|
       }
     = \frac{\ppxy(x,y)}{
       \left|\begin{array}{cc}
         \frac{ x}{\sqrt{x^2+y^2}}  & \frac{y}{\sqrt{x^2+y^2}}   \\
         \frac{-y}{x^2+y^2}         & \frac{x}{x^2+y^2}
       \end{array}\right|
       }
  \\&= \frac{\ppxy(x,y)}{
         \frac{x}{\sqrt{x^2+y^2}}\frac{x}{x^2+y^2}  -
         \frac{y}{\sqrt{x^2+y^2}}\frac{-y}{x^2+y^2}
       }
  \\&= \frac{\ppxy(x,y)}{
         \frac{x^2+y^2}{(x^2+y^2)^{3/2}}
       }
  \\&= \ppxy(x,y)\frac{(x^2+y^2)^{3/2}}{x^2+y^2}
  \\&= \ppxy(r\cos\theta,r\sin\theta)\frac{r^3}{r^2}
  \\&= r\;\ppxy(r\cos\theta,r\sin\theta)
\end{align*}
\end{proof}


%---------------------------------------
\begin{proposition}
\label{prop:XY->RT_n}
%---------------------------------------
Let $\rvX\sim\pN{0}{\sigma^2}$ and $\rvY\sim\pN{0}{\sigma^2}$ be
independent \fncte{random variable}s and
\[\R^2 \eqd\rvX^2 +\rvY^2 \hspace{10ex} \Theta \eqd \atan\frac{\rvY}{\rvX}. \]
Then
\propbox{\begin{array}{llrcl}
  1. & \mbox{$R$ and $\Theta$ are independent with joint distribution}
     & \ppx[R,\Theta](r,\theta) &=& \ppr(r)\ppth(\theta)
\\
  2. & \mbox{$R$ has Rayleigh distribution}
     & \ppr(r)  &=& \frac{r}{\sigma^2}\exp{\frac{r^2}{-2\sigma^2}}
\\
  3. & \mbox{$\Theta$ has uniform distribution}
     & \ppth(\theta) &=& \frac{1}{2\pi}
\end{array}}
\end{proposition}
\begin{proof}
\begin{align*}
  \ppx[R,\Theta](r,\theta)
    &= r\;\ppxy(r\cos\theta,r\sin\theta)
    && \text{by \prefp{prop:XY->RT}}
  \\&= r\;\ppx(r\cos\theta) \; \ppy(r\sin\theta)
    && \text{by independence hypothesis}
  \\&= r\;
       \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\frac{(r\cos\theta-0)^2}{-2\sigma^2}}
       \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\frac{(r\sin\theta-0)^2}{-2\sigma^2}}
  \\&= \frac{1}{2\pi\sigma^2}\;r\;
       \exp{\frac{r^2(\cos^2\theta + \sin^2\theta)}{-2\sigma^2}}
  \\&= \frac{1}{2\pi\sigma^2}\;r\;
       \exp{\frac{r^2}{-2\sigma^2}}
  \\&= \left[\frac{1}{2\pi}\right]
       \left[\frac{r}{\sigma^2}\exp{\frac{r^2}{-2\sigma^2}}\right]
\end{align*}
\end{proof}


%---------------------------------------
\begin{proposition}
%---------------------------------------
Let $\rvX\sim\pN{\pmeanx}{\pvarx}$ and $\rvY\sim\pN{\pmeany}{\pvary}$ be
jointly Gaussian \fncte{random variable}s and $\pvarxy=\cov{\rvX}{\rvY}$.
Then
\propbox{
  \psp\setn{\rvX>\rvY} = \pQ\brp{\frac{-\pmeanx + \pmeany}{\pvarx+\pvary-2\pvarxy}}
  }
\end{proposition}
\begin{proof}
Because $\rvX$ and $\rvY$ are \prope{jointly Gaussian},
their linear combination $\rvZ=\rvX-\rvY$ is also \prope{Gaussian}.
A Gaussian distribution is completely defined by its mean and variance.
So, to determine the distribution of $\rvZ$,
we just have to determine the mean and variance of $\rvZ$.
\begin{align*}
  \pE\rvZ
    &= \pE\rvX - \pE\rvY
  \\&= \pmeanx - \pmeany
\\
\\
  \var\rvZ
    &= \pE\rvZ^2 - (\pE\rvZ)^2
  \\&= \pE (\rvX-\rvY)^2 - (\pE\rvX - \pE\rvY)^2
  \\&= \pE (\rvX^2-2\rvX\rvY+\rvY^2) - [(\pE\rvX)^2 -2\pE\rvX \pE\rvY + (\pE\rvY)^2 ]
  \\&= [\pE\rvX^2- (\pE\rvX)^2]  + [Y^2- (\pE\rvY)^2] - 2[\pE\rvX\rvY - \pE\rvX \pE\rvY]
  \\&= \var\rvX + \var\rvY - 2\cov{\rvX}{\rvY}
  \\&\eqd \pvarx + \pvary -2\pvarxy
\\
\\
  \psp\setn{\rvX>\rvY}
    &= \psp\setn{\rvX-\rvY>0}
  \\&= \psp\setn{\rvZ>0}
  \\&= \brlr{\pQ\left(\frac{z-\pE\rvZ}{\var\rvZ} \right)}_{z=0}
  \\&= \pQ\brp{\frac{0-\pmeanx+\pmeany}{\pvarx+\pvary-2\pvarxy}}
\end{align*}
\end{proof}
