%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%=======================================
\chapter{Dimensionality Reduction}
\label{chp:dimreduct}
%=======================================
%=======================================
\section{Introduction}
%=======================================
A number of statistical estimation algorithms yield subspace structures with 
high dimensionality---one key example is \ope{Kernel Density Estimation} (\ope{KDE})\xref{chp:pdfest}.
Richard Bellman described this problem as the 
``curse of dimensionality".\footnote{
  \citerp{bellman1954}{206},
  \citerpc{bellman1961}{94}{\textsection ``5.16 The Curse of Dimensionality"},
  \citerp{bellman1971}{44},
  \citerppgc{bishop2006}{33}{38}{9780387310732}{\textsection1.4 ``The Curse of Dimensionality"},
  \citerpgc{gramacki2017}{3, 59}{9783319716886}{\textsection3.9 ``The Curse of Dimensionality"}
  }

Actually, a \fncte{time series} is a point in infinite dimensional space.\footnote{
  \citerpgc{wasserman2006}{1}{9780387306230}{\textsection ``1.1 What Is Nonparametric Inference?"}
  }
Statistical analysis is much to do with approximating this point in low dimensional space.

%---------------------------------------
\begin{example}
\footnote{
  \citerppgc{wasserman2013}{88}{89}{9780387217369}{``6.5 Example (Nonparametric estimation of functionals)."}
  }
%---------------------------------------
An example of a one-dimensional representation is the \fncte{estimated mean} or average of the time series.
\end{example}

%---------------------------------------
\begin{example}
\footnote{
  \citerpgc{wasserman2013}{88}{9780387217369}{``6.2 Example (Two-dimensional Parametric Estimation)"}
  }
%---------------------------------------
In the case where the data is (or assumed to be) IID Gaussian, the point can be represented in 
two dimensional space with the dimensions being mean and variance.
\end{example}

References discussing \ope{dimensionality reduction} include the following:
\\\begin{tabular}{ll}
    \citeP{cunningham2015}: & ``\emph{Linear Dimensionality Reduction:\ldots}" % Survey, Insights, and Generalizations}"
  \\\citeP{sorzano2014}:    & ``\emph{A survey of dimensionality reduction techniques}"
  \\\citer{lee2007}:        & ``\emph{Nonlinear Dimensionality Reduction}" 
\end{tabular}

%=======================================
\section{Principal Component Analysis}
%=======================================
A very common algorithm for dimensionality reduction is %the following:
%\begin{listi}
  \ope{Principal Component Analysis} (\ope{PCA}).\footnote{
          \citeP{pearson1901},
          \citeP{hotelling1933}, 
          \citeP{eckart1936},
          \citerppc{kendall1968}{10}{36}{``2. Component Analysis"},
          \citeP{jeffers1967},
          \citerc{jolliffe2013}{9781475719048}
          }
%\end{listi}

%---------------------------------------
\begin{example}
\footnote{
  \citerppgc{carmona2013}{171}{180}{9781461487883}{{\scshape\textsection ``3.5 Principal Component Analysis"}}
  }
\label{ex:pca}
%---------------------------------------
Data is often \prope{multi-variate} and represented in tabular form
with each column representing a variable and rows the samples.
Such a table is basically a matrix representing high dimensional space.
We can perform an \ope{eigen-decomposition} on this matrix.
Many eigenvalues may result. 
However, we can retain the largest eigenvalues (and their associated eigenvectors)
and set the remaining eignevalues to 0.
Estimation can then be performed using just the non-zero eigen-pairs (assumed to represent the true ``signal")
while ignoring the zeroed-out eigen-pairs (assumed to represent the ``noise").
This is in essence the technique of \ope{Principal Component Analysis}.
\end{example}

%---------------------------------------
\begin{remark}
%---------------------------------------
Despite the description of \ope{PCA} in \pref{ex:pca}, 
it is \emph{not} to say that small eigenvalues are never important. 
For several counter-examples demonstrating the importance of small eigenvalues, see \citeP{jolliffe1982}.
\end{remark}
