%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================

%======================================
\chapter{Estimation Overview}
\label{app:est}
%======================================
%======================================
\section{Estimation types}
%======================================
%--------------------------------------
\paragraph{Estimation types.}
%--------------------------------------
Let $\rvx(t;\theta)$ be a waveform with parameter $\theta$.
There are three basic types of estimation of $\rvx$:

\begin{enume}
   \item \ope{detection}:
      \begin{liste}
         \item The waveform $\rvx(t;\theta_n)$ is known except for the value of parameter $\theta_n$.
         \item The parameter $\theta_n$ is one of a finite set of values.
         \item Estimate $\theta_n$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{parametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t;\theta)$ is known except for the value of parameter $\theta$.
         \item The parameter $\theta$ is one of an infinite set of values.
         \item Estimate $\theta$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{nonparametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t)$ is unknown and assumed without any parameter $\theta$.
         \item Estimate $\rvx(t)$.
      \end{liste}
\end{enume}

%--------------------------------------
\paragraph{Estimation criterion.}
%--------------------------------------
Optimization requires a criterion against which the quality of an
estimate is measured.\footnote{\citergc{srv}{chapters 3, 5}{013125295X}.}
The most demanding and general criterion is the \prope{Bayesian} criterion.
The Bayesian criterion requires knowledge of the probability
distribution functions and the definition of a \fncte{cost function}.
Other criterion are special cases of the Bayesian criterion
such that the cost function is defined in a special way,
no cost function is defined, and/or the distribution is not known
\xref{fig:est-tech}.

%--------------------------------------
\paragraph{Estimation techniques.}
\label{ref:sec:parameter-est}
%--------------------------------------
Estimation techniques can be classified into
five groups \xref{fig:est-tech}:\footnote{%
  \citerpgc{nelles2001}{26}{3540673695}{``Fig 2.2 Overview of linear and nonlinear optimization techniques"},
  \citerpgc{nelles2001}{33}{3540673695}{``Fig 2.5 The Bayes method is the most general approach but\ldots"},
  \citerpgc{nelles2001}{63}{3540673695}{``Table 3.3 Relationship between linear recursive and nonlinear optimization techniques"},
  \citerpg{nelles2001}{66}{3540673695}
  }
\begin{enume}
   \item sequential decoding
   \item norm minimization
   \item gradient search
   \item inner product analysis
   \item direct search
\end{enume}

Sequential decoding is a non-linear estimation family.
Perhaps the most famous of these is the Veterbi algorithm which
uses a trellis to calculate the estimate.
The Verterbi algorithm has been shown to yield an optimal estimate
in the maximal likelihood (ML) sense.
Norm minimization and gradient search algorithms are all linear algorithms.
While this restriction to linear operations often simplifies calculations,
it often yields an estimate that is not optimal in the ML sense.

%=======================================
\section{Estimation criterion}
\label{sec:est_criterion}
%=======================================
\begin{figure}[h]
\centering%
\includegraphics{graphics/latestimation.pdf}
\caption{
   Estimation criterion
   \label{fig:est-criterion}
   }
\end{figure}

%--------------------------------------
\begin{definition}
\index{MAP}
\index{ML}
\index{maximum a-posteriori}
\index{maximum likelihood}
\label{def:MAP}
\label{def:ML}
\label{def:estB}
\label{def:estMS}
\label{def:estMM}
\label{def:estMAP}
\label{def:estML}
%--------------------------------------
Let\\
$\begin{array}{FlM}
    (A).& \rvx(t;\theta)            & be a random process with unknown parameter $\theta$
  \\(B).& \rvy(t)                   & an observed random process which is statistically dependent on $\rvx(t;\theta)$
  \\(C).& \fCost(\theta,\pdfp(x,y)) & be a cost function.
\end{array}$
\tbox{
\setlength{\unitlength}{0.15mm}
\begin{picture}(200,100)(-50,-50)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(- 50,  10){\makebox ( 50, 50)[b]{$x(t)$}            }
  \put(- 50,   0){\vector  (  1,  0)   {50}                }
  \put(   0, -50){\framebox(100,100)[c]{}                  }
  \put(   0, -20){\makebox (100, 50)[t]{stochastic}        }
  \put(   0, -20){\makebox (100, 50)[b]{operator}          }
  \put( 100,  10){\makebox ( 50, 50)[b]{$y(t)$}            }
  \put( 100,   0){\vector  (  1,  0)   {50}                }
\end{picture}
}
\\
Then the following \fnctd{estimate}s are defined as described here:
\defbox{\begin{array}{FMMlc>{\ds}l}
     (1).&\fnctd{Bayesian estimate}                         &                          & \estB   &\eqd& \argmin_{\theta} C(\theta,\pdfp(x,y))
   \\(2).&\fnctd{Mean square estimate}                      &(``\fnctd{MS  estimate}") & \estMS  &\eqd& \argmin_{\theta} \E\norm{C(\theta,\pdfp(x,y))}^2
   \\(3).&\fnctd{mini-max estimate}                         &(``\fnctd{MM  estimate}") & \estMM  &\eqd& \argmin_{\theta}\max_{\pdfp} C(\theta,\pdfp(x,y))
   \\(4).&\mc{2}{M}{\begin{tabular}[t]{@{}l}\fnctd{maximum a-posteriori probability estimate}\\
                                         (``\fnctd{MAP estimate}")
                    \end{tabular}}
         & \estMAP &\eqd& \argmax_{\theta} \psP\setn{x(t;\theta)|y(t)}
   \\(5).&\fnctd{maximum likelihood estimate}               &(``\fnctd{ML  estimate}") & \estML  &\eqd& \argmax_{\theta} \psP\setn{y(t)|x(t;\theta)}
\end{array}}
\end{definition}

%--------------------------------------
\begin{theorem}
\label{thm:map=ml}
%--------------------------------------
Let $\rvx(t;\theta)$ be a random process with unknown parameter $\theta$.
\thmbox{
  \brb{\text{$\psP\setn{\theta}=$\prope{constant}}}
  \quad\implies\quad
  \brb{\estMAP = \estML}
  }
\end{theorem}
\begin{proof}
\begin{align*}
   \estMAP
     &\eqd \argmax_{\theta} \psP\setn{s(t;\theta)|r(t)}
     &&    \text{by definition of $\estMAP$}
     &&    \text{\xref{def:estMAP}}
   \\&=    \argmax_{\theta} \frac{\psP\setn{s(t;\theta) \land \rvy(t)}}
                               {\psP\setn{r(t)}}
   \\&=    \argmax_{\theta} \frac{\psP\setn{r(t) | \rvx(t;\theta) }\psP\setn{s(t;\theta) }}
                               {\psP\setn{r(t)}}
   \\&=    \argmax_{\theta} \psP\setn{r(t) | \rvx(t;\theta) }\psP\setn{s(t;\theta) }
   \\&=    \argmax_{\theta} \psP\setn{r(t) | \rvx(t;\theta) }
   \\&\eqd \estML
     &&  \text{by definition of $\estML$}
     &&  \text{\xref{def:estML}}
\end{align*}
\end{proof}

%=======================================
\section{Measures of estimator quality}
\label{sec:quality}
%=======================================
%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{silverman1986}{35}{9780412246203}{\textsection ``1.3.2 Measures of discrepancy\ldots"},
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"},
  \citerp{bendat1966}{183}{\textsection ``5.3 Statistical Errors for Parameter Estimates"}
  }
\label{def:mse}
%---------------------------------------
\defbox{
  \begin{array}{M}
    The \fnctd{mean square error} $\mse(\estT)$ of an estimate $\estT$\\ 
    of a parameter $\theta$ is defined as
  \end{array}
  \qquad
  \begin{array}{rc>{\ds}l}
    \mse(\estT) &\eqd& \pE\brs{\brp{\estT-\theta}^2}
  \end{array}
  }
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"}
  }
\label{def:nre}
%---------------------------------------
\defbox{
  \begin{array}{M}
    The \fnctd{normalized rms error} $\nre(\estT)$\\ 
    of an estimate $\estT$\\ 
    of a parameter $\theta$ is defined as
  \end{array}
  \qquad
  \begin{array}{rc>{\ds}l}
    \nre(\estT) &\eqd& \frac{\sqrt{\mse(\estT)}}{\theta} 
                 \eqd \frac{\sqrt{\pE\brs{\brp{\estT-\theta}^2}}}{\theta}
  \end{array}
  }
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{silverman1986}{35}{9780412246203}{\textsection ``1.3.2 Measures of discrepancy\ldots"},
  \citePpc{rosenblatt1956}{835}{``integrated mean square error"}
  }
\label{def:mise}
%---------------------------------------
\defbox{
  \begin{array}{M}
    The \fnctd{mean integrated square error} $\mise(\estT)$\\
    of an estimate $\estT$ of a parameter $\theta$ is defined as
  \end{array}
  \qquad
  \begin{array}{rc>{\ds}l}
    \mise(\estT) &\eqd& \pE\int_{\theta\in\R}\brs{\brp{\estT-\theta}^2}
  \end{array}
  }
\end{definition}

The \fncte{mean square error} of $\estT$ can be expressed as the sum of two components:
the variance of $\estT$ and the bias of $\estT$ squared (next Theorem).
For an example of \pref{thm:mse} in action, see the proof for the $\mse(\meanest)$ of the 
\fncte{arithmetic mean estimate} as provided in \prefpp{thm:mse_mean}.
%---------------------------------------
\begin{theorem}
\footnote{
  \citerpgc{kay1988}{45}{8131733564}{\textsection\scshape``3.3 Estimation Theory"},
  \citerpgc{stuart1991}{629}{9780340560235}{``Minium mean-square-error estimation"},
  \citerpgc{clarkson1993}{51}{0849386098}{\textsection ``2.6 Estimation of Moments"},
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"},
  \citerp{bendat1966}{183}{\textsection ``5.3 Statistical Errors for Parameter Estimates"},
  \citerpgc{bendat1980}{39}{0471058874}{\textsection ``2.4.1 Bias versus Random Errors"}
  }
\label{thm:mse}
%---------------------------------------
Let $\mse(\estT)$ be the \fncte{mean square error} \xref{def:mse} 
and $\nre(\estT)$    the \fncte{normalized rms error} \xref{def:nre} of an estimator $\estT$.
\thmbox{\begin{array}{rc>{\ds}l | rc>{\ds}l}
  \mse(\estT) &=&
      \mcom{\pE\brs{\brp{\estT-\pE\estT}^2}}{variance of $\estT$}
    + \mcom{\brs{\pE\estT - \theta}^2}{bias of $\estT$ squared}
  &
  \nre(\estT) &=&
    \frac{\sqrt{
    \pE\brs{\brp{\estT-\pE\estT}^2} + \brs{\pE\estT - \theta}^2
    }}{\theta}
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \mse(\estT)
    &\eqd \pE\brs{\brp{\estT-\theta}^2}
    && \text{by definition of $\mse$}
    && \text{\xref{def:mse}}
  \\&= \mathrlap{\pE\brs{\brp{\estT\mcom{-\pE\estT+\pE\estT}{$0$}-\theta}^2}
     \qquad\text{by \prope{additive identity} property of $\fieldC$}}
  \\&= \pE\brs{
         \brp{\estT-\pE\estT}^2
        +\mcom{\brp{\pE\estT-\theta}^2}{constant}
        -2\brp{\estT-\pE\estT}\brp{\pE\estT-\theta}
       }
    && \text{by \thme{Binomial Theorem}}
    && \text{\ifxref{polynom}{thm:binomial}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2	
        -2\pE\brs{
         \estT\pE\estT
        -\estT\theta
        -\pE\estT\estT
        +\pE\estT\theta
        }
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
        -2\mcom{\brs{
         \pE\estT\pE\estT
        -\pE\estT\pE\theta
        -\pE\estT\pE\estT
        +\pE\estT\pE\theta
        }}{$0$}
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
\end{align*}
\end{proof}



%--------------------------------------
\section{Estimation techniques}
%--------------------------------------
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(1000,400)(-150,-300)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put( 325,  50){\framebox(100, 50)[c]{techniques}          }

  \put(  50,   0){\line    (  1,  0)   {750}                 }
 %\put( 375,   0){\circle*             { 10}                 }

  \put(-100,-225){\line    (  1,  0)   {300}                 }
  \put(  50,-225){\circle*             {  5}                 }
  \put(-100,-250){\line    (  0,  1)   { 25}                 }
  \put(  50,-250){\line    (  0,  1)   {150}                 }
  \put( 200,-250){\line    (  0,  1)   { 25}                 }

  \put(  50,- 50){\line    (  0,  1)   { 50}                 }
  \put( 200,- 50){\line    (  0,  1)   { 50}                 }
  \put( 350,- 50){\line    (  0,  1)   { 25}                 }
  \put( 500,- 50){\line    (  0,  1)   { 25}                 }
  \put( 425,- 25){\line    (  0,  1)   { 25}                 }
 %\put( 425,- 25){\circle*             { 10}                 }
  \put( 350,- 25){\line    (  1,  0)   {150}                 }
  \put( 650,- 50){\line    (  0,  1)   { 50}                 }
  \put( 800,- 50){\line    (  0,  1)   { 50}                 }
  \put( 375,   0){\line    (  0,  1)   { 50}                 }

  \put(  50,-150){\line    (  0,  1)   { 50}                 }
  \put( 200,-150){\line    (  0,  1)   { 50}                 }
  \put( 350,-150){\line    (  0,  1)   { 50}                 }
  \put( 500,-150){\line    (  0,  1)   { 50}                 }

  \put( 500,-250){\line    (  0,  1)   { 50}                 }

  \put(  60,- 45){\makebox (100, 40)[lt]{sequential}         }
  \put(  60,- 45){\makebox (100, 40)[lb]{decoding}           }
  \put( 210,- 45){\makebox (100, 35)[lt]{norm}       }
  \put( 210,- 45){\makebox (100, 35)[lb]{minimization}       }
  \put( 435,- 25){\makebox (100, 25)[l]{gradient search}     }
  \put( 360,- 50){\makebox (100, 25)[l]{use $\grad_\vp$ only}}
  \put( 510,- 50){\makebox (100, 25)[l]{$\grad_\vp,\grad^2_\vp$}}
  \put( 660,- 50){\makebox (100, 50)[l]{inner product}       }
  \put( 810,- 50){\makebox (100, 50)[l]{direct search}       }

  \put(   0,-100){\framebox(100, 50)[c]{}                    }
  \put(   0,- 90){\makebox (100, 30)[t]{code}                }
  \put(   0,- 90){\makebox (100, 30)[b]{tree}                }
  \put(   0,-300){\framebox(100, 50)[c]{}                    }

  \put(-150,-295){\makebox (100, 40)[t]{trellis}             }
  \put(-150,-295){\makebox (100, 40)[b]{(Viterbi)}           }
  \put(-150,-300){\framebox(100, 50)[c]{}                    }
  \put(   0,-295){\makebox (100, 40)[t]{Fano}                }
  \put(   0,-295){\makebox (100, 40)[b]{algorithm}           }
  \put(   0,-300){\framebox(100, 50)[c]{}                    }
  \put( 150,-295){\makebox (100, 40)[t]{Stack}               }
  \put( 150,-295){\makebox (100, 40)[b]{algorithm}           }
  \put( 150,-300){\framebox(100, 50)[c]{}                    }

  \put( 150,-100){\framebox(100, 50)[c]{}                    }
  \put( 150,- 90){\makebox (100, 30)[t]{min. mean}           }
  \put( 150,- 90){\makebox (100, 30)[b]{squares}             }
  \put( 150,-200){\framebox(100, 50)[c]{}                    }
  \put( 150,-190){\makebox (100, 30)[t]{least}               }
  \put( 150,-190){\makebox (100, 30)[b]{square}             }

  \put( 300,-100){\framebox(100, 50)[c]{}                    }
  \put( 300,- 90){\makebox (100, 30)[t]{steepest}            }
  \put( 300,- 90){\makebox (100, 30)[b]{descent}             }
  \put( 300,-200){\framebox(100, 50)[c]{}                    }
  \put( 300,-190){\makebox (100, 30)[t]{least mean}          }
  \put( 300,-190){\makebox (100, 30)[b]{square}             }

  \put( 450,-100){\framebox(100, 50)[c]{}                    }
  \put( 450,- 90){\makebox (100, 30)[t]{Newton}              }
  \put( 450,- 90){\makebox (100, 30)[b]{method}              }
  \put( 450,-200){\framebox(100, 50)[c]{Kalman}              }
  \put( 450,-190){\makebox (100, 30)[t]{}                    }
  \put( 450,-190){\makebox (100, 30)[b]{}                    }
  \put( 450,-295){\makebox (100, 40)[t]{recursive}           }
  \put( 450,-300){\framebox(100, 50)[c]{least}               }
  \put( 450,-295){\makebox (100, 40)[b]{square}             }

  \put( 600,-100){\framebox(100, 50)[c]{wavelets}            }
  \put( 600,- 90){\makebox (100, 30)[t]{}                    }
  \put( 600,- 90){\makebox (100, 30)[b]{}                    }

\end{picture}
\caption{
   Estimation techniques
   \label{fig:est-tech}
   }
\end{figure}

%\begin{figure}[ht]
%\center{\epsfig{file=estimate.eps, width=12cm, clip=}}
%%\center{\includegraphics[0,0][4in,4in]{df1.eps}}
%\caption{
%   Estimation techniques
%   \label{fig:estimate}
%   }
%\end{figure}

%--------------------------------------
\section{Sequential decoding}
%--------------------------------------
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(550,320)(-100,0)
  %\graphpaper[10](0,0)(300,300)
  \put( -10 , 300 ){\makebox(0,0)[r]{state $00$}}
  \put( -10 , 200 ){\makebox(0,0)[r]{state $01$}}
  \put( -10 , 100 ){\makebox(0,0)[r]{state $10$}}
  \put( -10 ,   0 ){\makebox(0,0)[r]{state $11$}}

  \thicklines
  \put(   0 ,   0 ){\circle*{10}}
  \put(   0 , 100 ){\circle*{10}}
  \put(   0 , 200 ){\circle*{10}}
  \put(   0 , 300 ){\circle*{10}}

\multiput(0,0)(100,0){5}{
  %\thicklines
  \linethickness{1mm}
  \put        (  0,300){\line( 1,-1){100}} % state0 path1
  \put        (  0,200){\line( 1,-1){100}} % state1 path1
  \put        (  0,100){\line( 1, 1){100}} % state2 path1
  \put        (  0,  0){\line( 1, 1){100}} % state3 path1

  %\thicklines
  \linethickness{0.1mm}
  %\put        (  0,300){\line( 1,-3){100}} % state0 path0
  %\put        (  0,200){\line( 1,-2){100}} % state1 path0
  %\put        (  0,100){\line( 1, 2){100}} % state2 path0
  %\put        (  0,  0){\line( 1, 3){100}} % state3 path0

  \qbezier[50](  0,300)(  0,300)(100,  0)  % state0 path0
  \qbezier[50](  0,200)(  0,200)(100,  0)  % state1 path0
  \qbezier[50](  0,100)(  0,100)(100,300)  % state2 path0
  \qbezier[50](  0,  0)(  0,  0)(100,300)  % state3 path0

  \put( 100 ,   0 ){\circle*{10}}
  \put( 100 , 100 ){\circle*{10}}
  \put( 100 , 200 ){\circle*{10}}
  \put( 100 , 300 ){\circle*{10}}
}
\end{picture}
\hspace{1cm}
\begin{tabular}{cl}
   $\cdots$ & $y_n=0$ \\
  ---     & $y_n=1$
\end{tabular}
\caption{
  Viterbi algorithm trellis
   \label{fig:est_trellis}
   }
\end{figure}

It has been shown that the Viterbi algorithm (trellis) produces
an optimal estimate in the maximal likelihood (ML) sense.
A Verterbi trellis is shown in \prefpp{fig:est_trellis}.




