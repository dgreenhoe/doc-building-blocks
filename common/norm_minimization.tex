%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%======================================
\chapter[Least Squares Algorithms]
        {Least Squares Algorithms when Model is Unknown}
%======================================
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(-100,  10){\makebox ( 50,  0)[b]{$x$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{operator} }
  \put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vtheta$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn$}           }

  \put( 100,  85){\makebox ( 50,  0)[b]{$y$}               }
  \put( 100,  75){\line    (  1,  0)   { 50}               }
  %\put( 100,- 65){\makebox ( 50,  0)[b]{$U\estn$}            }
  \put( 100,- 75){\line    (  1,  0)   { 50}               }
  \put( 150,  75){\line    (  0, -1)   { 50}               }
  \put( 150,- 75){\line    (  0,  1)   { 50}               }
  \put( 150,  25){\vector  (  1,  0)   { 50}               }
  \put( 150, -25){\vector  (  1,  0)   { 50}               }

  \put( 200,- 50){\framebox(100,100)[c]{cost}    }
  \put( 300,  10){\makebox (100,  0)[b]{$C(\estn,x,y)$}      }
  \put( 300,   0){\vector  (  1,  0)   {100}               }
  \put( 400,- 50){\framebox(100,100)[c]{$\grad_\vtheta$}        }
  \put( 510,  10){\makebox (100,  0)[lb]{$\grad_\vtheta C(\vtheta,x,y)$}      }
  \put( 500,   0){\vector  (  1,  0)   {100}               }

\end{picture}
\caption{
   Estimation using gradient of cost function
   \label{fig:est-grad}
   }
\end{figure}


Norm minimization techniques are useful when the system model is unknown.\footnote{\citerpgc{clarkson1993}{1}{0849386098}{chapter one Introduction}}
%However they are very powerful in that an optimum solution can be computed in one step without iteration or recursion.
In this section we present two types of norm minimization:\footnote{
   The Least Squares algorithm is nothing new to mathematics.
   It was first published by Legendre in 1805, but there is a credible claim by Gauss
   that he had it as far back as 1795.
   Gauss, by the way, was also the first to discover the FFT.
   References:
   \citePp{sorenson1970}{63},
   \citeP{plackett1972},
   \citeP{stigler1981},
   \citeP{dutka1995}
   }

\begin{enume}
  \item minimum mean square estimation (MMSE): \\
        The MMS estimate is a \prope{stochastic} estimate.
        To compute the MMS estimate, the we do not need to know
        the actual data values, but we must know certain system statistics
        which are the
        input data autocorrelation and input/output crosscorrelation.
        The cost function is the expected value of the norm squared error.
   \item least square estimation (LSE):\footnote{\citerp{scargle1979}{5}} \\
        The LS estimate is a \prope{deterministic} estimate.
        To compute the LS estimate, we must know the actual data values
        (although these may be ``noisy" measurements).
        The cost function is the norm squared error.
\end{enume}

Solutions to both are given in terms of two matrices:

\begin{tabular}{lll}
   $\setY$: Autocorrelation matrix \\
   $W$: Crosscorrelation matrix.
\end{tabular}

%--------------------------------------
\section{Minimum mean square estimation}
\label{sec:est_mms}
%--------------------------------------
%--------------------------------------
\begin{definition}
\label{def:est_matrices}
%--------------------------------------
Let the following vectors, matrices, and functions
be defined as follows:
\defbox{\begin{array}{lclM | lclM}
     \vx    &\in& \vCm         & \fncte{data vector}                      & U      &\in& \mCmn        & \ope{regression matrix}
   \\\vy    &\in& \vCn         & \fncte{processed data vector}            & R      &\in& \mCmm        & \ope{autocorrelation matrix}
   \\\vye   &\in& \vCn         & \fncte{processed data estimate vector}   & W      &\in& \vCm         & \fncte{cross-correlation vector}
   \\\ve    &\in& \vCn         & \fncte{error vector}                     & \fCost &:  & \vRm\to\Rnn  & \fncte{cost function}
   \\\vtheta    &\in& \vRm         & \fncte{parameter vector}                 &
\end{array}}
\end{definition}

%--------------------------------------
\begin{theorem}[\thmd{Minimum mean square estimation}]
\label{thm:est_mms}
%--------------------------------------
Let
\begin{array}[t]{|rcl|}
   \hline
     \vye(\vtheta)   &\eqd& \opU^H\vtheta
   \\\ve(\vtheta)    &\eqd& \vye-\vy
   \\\fCost(\vtheta) &\eqd& \pE\norm{\ve}^2 \eqd \pE\brs{\ve^H\ve}
   \\\estMS      &\eqd& \argmin_\vtheta \fCost(\vtheta)
   \\R           &\eqd& \pE\brs{\opU\opU^H}
   \\W           &\eqd& \pE\brs{\opU\vy}
   \\\hline
\end{array}
Then
\thmbox{\begin{array}{rcl}
     \estMS                          &=& (\Real{\setY})^{-1}(\Real{W})
   \\\fCost(\vtheta)                     &=& \vtheta^H R \vtheta - (W^H\vtheta)^\ast -W^H\vtheta + \pE{\vy^H\vy}
   \\\grad_\vtheta \fCost(\vtheta)           &=& 2\Real\brs{\setY}\vtheta - 2\Real{W}
   \\\fCost(\estMS)                  &=&
     \brbl{\begin{array}{>{\ds}lM}
         \mc{2}{>{\ds}l}{\pE{\vy^H\vy} + (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W})}
       \\\pE{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W}) & if $\opR$ is \propb{real-valued}
     \end{array}}
\end{array}}
\end{theorem}
\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{enumerate}
  \item Proof that cost $\fCost(\vtheta)=\vtheta^H R \vtheta - (W^H)^\ast\vtheta -W^H\vtheta + \pE{\vy^H\vy}$: \label{item:est_mms_cost}
    \begin{align*}
       \boxed{\fCost(\vtheta)}
         &\eqd \pE\norm{\ve}^2
         && \text{by definition of \fncte{cost function} $\fCost$}
       \\&\eqd \pE\brs{\ve^H\ve}
         && \text{by definition of \fncte{norm} $\normn$}
       \\&\eqd \pE\brs{\brp{\vye-\vy}^H\brp{\vye-\vy}}
         && \text{by definition of \fncte{error vector} $\ve$}
       \\&\eqd \pE\brs{\brp{\opU^H\vtheta-\vy}^H\brp{\opU^H\vtheta-\vy}}
         && \text{by definition of estimate $\vye$}
       \\&=\mathrlap{\pE\brs{\brp{\vtheta^H\opU-\vy^H}\brp{\opU^H\vtheta-\vy}}
            \text{by \prope{distributive} prop. of \structe{*-algebra}s
                  \xref{def:staralg}}}
       \\&=    \pE\brs{ \vtheta^H\opU\opU^H\vtheta - \vtheta^H\opU\vy -\vy^H\opU^H\vtheta + \vy^H\vy }
         && \text{by \structe{matrix algebra} ring property}
       \\&=    \vtheta^H\pE\brs{\opU\opU^H}\vtheta - \vtheta^H\pE\brs{\opU\vy} -\pE\brs{\vy^H\opU^H}\vtheta + \pE{\vy^H\vy}
         && \text{by \prope{linearity} $\pE$
                  \xref{thm:pE_linop}}
       \\&=    \vtheta^H\pE\brs{\opU\opU^H}\vtheta - (\pE\brs{\opU\vy}^H\vtheta)^H -\pE\brs{\opU\vy}^H\vtheta + \pE{\vy^H\vy}
       \\&\eqd \vtheta^H \opR \vtheta - (\opW^H\vtheta)^H -\opW^H\vtheta + \pE{\vy^H\vy}
         && \text{by definitions of $\opR$ and $\opW$}
       \\&=    \vtheta^H \opR \vtheta - (\opW^H\vtheta)^\ast -\opW^H\vtheta + \pE{\vy^H\vy}
         && \text{because $\opW^H\vtheta$ is a \prope{scalar}}
       \\&=    \boxed{\vtheta^H \opR \vtheta - (\opW^H)^\ast\vtheta -\opW^H\vtheta + \pE{\vy^H\vy}}
       \\&=    \vtheta^H \opR \vtheta - 2\Real\brs{\opW^H}\vtheta + \pE{\vy^H\vy}
    \end{align*}

  \item Proof that optimal $\vthetao=(\Real{\setY})^{-1}(\Real{W})$: \label{item:est_mms_vpo}
    \begin{align*}
      \grad_\vtheta \fCost(\vtheta)
        &= \grad_\vtheta \brs{\vtheta^H \opR \vtheta - (\opW^H)^\ast\vtheta -\opW^H\vtheta + \pE{\vy^H\vy} }
        && \text{by \pref{item:est_mms_cost}}
      \\&= \opR\vtheta + \opR^T\vtheta - \grad_{\vtheta}\brs{(\opW^H)^\ast\vtheta + \opW^H\vtheta} + 0
        && \text{by \thme{quadratic form} result \xref{thm:mc_xAx}}
      \\&= \opR\vtheta + \opR^T\vtheta - [(\opW^H)^\ast]^T - [\opW^H]^T
        && \text{by \thme{affine equations} result \xref{thm:mc_Ax}}
      \\&= \opR\vtheta + (R^H)^\ast\vtheta - \opW - \opW^\ast
        && \text{by definition of \ope{Hermitian Transpose} $^H$}
      \\&= \opR\vtheta + \opR^\ast\vtheta - \opW - \opW^\ast
        && \text{because $R$ is \prope{Hermitian symmetric}}
      \\&= (R + \opR^\ast)\vtheta - (\opW + \opW^\ast)
        && \text{by \prope{ring} property}
      \\&= 2(\Real{\setY})\vtheta - 2\Real{\opW}
        && \text{by definition of $\Real$ \xref{def:Re}}
      \\
      \implies\vthetao
        &= (\Real{\setY})^{-1}(\Real{\opW})
        && \text{by setting $\grad_\vtheta \fCost(\vtheta)=0$}
    \end{align*}

  \item Cost of optimal $\vthetao$:
    \begin{align*}
      \fCost(\vthetao)
        &=    \vthetao^H \opR \vthetao - 2\Real\brs{\opW^H}\vthetao + \pE{\vy^H\vy}
        &&    \text{by \pref{item:est_mms_cost}}
      \\&=    \brs{(\Real{\setY})^{-1}(\Real{\opW})}^H
              \opR\brs{(\Real{\setY})^{-1}(\Real{\opW})}
            - 2\Real\brs{\opW^H}\brs{(\Real{\setY})^{-1}(\Real{\opW})}
            + \pE{\vy^H\vy}
        &&    \text{by \pref{item:est_mms_vpo}}
      \\&=    (\Real{\opW^H})(\Real{\setY})^{-H} \opR (\Real{\setY})^{-1}(\Real{\opW}) - 2\Real\brs{\opW^H}(\Real{\setY})^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    (\Real{\opW^H})(\Real{\opR^H})^{-1} \opR (\Real{\setY})^{-1}(\Real{\opW}) - 2\Real\brs{\opW^H}(\Real{\setY})^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    (\Real{\opW^H})(\Real{\setY})^{-1} \opR (\Real{\setY})^{-1}(\Real{\opW}) - 2(\Real{\opW^H})(\Real{\setY})^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
        \\
        \\
      \fCost(\vthetao)|_{\opR\mbox{ real}}
        &=    (\Real{\opW^H})(\Real{\setY})^{-1} \opR (\Real{\setY})^{-1}(\Real{\opW}) - 2(\Real{\opW^H})(\Real{\setY})^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    (\Real{\opW^H})\opR^{-1} \opR \opR^{-1}(\Real{\opW}) - 2(\Real{\opW^H})\opR^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    (\Real{\opW^H}) \opR^{-1}(\Real{\opW}) - 2(\Real{\opW^H})\opR^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    \pE{\vy^H\vy} - (\Real{\opW^H})\opR^{-1}(\Real{\opW})
    \end{align*}
\end{enumerate}
\end{proof}

\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  \put(-100,  10){\makebox ( 50,  0)[b]{$\vx$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{process} }
  %\put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vtheta$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn=R^{-1}W$}           }

  \put( 110,  85){\makebox ( 50,  0)[lb]{$y$}               }
  \put( 110, -65){\makebox ( 50,  0)[lb]{$\hat{y}=\vx^T\vtheta$}               }
  \put( 100,  75){\vector  (  1,  0)   {150}               }
  \put( 100,- 75){\vector  (  1,  0)   {150}               }
  \put( 250,   0){\circle{40} }
  \put( 250,   0){\makebox(0,0)[c]{$+$} }
  \put( 250,  75){\vector  (  0, -1)   { 60}               }
  \put( 250, -75){\vector  (  0,  1)   { 60}               }

  \put( 300,  10){\makebox (100,  0)[b]{$e=\hat{y}-y$}      }
  \put( 270,   0){\vector  (  1,  0)   {80}               }
  \put( 350,   0){\vector  (  0, -1)   {225}               }
  \put( 350,-225){\vector  ( -1,  0)   {250}               }
\end{picture}
\caption{
   Adative filter example
   \label{fig:est_adapt}
   }
\end{figure}

In many adaptive filter and equalization applications,
the autocorrelation matrix $U$ is simply the $m$-element
random data vector $\vx(k)$ at time $k$, as in the \thme{Wiener-Hopf equations} (next).
%--------------------------------------
\begin{corollary}[\thmd{Wiener-Hopf equations}]
\footnote{
  \citerppgc{ifeachor1993}{547}{549}{020154413X}{\textsection ``9.3 Basic Wiener filter theory"},
  \citerppgc{ifeachor2002}{651}{654}{0201596199}{\textsection ``10.3 Basic Wiener filter theory"},
  \citerpgc{kay1988}{51}{9788131733561}{\textsection ``3.3.3 Random Parameters"}
  }
%--------------------------------------
\corbox{
  \brb{
   U \eqd \vx(k) \eqd
   \brs{\begin{array}{l}
      x(k) \\
      x(k-1) \\
      x(k-2) \\
      \vdots \\
      x(k-m+1)
   \end{array}}}
  \implies
  \brb{\begin{array}{rcl}
    \estMS                          &=& R^{-1}W  \\
    \fCost(\estMS)                  &=& W^T R^{-1} R R^{-1}W - 2W^T R^{-1}W + \pE{\vy^T\vy} \\
  \end{array}}
  }
\end{corollary}
\begin{proof}
This is a special case of the more general case discussed
in \prefpp{thm:est_mms}.
Here, the dimension of $U$ is $m\times1$ (n=1).
As a result,
$\vy$, $\vye$, and $\ve$ are simply scalar quantities (not vectors).
In this special case, we have the following results
\xref{fig:est_adapt}:
\\\indentx$\begin{array}{rcl@{\qquad\qquad}rcl}
      \hat{y}(\vtheta) &\eqd& \vx^T \vtheta                    & R                               &\eqd& \pE\brs{\vx\vx^T}                                    
    \\ e(\vtheta)      &\eqd& \hat{y}-y                        & W                               &\eqd& \pE\brs{ \vx y }                                     
    \\ \fCost(\vtheta) &\eqd& \E\norm{\ve}^2 \eqd \pE\brs{e^2} & \fCost(\vtheta)                 &=&    \vtheta^T R \vtheta -2W^T\vtheta  + \pE\brs{\vy^T\vy}
    \\ \estMS          &\eqd& \argmin_\vtheta \fCost(\vtheta)  & \grad_\vtheta \fCost(\vtheta)   &=&     2R\vtheta - 2W                                      
    \\                 &    &                                  & \fCost(\estMS)|_{R\mbox{ real}} &=&     \pE{\vy^T\vy} - W^T R^{-1}W                         
\end{array}$
\end{proof}

%======================================
\section{Least squares}
\label{sec:ls}
%======================================
%======================================
\subsection{General results}
%======================================
%--------------------------------------
\begin{theorem}[\thmd{Least squares}]
\label{thm:ls}
\index{least squares}
%--------------------------------------
Let
$\begin{array}[t]{rcl@{\qquad\qquad}rcl}
     \vye(\vtheta)   &\eqd& U^H\vtheta                  &  \estLS      &\eqd& \argmin_\vtheta \fCost(\vtheta) 
   \\\ve(\vtheta)    &\eqd& \vye-\vy                    &  R           &\eqd& UU^H                            
   \\\fCost(\vtheta) &\eqd& \norm{\ve}^2 \eqd \ve^H\ve  &  W           &\eqd& U\vy.                           
\end{array}$
\indentx
Then
\thmbox{\begin{array}{rcl}
   \estLS                          &=& (\Real{\setY})^{-1}(\Real{W})  \\
   \fCost(\vtheta)                     &=& \vtheta^H R \vtheta - (W^H\vtheta)^\ast -W^H\vtheta + \pE{\vy^H\vy} \\
   \grad_\vtheta \fCost(\vtheta)           &=& 2\Real\brs{\setY}\vtheta - 2\Real{W}  \\
   \fCost(\estLS)                  &=& (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \pE{\vy^H\vy} \\
   \fCost(\estLS)|_{R\mbox{ real}} &=& \pE{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W}).
\end{array}}
\end{theorem}

\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{align*}
   \fCost(\vtheta)
     &\eqd \norm{\ve}^2
   \\&=    e^H\ve
   \\&=    \left(\vye-\vy\right)^H\left(\vye-\vy\right)
   \\&=    \left(U^H\vtheta-\vy\right)^H\left(U^H\vtheta-\vy\right)
   \\&=    \left(\vtheta^HU-\vy^H\right)\left(U^H\vtheta-\vy\right)
   \\&=    \vtheta^HUU^H\vtheta - \vtheta^HU\vy -\vy^HU^H\vtheta + \vy^H\vy
   \\&=    \vtheta^H R \vtheta - (W^H\vtheta)^H -W^H\vtheta + \vy^H\vy
   \\&=    \vtheta^H R \vtheta - (W^H\vtheta)^\ast -W^H\vtheta + \vy^H\vy
   \\&=    \vtheta^H R \vtheta - (W^H)^\ast\vtheta -W^H\vtheta + \vy^H\vy
   \\&=    \vtheta^H R \vtheta - 2\Real\brs{W^H}\vtheta + \vy^H\vy
\\
\\
   \grad_\vtheta \fCost(\vtheta)
     &= \grad_\vtheta \left[ \vtheta^H R \vtheta - (W^H)^\ast\vtheta -W^H\vtheta + \vy^H\vy \right]
   \\&= R\vtheta + R^T\vtheta - [(W^H)^\ast]^T - [W^H]^T + 0
   \\&= R\vtheta + (R^H)^\ast\vtheta - W - W^\ast
   \\&= R\vtheta + R^\ast\vtheta - W - W^\ast
   \\&= (R + R^\ast)\vtheta - (W + W^\ast)
   \\&= 2(\Real{\setY})\vtheta - 2\Real{W}
\\
\\
   \vthetao
     &= (\Real{\setY})^{-1}(\Real{W})
\\
\\
   \fCost(\vthetao)
     &=    \vthetao^H R \vthetao - 2\Real\brs{W^H}\vthetao + \vy^H\vy
   \\&=    [(\Real{\setY})^{-1}(\Real{W})]^H R [(\Real{\setY})^{-1}(\Real{W})] - 2\Real\brs{W^H}[(\Real{\setY})^{-1}(\Real{W})] + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{\setY})^{-H} R (\Real{\setY})^{-1}(\Real{W})     - 2\Real\brs{W^H}(\Real{\setY})^{-1}(\Real{W})       + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{R^H})^{-1} R (\Real{\setY})^{-1}(\Real{W})   - 2\Real\brs{W^H}(\Real{\setY})^{-1}(\Real{W})     + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W})     - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W})      + \vy^H\vy
\\
\\
   \fCost(\vthetao)|_{R\mbox{ real}}
     &=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \vy^H\vy
   \\&=    (\Real{W^H})R^{-1} R R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \vy^H\vy
   \\&=    (\Real{W^H}) R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \vy^H\vy
   \\&=    \vy^H\vy - (\Real{W^H})R^{-1}(\Real{W})
\end{align*}
\end{proof}

%======================================
\subsection{Examples using polynomial modeling}
%======================================
%---------------------------------------
\begin{example}[Polynomial approximation]
\index{polynomial approximation}
\index{least squares}
\index{Vandermonde matrix}
%---------------------------------------
\mbox{}\\
\begin{minipage}{\tw-80mm}
Suppose we {\bf know} the locations
$\set{(x_n,y_n)}{n=1,2,3,4,5}$ of 5 data points.
Let $\vx$ and $\vy$ represent the locations of these points such that
\end{minipage}
\qquad
\tboxc{$
   \vx \eqd
   \left[\begin{array}{l}
      x_1  \\
      x_2  \\
      x_3  \\
      x_4  \\
      x_5
   \end{array}\right]
   \qquad
   \vy \eqd
   \left[\begin{array}{l}
      y_1  \\
      y_2  \\
      y_3  \\
      y_4  \\
      y_5
   \end{array}\right]
$}
\\
\begin{minipage}{\tw-80mm}
Suppose we want to find a second order polynomial
  $c x^2 + bx + a$
that best approximates
these 5 points in the least squares sense.
We define the matrix $U$ (known) and vector $\estn$ (to be computed)
as follows:\footnotemark
\end{minipage}
\footnotetext{\citer{horn}{29}}
\qquad
\tboxc{$
   U^H \eqd
   \mcom{
   \left[\begin{array}{lll}
      1  & x_1 & x_1^2  \\
      1  & x_2 & x_2^2  \\
      1  & x_3 & x_3^2  \\
      1  & x_4 & x_4^2  \\
      1  & x_5 & x_5^2
   \end{array}\right]
   }{Vandermonde matrix}
   \qquad
   \estn \eqd
   \left[\begin{array}{l}
      a  \\
      b  \\
      c  \\
   \end{array}\right]
$}
\\
Then, using \prefpp{thm:ls}, the best coefficients $\estn$
for the polynomial are
\\\indentx$
  \estn
     = \left[\begin{array}{l}
          a  \\
          b  \\
          c  \\
       \end{array}\right]
     = \mcom{(UU^H)^{-1}}{$R^{-1}$} \; \mcom{(U\vy)}{$W$}
     = \brp{\brs{\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}}^H
       \brs{\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}}}^{-1}
       \brp{
       \brs{\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}}^H
       \brs{\begin{array}{l}
          y_1  \\
          y_2  \\
          y_3  \\
          y_4  \\
          y_5
       \end{array}}
       }
$
\end{example}

%---------------------------------------
\begin{figure}
%---------------------------------------
  \centering
  \includegraphics{../common/math/graphics/pdfs/global-temp.pdf}
  \caption{Global temperature deviations from average over time \xref{ex:global-temp}\label{fig:global-temp}}
\end{figure}
%---------------------------------------
\begin{example}
\footnote{
  \citeP{callery2019},
  \citeP{uea_hadcrut4}
  }
\label{ex:global-temp}
%---------------------------------------
Find the best fit 3rd order polynomial
$\fp(x) = dx^3 + cx^2 + bx + a$
for the climate measurements illustrated in \prefpp{fig:global-temp}.
{\footnotesize\begin{align*}
  \estn
    &= \brs{\begin{array}{c}
         a\\b\\c\\d
       \end{array}}
     = \brp{\opU\opU^H}^{-1}\brp{\opU\vy}
     = \brp{\brs{\begin{array}{rrrr}
                  1 & 0      & 0^2    & 0^3
         \\       1 & 1      & 1^2    & 1^3
         \\   \vdots& \vdots & \vdots & \vdots
         \\       1 & 138    & 138^2  & 138^3
       \end{array}}^H
       \brs{\begin{array}{rrrr}
                  1 & 0      & 0^2    & 0^3
         \\       1 & 1      & 1^2    & 1^3
         \\   \vdots& \vdots & \vdots & \vdots
         \\       1 & 138    & 138^2  & 138^3
       \end{array}}}^{-1}
       \brp{
       \brs{\begin{array}{rrrr}
                  1 & 0      & 0^2    & 0^3
         \\       1 & 1      & 1^2    & 1^3
         \\   \vdots& \vdots & \vdots & \vdots
         \\       1 & 138    & 138^2  & 138^3
       \end{array}}^H
       \brs{\begin{array}{l}
            y_0
         \\ y_1
         \\ \vdots
         \\ y_{138}
       \end{array}}
       }
  \\&= \frac{1}{139} \brs{\begin{array}{rrrr}
                1   &        69 &       6371 &        661779
         \\    69   &      6371 &     661779 &      73323839
         \\  6371   &    661779 &   73323839 &    8462609259
         \\661779   &  73323839 & 8462609259 & 1004606426831
       \end{array}}^{-1}
       \brp{
       \brs{\begin{array}{rrrr}
                  1 & 0      & 0^2    & 0^3
         \\       1 & 1      & 1^2    & 1^3
         \\   \vdots& \vdots & \vdots & \vdots
         \\       1 & 138    & 138^2  & 138^3
       \end{array}}^H
       \brs{\begin{array}{l}
            y_0
         \\ y_1
         \\ \vdots
         \\ y_{138}
       \end{array}}
       }
  \\&=
    \brs{\begin{array}{r}
      -0.24606376973\\
      -0.00076398238\\
       0.00001657135\\
       0.00000034336
    \end{array}}
\end{align*}}
Using this polynomial, we could ``predict"/estimate (not necessarily very well)
what the temperature deviation will be
in 2030 (+1.1710) and was in 1870 (-0.23711).
\begin{lstlisting}[language=matlab]
Y = [
 -0.17
 -0.08
 -0.11
  ...
  0.93
  0.85
];
N = length(Y);
X = [0:2018-1880]';   % years-1880
U = [ones(1,N) ; X', ; X'.^2, ; X'.^3];
coefs = inv(U*U')*(U*Y)
a=coefs(1)
b=coefs(2)
c=coefs(3)
d=coefs(4)
bestFit = d*X.^3 + c*X.^2 + b*X + a;
plot(bestFit)
x = 2030;
predict2030 = d*(x-1880)^3 + c*(x-1880)^2 + b*(x-1880) + a
x = 1870;
predict1870 = d*(x-1880)^3 + c*(x-1880)^2 + b*(x-1880) + a
\end{lstlisting}
\end{example}

%---------------------------------------
\begin{figure}
%---------------------------------------
  \centering
  \includegraphics{../common/math/graphics/pdfs/cagefree-ratios.pdf}
  \caption{Ratio of cagefree chickens in U.S. \xref{ex:cagefree}\label{fig:cagefree}}
\end{figure}
%---------------------------------------
\begin{minipage}{\tw-95mm}
\begin{example}
\footnotemark
\label{ex:cagefree}
%---------------------------------------
Using a 3rd order polynomial fitting \xref{fig:cagefree},
we can predict that the number of cagefree chickens
in the U.S. will reach 32.07\% in 2022.
Note that this is also an example of \opb{non-uniform sampling}.
That is, there is only one sample per year before 2016, but multiple samples per year
from 2016 onward.
This is no problem for the LS algorithm. Simply use the data points that are available 
and ignore the ones that are not:
\end{example}
\end{minipage}
\footnotetext{%
  \citeP{mendez2019}
  }%
\hfill\tboxc{$\ds
  \opU^H = 
  \brs{\begin{array}{llll}
      1      & 07.997260 & 07.997260^2 & 07.997260^3
    \\1      & 08.997268 & 08.997268^2 & 08.997268^3
    \\1      & 09.997260 & 09.997260^2 & 09.997260^3
    \\\vdots & \vdots    & \vdots      & \vdots
    \\1      & 15.997260 & 15.997260^2 & 15.997260^3
    \\1      & 16.581967 & 16.581967^2 & 16.581967^3
    \\1      & 16.666667 & 16.666667^2 & 16.666667^3
    \\1      & 16.748634 & 16.748634^2 & 16.748634^3
    \\1      & 16.833333 & 16.833333^2 & 16.833333^3
   %\\1      & 16.915301 & 16.915301^2 & 16.915301^3
    \\\vdots & \vdots    & \vdots      & \vdots
  \end{array}}
$}
\\
\begin{lstlisting}[language=matlab]
% years - 2000
X = [ 
  07.997260  % 2007-12-31 decimal form
  08.997268  % 2008-12-31 decimal form
  09.997260  % 2009-12-31 decimal form
  10.997260  % 2010-12-31 decimal form
  11.997260  % 2011-12-31 decimal form
  12.997268  % 2012-12-31 decimal form
  13.997260  % 2013-12-31 decimal form
  14.997260  % 2014-12-31 decimal form
  15.997260  % 2015-12-31 decimal form
  16.581967  % 2016-08-01 decimal form
  16.666667  % 2016-09-01 decimal form
  16.748634  % 2016-10-01 decimal form
  16.833333  % 2016-11-01 decimal form
  16.915301  % 2016-12-01 decimal form
  ...
];
Y = [
  03.2000    % data for 2007-12-31
  03.5000    % data for 2008-12-31
  03.6000    % data for 2009-12-31
  04.4000    % data for 2010-12-31
  05.4000    % data for 2011-12-31
  06.0000    % data for 2012-12-31
  05.9000    % data for 2013-12-31
  05.7000    % data for 2014-12-31
  08.6000    % data for 2015-12-31
  10.1357    % data for 2016-08-01
  10.0570    % data for 2016-09-01
  12.2935    % data for 2016-10-01
  12.1006    % data for 2016-11-01
  11.7935    % data for 2016-12-01
  ...
];
format long g
pkg load signal;
N = length(Y);
U = [ones(1,N) ; X', ; X'.^2, ; X'.^3];
coefs = inv(U*U')*(U*Y)
a=coefs(1)
b=coefs(2)
c=coefs(3)
d=coefs(4)
bestFit = d*X.^3 + c*X.^2 + b*X + a;
plot(bestFit)
x = 2022;
predict2022 = d*(x-2000)^3 + c*(x-2000)^2 + b*(x-2000) + a
\end{lstlisting}
You can convert a date in the yyyy-mm-dd form such as 2007-12-31 to its decimal form 07.997260
using R with the \lstinline[language=R]{lubridate} package like this:
\begin{lstlisting}[language=R]
 install.packages("lubridate");
 require(lubridate);
 x     = read.csv("../common/datasets/cagefree-ratios_osf-6hty8.csv", comment.char = "#");
 ratio = x$ratio_hens;
 year  = lubridate::decimal_date(lubridate::ymd(x$observed_month));
 plot( year, ratio, col="blue", type='o' );
\end{lstlisting}

%---------------------------------------
\begin{example}
\footnote{
  \url{https://math.stackexchange.com/questions/3990086/}
  }
\label{ex:plant}
%---------------------------------------
let $tn = c(0:10)$ and
$yn = c(18,33,56,90,130,170,203,225,239,247,251)$ represent the growth of a plant.

%Have you considered using the _Method of Least Squares_ to find $\alpha $? 
%For that method you can define an _error function_ $$e(t_n)\eqd N(t_n)-y_n\eqd \frac{H_{\infty}}{1+\brs{H_{\infty}/N_0-1}e^{-\alpha t_n}}-y_n$$ and 
 You want to find the $\alpha $ that minimizes that cost; that is, you want to find where (with respect to $\alpha $) the cost function 
``goes to the lowest point". 
Suppose you set $N_0\eqd N(0)=18$ and $N_*\eqd252$. % $\ldots$
%From the plot, it appears that $cost(\alpha )$ is minimized around $\alpha =0.65$.
Let 
$\ds \esth(t) \eqd \frac{H_{\infty}}{1+\brp{\frac{H_{\infty}}{H_0}-1}e^{-\alpha t}}$
\end{example}
\begin{proof}
\begin{enumerate}
  \item definition: \label{idef:plant_cost} Define a cost function (the error cost) as the 
        norm squared of $e(t_n)$ as 
        \\\indentx$\ds \fCost(\alpha )\eqd\norm{e}^2\eqd\sum_{n=0}^{n=10}e^2(t_n)$

  \item lemma:\label{ilem:plant1}
    \begin{align*}
      \boxed{\pderiv{}{\alpha }\esth(t)}
        &\eqd \pderiv{}{\alpha }\brs{\frac{H_{\infty}}{1+\brp{\frac{H_{\infty}}{H_0}-1}e^{-\alpha t}}}
        && \text{by definition of $\esth(t)$}
      \\&= \frac{0-H_{\infty}\brs{\frac{H_{\infty}}{H_0}-1}e^{-\alpha t}(-t)}{\brp{1+\brs{\frac{H_{\infty}}{H_0}-1}e^{-\alpha t}}^2}
        && \text{by Quotient Rule}
      \\&= \frac{H_{\infty}^2}{\brp{1+\brs{\frac{H_{\infty}}{H_0}-1}e^{-\alpha t}}^2}
      \brs{\frac{1}{H_0}-\frac{1}{H_{\infty}}}te^{-\alpha t}
      \\&\eqd \boxed{\brs{\frac{1}{H_0}-\frac{1}{H_{\infty}}}\esth^2(t)te^{-\alpha t}}
      && \text{by definition of $\esth(t)$}
    \end{align*}

  \item lemma:\label{ilem:plant2}
    \begin{align*}
      \boxed{\pderiv{}{A}\esth(t)}
        &\eqd \pderiv{}{A }\brs{\frac{A}{1+\brp{\frac{A}{B}-1}e^{-\alpha t}}}
        && \text{by definition of $\esth(t)$}
      \\&= \frac{\brs{1+\brp{\frac{A}{B}-1}e^{-\alpha t}} - A\brs{\frac{1}{B}}e^{-\alpha t}}
                {\brp{1+\brs{\frac{A}{B}-1}e^{-\alpha t}}^2}
        && \text{by Quotient Rule}
      \\&= \frac{1 - e^{-\alpha t}}
                {\brp{1 + \brs{\frac{A}{B}-1}e^{-\alpha t}}^2}
      \\&= \brs{\frac{1 - e^{-\alpha t}} {A^2}}
           \brs{\frac{A}                 {1 + \brs{\frac{A}{B}-1}e^{-\alpha t}}}^2
      \\&\eqd \boxed{\brs{\frac{1 - e^{-\alpha t}}{A^2}} \esth^2(t)}
        && \text{by definition of $\esth(t)$}
    \end{align*}

  \item lemma:\label{ilem:plant3}
  \begin{align*}
      \boxed{\pderiv{}{N_0}\esth(t)}
        &\eqd \pderiv{}{N_0 }\brs{\frac{N_*}{1+\brp{\frac{N_*}{N_0}-1}e^{-a_0 t}}}
        && \text{by definition of $\esth(t)$}
      \\&= \frac{ - N_*\brp{\frac{-1}{N_0^2}}e^{-a_0 t}}
                {\brs{1+\brp{\frac{N_*}{N_0}-1}e^{-a_0 t}}^2}
        && \text{by Quotient Rule}
      \\&= \brs{ \frac{e^{-a_0 t}}{N_*N_0^2}}
           \brs{\frac{N_*}{1+\brs{\frac{N_*}{N_0}-1}e^{-a_0 t}}}^2
      \\&= \brs{ \frac{e^{-a_0 t}}{N_*N_0^2}}
           \esth^2(t)
        && \text{by definition of $\esth(t)$}
    \end{align*}

  \item 
    \begin{align*}
    \boxed{0}
      &= \frac{1}{2\brp{\frac{1}{H_0}-\frac{1}{H_{\infty}}}}\cdot0
    \\&=\frac{1}{2\brp{\frac{1}{H_0}-\frac{1}{H_{\infty}}}}\pderiv{}{\alpha }\norm{e}^2
    \\&\eqd \frac{1}{2\brp{\frac{1}{H_0}-\frac{1}{H_{\infty}}}}\pderiv{}{\alpha }\sum_{n=0}^{n=10}e^2(t_n)
      && \text{by definition of $\norm{\cdot}$}
    \\&\eqd \frac{1}{2\brp{\frac{1}{H_0}-\frac{1}{H_{\infty}}}}\pderiv{}{\alpha }\sum_{n=0}^{n=10}\brs{\esth(t_n)-y_n}^2
      && \text{by definition of $e$}
    \\&= \frac{1}{2\brp{\frac{1}{H_0}-\frac{1}{H_{\infty}}}}\sum_{n=0}^{n=10}2\brs{\esth(t_n)-y_n}\esth'(t_n)
      && \text{by \thme{Chain Rule}}
    \\&= \frac{1}{2\brp{\frac{1}{H_0}-\frac{1}{H_{\infty}}}}\sum_{n=0}^{n=10}2\brs{\esth(t_n)-y_n}\brs{\frac{1}{H_0}-\frac{1}{H_{\infty}}}\esth^2(t_n)te^{-\alpha t_n}
      && \text{by \pref{ilem:plant1}}
    \\&= \boxed{\sum_{n=0}^{n=10}\esth^2(t_n)\brs{\esth(t_n)-y_n} \brs{t_ne^{-\alpha t_n}}}
    \\&\eqd D\fCost(\alpha ) 
      && \text{(call the sum $D\fCost(\alpha )$)}
    \end{align*}

  \item 
    \begin{align*}
    \boxed{0}
      &= \frac{A^2}{2}\cdot0
    \\&=\frac{A^2}{2}
         \pderiv{}{A}\norm{e}^2
    \\&\eqd \frac{A^2}{2}
         \pderiv{}{A}\sum_{n=0}^{n=10}e^2(t_n)
      && \text{by definition of $\norm{\cdot}$}
    \\&\eqd \frac{A^2}{2}
         \pderiv{}{A}\sum_{n=0}^{n=10}\brs{\esth(t_n)-y_n}^2
      && \text{by definition of $e$}
    \\&= \frac{A^2}{2}
         \sum_{n=0}^{n=10}2\brs{\esth(t_n)-y_n}\esth'(t_n)
      && \text{by \thme{Chain Rule}}
    \\&= A^2
         \sum_{n=0}^{n=10}  \brs{\esth(t_n)-y_n}
                            \brs{\frac{1 - e^{-\alpha t_n}}{A^2}} \esth^2(t_n)
      && \text{by \pref{ilem:plant2}}
    \\&= \boxed{\sum_{n=0}^{n=10} \esth^2(t_n) \brs{\esth(t_n)-y_n} \brs{1 - e^{-\alpha t_n} }}
    \end{align*}

  \item 
    \begin{align*}
    \boxed{0}
      &= \frac{N_*N_0^2}{2}\cdot0
    \\&=\frac{N_*N_0^2}{2}
         \pderiv{}{N_0}\norm{e}^2
    \\&\eqd \frac{N_*N_0^2}{2}
         \pderiv{}{N_0}\sum_{n=0}^{n=10}e^2(t_n)
      && \text{by definition of $\norm{\cdot}$}
    \\&\eqd \frac{N_*N_0^2}{2}
         \pderiv{}{N_0}\sum_{n=0}^{n=10}\brs{\esth(t_n)-y_n}^2
      && \text{by definition of $e$}
    \\&= \frac{N_*N_0^2}{2}
         \sum_{n=0}^{n=10}2\brs{\esth(t_n)-y_n} \pderiv{}{N_0}\esth(t_n)
      && \text{by Chain Rule}
    \\&= \frac{N_*N_0^2}{2}
         \sum_{n=0}^{n=10}2\brs{\esth(t_n)-y_n} \brs{ \frac{e^{-a_0 t_n}}{N_*N_0^2}}\esth^2(t_n)
      && \text{by lemma}
    \\&= \boxed{\sum_{n=0}^{n=10} \esth^2(t_n)\brs{\esth(t_n)-y_n} e^{-a_0 t_n} }
    \end{align*}
\end{enumerate}

Plotting $Dcost$ with respect to $\alpha $, it appears that $D\fCost(\alpha )$ crosses $0$ at around $\alpha =0.66$.

The \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/uniroot}{uniroot} function from the R 
\href{https://www.rdocumentation.org/packages/stats/versions/3.6.2}{stats} package indicates that $D\fCost(\alpha )$ crosses $0$ 
at $\alpha =0.6631183$ with estim.prec=6.103516e-05.

Using $H_0\eqd18$, $H_{\infty}\eqd252$, and $\alpha \eqd0.6631183$, $\esth(t)$ 
seems to fit the 11 data points fairly well ($cost(0.6631183)=31.32307$) $\ldots$

\end{proof}

%======================================
%\section{Gradient search techniques}
\section{Recursive forms}
%=======================================
One of the biggest advantages of using a recursive form / gradient search technique is
that it can be implemented \hie{recursively} as shown in the next equation.
The general form of the gradient search parameter estimation techniques is\footnote{\citerp{nelles2001}{90}}
\thmbox{
   \vtheta_n = \vtheta_{n-1} - \eta_{n-1} R \;\left[\gradxy{\vtheta}{\fCost(\vtheta_n)}\right]
}
where at time $n$

\begin{tabular}{lll}
   $\vtheta_n$      & is the \hie{state    }   & (vector)  \\
   $\eta_n$     & is the \hie{step size}   & (scalar)  \\
   $\setY$          & is the \hie{direction}   & (matrix)  \\
   $\gradxy{\vtheta}{\fCost(\vtheta_n)}$      & is the \hie{gradient } of the cost function $\fCost(\vtheta_n)$   & (vector)
\end{tabular}

Two major categories of gradient search techniques are
\begin{liste}
   \item steepest descent (includes LMS)
   \item Newton's method (includes RLS and Kalman filters).
\end{liste}

The key difference between the two is that
{\bf \hie{steepest descent} uses only first derivative information},
while
{\bf \hie{Newton's method} uses both first and second derivative information}
making it converge much faster but with significantly higher
complexity.

%======================================
\subsubsection*{First derivative techniques}
\label{sec:1st-deriv}
%======================================
\paragraph{Steepest descent.}
In this algorithm, $R=I$ (identity matrix).
First derivative information is contained in $\grad\fCost$.
Second derivative information, if present, is contained in $\setY$.
Thus, steepest descent algorithms do not use second derivative information.
\thmbox{
  \vtheta_n = \vtheta_{n-1} - \eta_{n-1} \;\left[ \gradxy{\vtheta}{\fCost(\vtheta_n)} \right]
}
\paragraph{Least Mean Squares (LMS).}\footnote{\citerpg{manolakis2000}{526}{0070400512}}
This is a special case of \hie{steepest descent}.
In minimum mean square estimation \xref{sec:est_mms},
the cost function $\fCost(\vtheta)$ is defined as a
\hie{statistical average} of the error vector such that
$\fCost(\vtheta) = \Eb{\ve^H\ve}$.
In this case the gradient $\grad\fCost$ is difficult to compute.
However, the LMS algorithm greatly simplifies the problem by
instead defining the cost function as a function of the
\hie{instantaneous error} such that
\begin{align*}
   \vy &= y(n)
\\
   \vye &= \hat{y}(n)
\\
   \fCost(\vtheta)
   &= \norm{e(n)}^2
 \\&= e^2(n)
 \\&= (\hat{y}(n)-y(n))^2
\end{align*}

Computing the gradient of this cost function is then
just a special case of \hie{least squares estimation} \xref{sec:ls}.
Using LS, we let $U=\vx^T$ and hence
\begin{align*}
   \gradxy{\vtheta}{\fCost(\vtheta)}
   &= 2U^TU \vtheta -2U^T\vy                   && \text{ by \prefp{thm:ls}}
\\ &= 2\vx\vx^T \vtheta -2\vx y               && \text{ by above definitions}
\\ &= 2\vx\hat{y} -2\vx y                    && \text{ }
\\ &= 2\vx(\hat{y} -y)                      && \text{ }
\\ &= 2\vx e(n)                && \text{ }
\end{align*}

The LMS algorithm uses this instantaneous gradient for $\grad\fCost$,
lets $R=I$, and uses a constant step size $\eta$ to give
\thmbox{
  \vtheta_n = \vtheta_{n-1} - 2\eta \vx_n e(n)
}
%--------------------------------------
\subsubsection*{Second derivative techniques}
%--------------------------------------
\paragraph{Newton's Method.}
This algorithm uses the \hie{Hessian} matrix $H$,
which is the second derivative of the cost function $\fCost(\vtheta)$,
and lets $R=H^{-1}$.
\begin{align*}
   H_n &\eqd& \grad_\vtheta\grad_\vtheta \fCost(\vtheta_n)
\\
   \vtheta_n &= \vtheta_{n-1} - \eta_{n-1} H_n^{-1} \left[\grad_\vtheta\fCost(\vtheta_n)\right]
\end{align*}


\paragraph{Kalman filtering}\footnote{\citerp{nelles2001}{66}}
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vthetae(k-1) \\
   \vthetae(k) &= \vthetae(k-1) + \gamma(k)e(k)
\end{align*}

\if 0
\paragraph{RLS with forgetting}\footnote{\citerp{nelles2001}{64}}
This algorithm introduces a forgetting factor $\lambda$
to help the algorithm track non-stationary channels.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+\lambda}  P(k-1)x(k) \\
   P(k) &= \frac{1}{\lambda}(I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vthetae(k-1) \\
   \vthetae(k) &= \vthetae(k-1) + \gamma(k)e(k)
\end{align*}
\fi

\paragraph{Recursive Least Squares (RLS)}\footnote{\citerp{nelles2001}{66}}
This algorithm is a special case of either the RLS with forgetting
or the Kalman filter.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}  P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1) \\
   e(k) &= y(k) - x^T(k)\vthetae(k-1) \\
   \vthetae(k) &= \vthetae(k-1) + \gamma(k)e(k)
\end{align*}




%--------------------------------------
\section{Direct search}
%--------------------------------------
A direct search algorithm may be used in cases where the cost
function over $\vtheta$ has several local minimums, making convergence difficult.
Furthermore, direct search algorithms can be very computationally demanding.

