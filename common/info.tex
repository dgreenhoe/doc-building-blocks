%============================================================================
% NCTU - Hsinchu, Taiwan
% LaTe\rvX File
% Daniel Greenhoe
%============================================================================

%======================================
\chapter{Information Theory}
\label{chp:capacity}
\index{information theory}
%======================================
\begin{figure}[ht]
\color{figcolor}
\begin{center}
\begin{fsK}
\setlength{\unitlength}{0.17mm}                  
\begin{picture}(900,150)(-100,-50)
  \thicklines                                      
  %\graphpaper[10](0,0)(700,100)                  
  \put(-100, 125 ){\vector(0,-1){40}}
  \put(-100, 130 ){\makebox(0,0)[bl]{$R_d$ (data rate)}}
  \put(-100,  60 ){\makebox(30,0)[br]{$\{u_n\}$} }
  \put(-100,  50 ){\vector(1,0){50} }

  \put(-70, -50 ){\dashbox{4}( 280,160){} }
  \put(-70, -40 ){\makebox( 280,160)[b]{transmitter} }
  \put(   50, 130 ){\makebox(0,0)[bl]{$R_c$ (signal rate)}}
  \put(   75, 125 ){\vector(0,-1){40}}
  \put(   50,  60 ){\makebox(50,50)[b]{$\{y_n\}$} }
  \put(   50,  50 ){\vector(1,0){50} }

  \put(- 50,  00 ){\framebox( 100,100){} }
  \put(- 50,  10 ){\makebox( 100,80)[t]{channel} }
  \put(- 50,  10 ){\makebox( 100,80)[ ]{coder} }
  \put(- 50,  10 ){\makebox( 100,80)[b]{$\opair{L}{N}$} }
  \put( 100,  00 ){\framebox( 100,100){modulator} }
  \put( 200,  50 ){\vector(1,0){100} }

  \put( 350, 125 ){\vector(0,-1){25}}
  \put( 300, 130 ){\makebox(0,0)[bl]{$\frac{L}{N}=\frac{R_d}{R_c}\eqd R<C$ (channel capacity)}}
  \put( 300,  00 ){\framebox( 100,100){} }
  \put( 300,  30 ){\makebox( 100, 40)[t]{channel} }
  \put( 300,  10 ){\makebox( 100, 40)[b]{$\pp(\rvY|\rvX)$} }
  \put( 210,  60 ){\makebox( 90, 50)[b]{$s(t)$} }
  \put( 400,  60 ){\makebox( 80, 50)[b]{$r(t)$} }

  \put( 400,  50 ){\vector(1,0){100} }
  \put( 500,  00 ){\framebox(100,100){demodulator} }
  \put( 600,  60 ){\makebox(50,50)[b]{$\{\hat{y}_n\}$} }
  \put( 600,  50 ){\vector(1,0){50}}
  \put( 650,  00 ){\framebox(100,100){} }
  \put( 650,  30 ){\makebox(100,40)[t]{channel} }
  \put( 650,  30 ){\makebox(100,40)[b]{decoder} }
  \put( 480, -50 ){\dashbox{4}( 280,160){} }
  \put( 480, -40 ){\makebox( 280,160)[b]{receiver} }

  \put( 760,  60 ){\makebox(40,50)[b]{$\{\ue_n\}$} }
  \put( 750,  50 ){\vector(1,0){50}}
\end{picture}                                   
\end{fsK}
\end{center}
\caption{
   Memoryless modulation system model
   \label{fig:i_mod_model}
   }
\end{figure}




%=======================================
\section{Information Theory}
%=======================================
%=======================================
\subsection{Definitions}
%=======================================
The \fncte{Kullback Leibler distance} $\iD{\pp_1}{\pp_2}$ \xref{def:kld} is a 
measure between two probability density functions $\pp_1$ and $\pp_2$.
It is not a true distance measure\footnote{
  {\em Distance measure}: \prefpp{def:metric}
  }
but it behaves in a similar manner.
If $\pp_1=\pp_2$, then the \fncte{KL distance} is 0.
If $\pp_1$ is very different from $\pp_2$, then $|\iD{\pp_1}{\pp_2}|$ 
will be much larger.

%--------------------------------------
\begin{definition}
\footnote{
  \citerp{cover}{18}
  }
\label{def:kld}
\index{Kullback Leibler distance}
\index{relative entropy}
%--------------------------------------
Let $\pp_1$ and $\pp_2$ be probability density functions.
Then the {\bf Kullback Leibler distance}
(the \fncte{KL distance}, also called the {\bf relative entropy})
of $\pp_1$ and $\pp_2$ is
\defbox{
  \iD{\pp_1}{\pp_2} \eqd \pE \log_2 \frac{ \pp_1(\rvX) }{\pp_2(\rvX)} 
  \hspace{3ex}\mbox{bits}
  }
If the base of logarithm is $e$ (the ``natural logarithm") rather than $2$,
then the units are {\em nats} rather than {\em bits}.
\end{definition}

The \fncte{mutual information} $\iI(\rvX;\rvY)$ of random variable $\rvX$ and $\rvY$ is
the \fncte{KL distance} between their \fncte{joint distribution} $\pp(\rvX,\rvY)$ and the 
product of their \fncte{marginal distribution}s $\pp(\rvX)$ and $\pp(\rvY)$.
If $\rvX$ and $\rvY$ are independent, then the \fncte{KL distance} between 
joint and marginal product is $\log1=0$ and they have no 
\fncte{mutual information} ($\iI(\rvX;\rvY)=0$).
If $\rvX$ and $\rvY$ are highly correlated, then the \fncte{joint distribution} is
much different than the product of the marginals making the \fncte{KL distance}
greater and along with it the \fncte{mutual information} greater as well.
%--------------------------------------
\begin{definition}[Mutual information]
\footnote{
  \citerpp{cover}{18}{19}
  }
\label{def:I(X;Y)}
\index{information}
\index{information!mutual information}
%--------------------------------------
\defbox{
  \iI(\rvX;\rvY) \eqd \iD{\pp(\rvX,\rvY)}{\pp(\rvX)\pp(\rvY)} 
           \eqd \pExy \log_2 \frac{ \pp(\rvX,\rvY) }{\pp(\rvX)\pp(\rvY)} 
                \hspace{3ex}\mbox{bits}
  }
\end{definition}

The {\em self information} $\iI(\rvX;\rvX)$ of random variable $\rvX$ is the 
\fncte{mutual information} between $\rvX$ and itself.
That is, it is a measure of the information contained in $\rvX$.
Self information $\iI(\rvX;\rvX)$ can also be viewed as the \fncte{KL distance} between
the constant $1$ (no information because $1$ is completely known)
and $\pp(\rvX)$.
%--------------------------------------
\begin{definition}[Self information]
\footnote{
  \citerpp{cover}{18}{19}
  }
\label{def:I(X;X)}
\index{information!self information}
%--------------------------------------
\defbox{
  \iI(\rvX;\rvX) \eqd \iD{1}{\pp(\rvX)} 
           \eqd \pEx \log_2 \frac{1}{\pp(\rvX)} 
                \hspace{3ex}\mbox{bits}
  }
\end{definition}

The \hie{entropy} $\iH(\rvX)$ of a random variable $\rvX$ is equivalent to
the self information $\iI(\rvX;\rvX)$ of $\rvX$.
That is, the entropy of $\rvX$ is a measure of the information contained
in $\rvX$.

Likewise, the {\em conditional entropy} $\iH(\rvX|\rvY)$ 
of $\rvX$ given $\rvY$ is the information
contained in $\rvX$ given $\rvY$ has occurred. 
If $\rvX$ and $\rvY$ are independent, then $\rvX$ does not care about the occurrence of
$\rvY$. Thus in this case, 
the occurrence of $\rvY=y$ does not change the amount of information
provided by $\rvX$ and $\iH(\rvX|\rvY)=\iH(\rvX)$.
If $\rvX$ and $\rvY$ are highly correlated, 
the occurrence of $\rvY=y$ tells us a lot about what the value of $\rvX$ might
turn out to be.
Thus in this case, the information provided by $\rvX$ given $\rvY$ is greatly reduced
and $\iH(\rvX|\rvY)<<\iH(\rvX)$.

The {\em joint entropy} $\iH(\rvX,\rvY)$ of $\rvX$ and $\rvY$ is the amount of information 
contained in the ordered pair $(\rvX,\rvY)$.

%--------------------------------------
\begin{definition}[Entropy]
\footnote{
  \citerpp{cover}{15}{17}
  }
\label{def:H(X)}
\label{def:H(XY)}
\index{entropy}
\index{entropy!joint entropy}
\index{entropy!conditional entropy}
%--------------------------------------
\defbox{
\begin{array}{l@{\hspace{1cm}}rcl@{\hspace{1cm}}l}
  \mbox{entropy of $\rvX$}                       : & \iH(\rvX)        &\eqd& \pEx  \log_2 \frac{1}{\pp(\rvX)  }  & \mbox{bits} \\
  \mbox{joint entropy of $\rvX,\rvY$}               : & \iH(\rvX,\rvY)      &\eqd& \pExy \log_2 \frac{1}{\pp(\rvX,\rvY)}  & \mbox{bits} \\
  \mbox{conditional entropy of $\rvX$ given $\rvY$} : & \iH(\rvX|\rvY)      &\eqd& \pExy \log_2 \frac{1}{\pp(\rvX|\rvY)}  & \mbox{bits}   
\end{array}
}
\end{definition}


%=======================================
\subsection{Relations}
%=======================================
\begin{figure}[ht]
\begin{center}\begin{footnotesize}
\setlength{\unitlength}{0.4mm}
\begin{picture}(150,180)(0,0)
  \thicklines
  {\color[rgb]{0,0,1}
  \put(  50,  50){\oval(100,100)}
  \put(  50, 105){\makebox(0,0)[b]{$\iH(\rvX)$}}
  \put(  25,  50){\makebox(0,0){$\iH(\rvX|\rvY)$}}
  }
  {\color[rgb]{1,0,0}
  \put( 100,  50){\oval(100,100)}
  \put( 100, 105){\makebox(0,0)[b]{$\iH(\rvY)$}}
  \put( 125,  50){\makebox(0,0){$\iH(\rvY|\rvX)$}}
  }
  {\color[rgb]{0.5,0,0.5}
  \put(  75, 120){\makebox(0,0)[b]{$\iH(\rvX,\rvY)$}}
  \put(  75,  50){\makebox(0,0)[b]{$\iI(\rvX;\rvY)$}}
  }
\end{picture}
\end{footnotesize}\end{center}
\caption{
  Relationship between information and entropy
  \label{fig:HI}
  }
\end{figure}

%--------------------------------------
\begin{theorem}
%--------------------------------------
\thmbox{ \iH(\rvX,\rvY) = \iH(\rvY,\rvX)  }
\end{theorem}
\begin{proof}
\begin{eqnarray*}
  \iH(\rvX,\rvY)
    &\eqd& \pExy \log \frac{1}{\pp_{xy}(\rvX,\rvY)}
  \\&=&    \pEyx \log \frac{1}{\pp_{yx}(\rvY,\rvX)}
  \\&\eqd& \iH(\rvY,\rvX)
\end{eqnarray*}
\end{proof}

%--------------------------------------
\begin{theorem}[Entropy chain rule]
\label{thm:chain}
\index{chain rule!entropy}
\index{Entropy chain rule}
\index{theorems!Entropy chain rule}
%--------------------------------------
\thmbox{
  \begin{array}{rcl}
    \iH(\rvX,\rvY) &=& \ds \iH(\rvX|\rvY) + \iH(\rvY)  \\
             &=& \ds \iH(\rvY|\rvX) + \iH(\rvX). \\
    \iH(\rvX_1,\rvX_2,\ldots,\rvX_N) &=& \ds \sum_{n=1}^{N-1}\iH(\rvX_n|\rvX_{n+1},\ldots,\rvX_N) + \iH(\rvX_N)
  \end{array}
}
\end{theorem}
\begin{proof}
\begin{eqnarray*}
  \iH(\rvX,\rvY)
    &\eqd& \pExy \log \frac{1}{\pp(\rvX,\rvY)}
  \\&=&    \pExy \log \frac{1}{\pp(\rvX|\rvY)\pp(\rvY)}
  \\&=&    \pExy \log \frac{1}{\pp(\rvX|\rvY)} + \pExy \log \frac{1}{\pp(\rvY)}
  \\&=&    \pExy \log \frac{1}{\pp(\rvX|\rvY)} + \pEy \log \frac{1}{\pp(\rvY)}
  \\&=&    \iH(\rvX|\rvY) + \iH(\rvY)
\\
\\
  \iH(\rvX,\rvY)
    &\eqd& \pExy \log \frac{1}{\pp(\rvX,\rvY)}
  \\&=&    \pExy \log \frac{1}{\pp(\rvY|\rvX)\pp(\rvX)}
  \\&=&    \pExy \log \frac{1}{\pp(\rvY|\rvX)} + \pExy \log \frac{1}{\pp(\rvX)}
  \\&=&    \pExy \log \frac{1}{\pp(\rvY|\rvX)} + \pEy \log \frac{1}{\pp(\rvX)}
  \\&=&    \iH(\rvY|\rvX) + \iH(\rvX)
\\
\\
  \iH(\rvX_1,\rvX_2,\ldots,\rvX_N) 
    &=& \iH(\rvX_1|\rvX_2,\ldots,\rvX_N) + \iH(\rvX_2,\ldots,\rvX_N)
  \\&=& \iH(\rvX_1|\rvX_2,\ldots,\rvX_N) + \iH(\rvX_2|\rvX_3,\ldots,\rvX_N) + \iH(\rvX_3,\ldots,\rvX_N)
  \\&=& \iH(\rvX_1|\rvX_2,\ldots,\rvX_N) + \iH(\rvX_2|\rvX_3,\ldots,\rvX_N) + \iH(\rvX_3|\rvX_4,\ldots,\rvX_N) + \iH(\rvX_4,\ldots,\rvX_N)
  \\&=& \sum_{n=1}^{N-1}\iH(\rvX_n|\rvX_{n+1},\ldots,\rvX_n) + \iH(\rvX_N)
\end{eqnarray*}
\end{proof}


%--------------------------------------
\begin{theorem}
%--------------------------------------
\thmbox{
  \begin{array}{rcl}
    \iI(\rvX;\rvY) &=& \iH(\rvX) - \iH(\rvX|\rvY)           \\
    \iI(\rvX;\rvY) &=& \iH(\rvY) - \iH(\rvY|\rvX)           \\
    \iI(\rvX;\rvY) &=& \iH(\rvX) + \iH(\rvY) - \iH(\rvX,\rvY)  \\
    \iI(\rvX;\rvY) &=& \iI(\rvY;\rvX)                    \\
    \iI(\rvX;\rvX) &=& \iH(\rvX)
  \end{array}
}
\end{theorem}
\begin{proof}
\begin{eqnarray*}
  \iI(\rvX;\rvY)
    &\eqd& \pExy \log_2 \frac{ \pp(\rvX,\rvY) }{\pp(\rvX)\pp(\rvY)}
  \\&=&    \pExy \log_2 \frac{ \pp(\rvX|\rvY) }{\pp(\rvX)}
  \\&=&    \pExy \log_2 \frac{ 1 }{\pp(\rvX)} + \pExy \log_2 \pp(\rvX|\rvY) 
  \\&=&    \pExy \log_2 \frac{ 1 }{\pp(\rvX)} - \pExy \log_2 \frac{1}{\pp(\rvX|\rvY)}
  \\&\eqd& \iH(\rvX) - \iH(\rvX|\rvY)
\\
\\
  \iI(\rvX;\rvY)
    &\eqd& \pExy \log_2 \frac{ \pp(\rvX,\rvY) }{\pp(\rvX)\pp(\rvY)}
  \\&=&    \pExy \log_2 \frac{ \pp(\rvY|\rvX) }{\pp(\rvY)}
  \\&=&    \pExy \log_2 \frac{ 1 }{\pp(\rvY)} + \pExy \log_2 \pp(\rvY|\rvX) 
  \\&=&    \pExy \log_2 \frac{ 1 }{\pp(\rvY)} - \pExy \log_2 \frac{1}{\pp(\rvY|\rvX)}
  \\&\eqd& \iH(\rvY) - \iH(\rvY|\rvX)
\\
\\
  \iI(\rvX;\rvY)
    &=&    \iH(\rvY) - \iH(\rvY|\rvX)
  \\&=&    \iI(\rvY;\rvX)
\\
\\
  \iI(\rvX;\rvX)
    &\eqd& \pExy \log_2 \frac{ \pp(\rvX,\rvX) }{\pp(\rvX)\pp(\rvX)}
  \\&=&    \pExy \log_2 \frac{ \pp(\rvX)   }{\pp(\rvX)\pp(\rvX)}
  \\&=&    \pExy \log_2 \frac{ 1        }{\pp(\rvX)      }
  \\&\eqd& \iH(\rvX)
\\
\\
  \iI(\rvX;\rvY)
    &\eqd& \iH(\rvX) - \iH(\rvX|\rvY)
  \\&=&    \iH(\rvX) - [ \iH(\rvX,\rvY) - \iH(\rvY) ]
  \\&=&    \iH(\rvX) + \iH(\rvY) - \iH(\rvX,\rvY) 
\end{eqnarray*}
\end{proof}

%--------------------------------------
\begin{theorem}[Information chain rule]
\index{chain rule!information}
\index{information chain rule}
\index{theorems!information chain rule}
%--------------------------------------
\thmbox{
  \iI(\rvX_1,\rvX_2,\ldots,\rvX_N;\rvY)
    = \sum_{n=1}^{N-1}\iI(\rvX_n|\rvX_{n+1},\ldots,\rvX_N) 
        + \iI(\rvX_N) 
  }
\end{theorem}
\begin{proof}
\begin{eqnarray*}
  \iI(\rvX_1,\rvX_2,\ldots,\rvX_N;\rvY)
    &=& \iH(\rvX_1,\rvX_2,\ldots,\rvX_N) - \iH(\rvX_1,\rvX_2,\ldots,\rvX_N|\rvY)
  \\&=& \sum_{n=1}^{N-1}\iH(\rvX_n|\rvX_{n+1},\ldots,\rvX_N) + \iH(\rvX_N) 
        - \sum_{n=1}^{N-1}\iH(\rvX_n|\rvX_{n+1},\ldots,\rvX_N,\rvY) - \iH(\rvX_N|\rvY) 
  \\&=& \sum_{n=1}^{N-1}\left[ 
        \iH(\rvX_n|\rvX_{n+1},\ldots,\rvX_N) - \iH(\rvX_n|\rvX_{n+1},\ldots,\rvX_N,\rvY) 
        \right]
        + \left[ \iH(\rvX_N) - \iH(\rvX_N|\rvY) \right]
  \\&=& \sum_{n=1}^{N-1}\iI(\rvX_n|\rvX_{n+1},\ldots,\rvX_N) 
        + \iI(\rvX_N) 
\end{eqnarray*}
\end{proof}

%=======================================
\subsection{Properties}
%=======================================
%---------------------------------------
\begin{theorem}
\footnote{
  \citerp{cover}{26}
  }
%---------------------------------------
\thmbox{
\begin{array}{rcl}
  \iD{\pp_1}{\pp_2} &\ge& 0  \\
  \iI(\rvX;\rvY)          &\ge& 0
\end{array}
}
\end{theorem}
\begin{proof}
\begin{eqnarray*}
  \iD{\pp_1}{\pp_2}
    &\eqd& \pEx\log\frac{\pp_1(\rvX)}{\pp_2(\rvX)}
  \\&=&    \pEx\left[-\log\frac{\pp_2(\rvX)}{\pp_1(\rvX)} \right]
  \\&\ge&  -\log\pEx\left[\frac{\pp_2(\rvX)}{\pp_1(\rvX)} \right]
    \hspace{1cm}\mbox{by \thme{Jensen's Inequality} \xref{thm:jensen}}
  \\&=&    -\log\int_x \pp_1(x)\frac{\pp_2(x)}{\pp_1(x)} \dx
  \\&=&    -\log\int_x \pp_2(x) \dx
  \\&=&    -\log(1)
  \\&=&    0
\end{eqnarray*}
\end{proof}
%=======================================
\section{Channel Capacity}
%=======================================
%--------------------------------------
\begin{definition}
\label{def:iC}
%--------------------------------------
Let $\opair{L}{N}$ be a block coder with $\xN$ output bits for each $L$ input bits.
\[
\begin{array}{rcll}
  R   &\eqd& \frac{L}{N}    & \mbox{coding rate}      \\
  \iC &\eqd& \max \iI(\rvX;\rvY)  & \mbox{channel capacity} \\
  \iE(R) &\eqd& \max_\rho \max_Q [\iE_0(\rho,Q)-\rho R ]            & \mbox{random coding exponent}
\end{array}
\]
\end{definition}

%--------------------------------------
\begin{theorem}[\thmd{noisy channel coding theorem}]
\footnote{
  \citerp{gallager}{143}
  }
\label{thm:ncct}
%--------------------------------------
\thmboxp{
If $ R < \iC$
then it is possible to construct an encoder and decoder such that 
the probability of error $P_e$ is arbitrarily small. Specifically
\\\indentx$P_e \le e^{-N\iE(R)}$
\\
For $0\le R\ge\iC$, the function $\iE(R)$ is
\prope{positive},
\prope{decreasing}, and
\prope{convex}.
}
\end{theorem}

\begin{figure}[ht]
\color{figcolor}
\setlength{\unitlength}{0.2mm}
\begin{center}
\begin{picture}(200,100)(-50,0)
  \thicklines
  \put(  0,  0){\line(1, 0){120}}
  \put(  0,  0){\line(0, 1){120}}
  %\put(125,  0){\makebox(0,0)[l]{$e$}}
  \put(100, -5){\makebox(0,0)[t]{$\iC$}}
  %\put( -5,100){\makebox(0,0)[r]{$\iC$}}
  \qbezier(0,100)(20,20)(100,0)
  \qbezier[32](0,100)(50,100)(100,100)
  \qbezier[32](100,0)(100,50)(100,100)
  \put( 50,90){\makebox(0,0)[t]{$\iE(R)$}}
\end{picture}
\end{center}
\caption{
  Typical $\iE(R)$
  \label{fig:E(R)}
  }
\end{figure}


\begin{figure}[ht]
\color{figcolor}
\setlength{\unitlength}{0.2mm}
\begin{center}
\begin{tabular}{cccc}
\begin{picture}(200,100)(-50,0)
  \thicklines
  \put(  0,100){\vector(1, 0){100}}
  \put(  0,  0){\vector(1, 0){100}}
  \put(  0,100){\vector(1,-1){100}}
  \put(  0,  0){\vector(1, 1){100}}
  \put(-30, 50){\makebox(0,0)[r]{$\rvX$}}
  \put(130, 50){\makebox(0,0)[l]{$\rvY$}}
  \put(-05,100){\makebox(0,0)[r]{$0$}}
  \put(-05,  0){\makebox(0,0)[r]{$1$}}
  \put(105,100){\makebox(0,0)[l]{$0$}}
  \put(105,  0){\makebox(0,0)[l]{$1$}}
  \put( 50,105){\makebox(0,0)[b]{$1-\epsilon$}}
  \put( 50, -5){\makebox(0,0)[t]{$1-\epsilon$}}
  \put( 27, 77){\makebox(0,0)[bl]{$\epsilon$}}
  \put( 23, 27){\makebox(0,0)[br]{$\epsilon$}}
\end{picture}
&
\begin{picture}(200,100)(-50,0)
  \thicklines
  \put(  0,  0){\line(1, 0){120}}
  \put(  0,  0){\line(0, 1){120}}
  \put(125,  0){\makebox(0,0)[l]{$p$}}
  \put( -5,100){\makebox(0,0)[r]{$1$}}
  \put(100, -5){\makebox(0,0)[t]{$1$}}
  \put( 50, -5){\makebox(0,0)[t]{$\frac{1}{2}$}}
  \qbezier[32](50,0)(50,50)(50,100)
  \qbezier(0,0)(50,200)(100,0)
  \qbezier[16](0,100)(25,100)(50,100)
  \put( 80,80){\makebox(0,0)[l]{$\iH(\rvX)$}}
\end{picture}
&
\begin{picture}(200,100)(-50,0)
  \thicklines
  \put(  0,  0){\line(1, 0){120}}
  \put(  0,  0){\line(0, 1){120}}
  \put(125,  0){\makebox(0,0)[l]{$e$}}
  \put(100, -5){\makebox(0,0)[t]{$1$}}
  \put( 50, -5){\makebox(0,0)[t]{$\frac{1}{2}$}}
  \put( -5,100){\makebox(0,0)[r]{$1$}}
  \qbezier[32](50,0)(50,50)(50,100)
  \qbezier(0,0)(50,200)(100,0)
  \qbezier[16](0,100)(25,100)(50,100)
  \put( 80,80){\makebox(0,0)[l]{$\iH(\rvY|\rvX)$}}
\end{picture}
&
\begin{picture}(200,100)(-50,0)
  \thicklines
  \put(  0,  0){\line(1, 0){120}}
  \put(  0,  0){\line(0, 1){120}}
  \put(125,  0){\makebox(0,0)[l]{$\epsilon$}}
  \put(100, -5){\makebox(0,0)[t]{$1$}}
  \put( 50, -5){\makebox(0,0)[t]{$\frac{1}{2}$}}
  \put( -5,100){\makebox(0,0)[r]{$1$}}
  \qbezier(0,100)(50,-100)(100,100)
  \qbezier[32](0,100)(50,100)(100,100)
  \qbezier[32](100,0)(100,50)(100,100)
  \put( 50,90){\makebox(0,0)[t]{$\iI(\rvX;\rvY)$}}
\end{picture}
\end{tabular}
\end{center}
\caption{
  Binary symmetric channel (BSC)
  \label{fig:bsc}
  }
\end{figure}


%=======================================
%\section{Channel Capacity}
%=======================================
\begin{figure}[ht]
\color{figcolor}
\begin{center}
\begin{fsK}
\setlength{\unitlength}{0.17mm}
\begin{picture}(900,200)(-100,-50)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(-100, 125 ){\vector(0,-1){40}}
  \put(-100, 130 ){\makebox(0,0)[bl]{$R_d$ (data rate)}}
  \put(-100,  60 ){\makebox(30,0)[br]{$\{u_n\}$} }
  \put(-100,  50 ){\vector(1,0){50} }

  \put(-70, -50 ){\dashbox{4}( 280,160){} }
  \put(-70, -40 ){\makebox( 280,160)[b]{transmitter} }
  \put(   50, 130 ){\makebox(0,0)[bl]{$R_c$ (signal rate)}}
  \put(   75, 125 ){\vector(0,-1){40}}
  \put(   50,  60 ){\makebox(50,50)[b]{$\{y_n\}$} }
  \put(   50,  50 ){\vector(1,0){50} }

  \put(- 50,  00 ){\framebox( 100,100){} }
  \put(- 50,  10 ){\makebox( 100,80)[t]{channel} }
  \put(- 50,  10 ){\makebox( 100,80)[ ]{coder} }
  \put(- 50,  10 ){\makebox( 100,80)[b]{$\opair{L}{N}$} }
  \put( 100,  00 ){\framebox( 100,100){modulator} }
  \put( 200,  50 ){\vector(1,0){100} }

  \put( 350, 125 ){\vector(0,-1){25}}
  \put( 300, 130 ){\makebox(0,0)[bl]{$\frac{L}{\xN}=\frac{R_d}{R_c}\eqd R<C$ (channel capacity)}}
  \put( 300,  00 ){\framebox( 100,100){} }
  \put( 300,  30 ){\makebox( 100, 40)[t]{channel} }
  \put( 300,  10 ){\makebox( 100, 40)[b]{$\pp(Y|X)$} }
  \put( 210,  60 ){\makebox( 90, 50)[b]{$\fs(t)$} }
  \put( 400,  60 ){\makebox( 80, 50)[b]{$\fr(t)$} }

  \put( 400,  50 ){\vector(1,0){100} }
  \put( 500,  00 ){\framebox(100,100){demodulator} }
  \put( 600,  60 ){\makebox(50,50)[b]{$\{\hat{y}_n\}$} }
  \put( 600,  50 ){\vector(1,0){50}}
  \put( 650,  00 ){\framebox(100,100){} }
  \put( 650,  30 ){\makebox(100,40)[t]{channel} }
  \put( 650,  30 ){\makebox(100,40)[b]{decoder} }
  \put( 480, -50 ){\dashbox{4}( 280,160){} }
  \put( 480, -40 ){\makebox( 280,160)[b]{receiver} }

  \put( 760,  60 ){\makebox(40,50)[b]{$\{\ue_n\}$} }
  \put( 750,  50 ){\vector(1,0){50}}
\end{picture}
\end{fsK}
\end{center}
\caption{
   Memoryless modulation system model
   %\label{fig:i_mod_model}
   }
\end{figure}






\begin{figure}[ht] \color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.20mm}
\begin{picture}(700,150)(-100,0)
  \thicklines
  %\graphpaper[10](0,0)(500,100)
  \put(-100 ,  60 ){\makebox( 100,0)[b]{$\su$} }
  \put(-100 ,  50 ){\vector(1,0){100} }

  \put(  00 ,  10 ){\makebox( 100, 80)[t]{transmit} }
  \put(  00 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put(  00 ,  10 ){\makebox( 100, 80)[b]{$\opT$} }
  \put(  00 ,  00 ){\framebox( 100,100){} }

  \put( 100 ,  60 ){\makebox( 100,0)[b]{$X$} }
  \put( 100 ,  50 ){\vector(1,0){140} }


  \put( 200 ,  00 ){\makebox(100, 95)[t]{$Z$} }
  \put( 260,   50 ){\line  (1,0){ 45} }
  \put( 250 ,  80 ){\vector(0,-1){20} }
  \put( 250,   50) {\circle{20}                   }
  \put( 200 ,  00 ){\dashbox(100,100){$+$} }
  \put( 200 ,  10 ){\makebox(100, 90)[b]{channel $\opC$} }

  %\put( 200 ,  10 ){\makebox( 100, 80)[t]{channel} }
  %\put( 200 ,  10 ){\makebox( 100, 80)[c]{operation} }
  %\put( 200 ,  10 ){\makebox( 100, 80)[b]{\opC} }
  %\put( 200 ,  00 ){\framebox(100,100){} }

  \put( 300 ,  60 ){\makebox( 100,0)[b]{$Y$} }
  \put( 300 ,  50 ){\vector(1,0){100} }

  \put( 400 ,  00 ){\framebox(100,100){} }
  \put( 400 ,  10 ){\makebox( 100, 80)[t]{receive} }
  \put( 400 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put( 400 ,  10 ){\makebox( 100, 80)[b]{$\opR$} }

  \put( 500 ,  60 ){\makebox( 100,0)[b]{$\sue$} }
  \put( 500 ,  50 ){\vector(1,0){100} }

  %\put(- 90 , -10 ){\makebox( 0, 0)[tl]{$\vu\eqd\su$} }
  %\put( 110 , -10 ){\makebox( 0, 0)[tl]{$\fs(t;\vu)=\opT\vu$} }
  %\put( 310 , -10 ){\makebox( 0, 0)[tl]{$\fr(t;\vu)=\opC\opT\vu$} }
  %\put( 510 , -10 ){\makebox( 0, 0)[tl]{$\sue=\opR\opC\opT\vu$} }

\end{picture}
\end{fsL}
\end{center}
\caption{
   Additive noise system model
   %\label{fig:i_addNoise_model}
   }
\end{figure}

How much information can be reliably sent through the channel?
The answer depends on the \hie{channel capacity}\ifsxref{info}{def:iC}
$\iC$.
As proven by the \hie{Noisy Channel Coding Theorem} (NCCT)\ifxref{info}{thm:ncct},
each transmitted symbol can carry up to $\iC$ bits for any arbitrarily
small probability of error greater than zero.
The price for decreasing error is increasing the block code size.

Note that the NCCT does not say at what rate
(in bits/second) you can send data through the AWGN channel.
The AWGN channel knows nothing of time (and is therefore not a
realistic channel).
The NCCT channel merely gives a \hie{coding rate}.
That is, the number of information bits each symbol can carry.
Channels that limit the rate (in bits/second) that can be sent through
it are obviously aware of time and are often referred to as
\hie{bandlimited channels}.

%--------------------------------------
\begin{theorem}
%--------------------------------------
Let $Z\sim\pN{0}{\sigma^2}$. Then
\thmbox{  \iH(Z) = \frac{1}{2}\log_2 2\pi e \sigma^2  }
\end{theorem}
\begin{proof}
\begin{align*}
  \iH(Z)
    &= \pEz \log \frac{1}{\pp(Z)}
  \\&= -\pEz \log \pp(z)
  \\&= -\pEz
         \log \left[\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-z^2}{2\sigma^2}} \right]
  \\&= -\pEz \left[
        -\frac{1}{2}\log(2\pi\sigma^2)
        + \frac{-z^2}{2\sigma^2} \log e
        \right]
  \\&= \frac{1}{2} \pEz \left[
        \log(2\pi\sigma^2)
        + \frac{\log e}{\sigma^2}z^2
        \right]
  \\&= \frac{1}{2} \left[
        \log(2\pi\sigma^2) + \frac{\log e}{\sigma^2}\pEz z^2
        \right]
  \\&= \frac{1}{2} \left[
        \log(2\pi\sigma^2) + \frac{\log e}{\sigma^2}(\sigma^2+0)
        \right]
  \\&= \frac{1}{2} \left[
        \log(2\pi\sigma^2) + \log e
        \right]
  \\&= \frac{1}{2} \log(2\pi e\sigma^2)
\end{align*}
\end{proof}

%--------------------------------------
\begin{theorem}
%--------------------------------------
Let $Y=X+Z$ be a Gaussian channel with $\pE X^2=P$ and
$Z\sim\pN{0}{\sigma^2}$. Then
\thmbox{
  \iI(X;Y) \le \frac{1}{2}\log\left( 1 + \frac{P}{\sigma^2}\right) = \iC
  %\hspace{1cm}\mbox{bits per usage}
  }
\end{theorem}
\begin{proof}
No proof at this time. \attention

Reference: \cite[page 241]{cover}
\end{proof}

%---------------------------------------
\begin{example}
%---------------------------------------

\begin{enumerate}
  \item If there is no transmitted energy ($P=0$), then the capacity of
        the channel to pass information is
    \begin{align*}
      \iC
        &= \frac{1}{2}\log_2\left( 1 + \frac{P}{\sigma^2}\right)
      \\&= \frac{1}{2}\log_2\left( 1 + \frac{0}{\sigma^2}\right)
      \\&= 0
    \end{align*}
  That is, the symbols cannot carry any information.

  \item If there is finite symbol energy and no noise ($\sigma^2=0$),
        then the capacity of the channel to pass information is
    \begin{align*}
      \iC
        &= \frac{1}{2}\log_2\left( 1 + \frac{P}{0}\right)
      \\&= \infty
    \end{align*}
  That is, each symbol can carry an infinite amount of information.
  That is, we can use a modulation scheme with an infinite
  number of of signaling waveforms (analog modulation)
  and thus each symbol can be represented by one of an
  infinite number of waveforms.

  \item If the transmitted energy is ($P=15\sigma^2$),
        then the capacity of the channel to pass information is
    \begin{align*}
      \iC
        &= \frac{1}{2}\log_2\left( 1 + \frac{15\sigma^2}{\sigma^2}\right)
      \\&= \frac{1}{2}\log_2\left( 1 + 15\right)
      \\&= \frac{1}{2} 4
      \\&= 2
    \end{align*}
  This means
  \begin{align*}
    2
      &= \iC
       >  \rchan
       \eqd \frac{\mbox{information bits}}{\mbox{symbol}}
       = \frac{\mbox{information bits}}{\mbox{coded bits}} \times
         \frac{\mbox{coded bits}}{\mbox{symbol}}
       = \rcode \rsym
  \end{align*}
  This means that if the coding rate is $\rcode=1/4$,
  then we must use a modulation with $256$ ($\rsym=8$ bits/symbol)
  or fewer waveforms.

  Conversely, if the modulation scheme uses $4$ waveforms, then
  $\rsym=2$ bits/symbol and so the code rate $\rcode$ can be
  up to $1$ (almost no coding redundancy is needed).


  \item If there is the transmitted energy ($P=\sigma^2$),
        then the capacity of the channel to pass information is
    \begin{align*}
      \iC
        &= \frac{1}{2}\log_2\left( 1 + \frac{\sigma^2}{\sigma^2}\right)
      \\&= \frac{1}{2}\log_2\left( 1 + 1\right)
      \\&= \frac{1}{2}
    \end{align*}
  That is, each symbol can carry just under $1/2$ bits of information.
  This means
  \begin{align*}
    \frac{1}{2}
      &= \iC
       >  \rchan
       \eqd \frac{\mbox{information bits}}{\mbox{symbol}}
       = \frac{\mbox{information bits}}{\mbox{coded bits}} \times
         \frac{\mbox{coded bits}}{\mbox{symbol}}
       = \rcode \rsym
  \end{align*}
  This means that if the coding rate is $\rcode=1/4$,
  then we must use a modulation with $4$ ($\rsym=2$ bits/symbol)
  or fewer waveforms.

  Conversely, if the modulation scheme uses $16$ waveforms, then
  $\rsym=4$ bits/symbol and so the code rate $\rcode$ must be
  less than $1/8$.
\end{enumerate}
\end{example}




%=======================================
\section{Specific channels}
%=======================================
%=======================================
\subsection{Binary Symmetric Channel (BSC)}
%=======================================
The properties of the {\em binary symmetric channel (BSC)} 
are illustrated in \prefpp{fig:bsc} and stated in 
\pref{thm:bsc} (next).
%--------------------------------------
\begin{theorem}[Binary symmetric channel]
\label{thm:bsc}
\index{Binary symmetric channel}
\index{theorems!Binary symmetric channel}
%--------------------------------------
Let $\opC:\rvX\to \rvY$ be a channel operation with $\rvX,\rvY\in\{0,1\}$ and
\begin{eqnarray*}
  p &\eqd& \pP{\rvX=1} \\
  \pP{\rvY=1|\rvX=0} &=& \pP{\rvY=0|\rvX=1} \eqd \epsilon
\end{eqnarray*}
Then
\thmbox{\begin{array}{rcl}
  \pP{\rvY=1} &=& \epsilon+p-2\epsilon p
\\
  \pP{\rvY=0} &=& 1-p-\epsilon +2\epsilon p
\\
  \iH(\rvX)    &=&    p     \log_2 \frac{1}{p} +
           (1-p) \log_2 \frac{1}{(1-p)}
\\
  \iH(\rvY)    &=&    (1-p-\epsilon +2\epsilon p) \log_2 \frac{1}{1-p-\epsilon +2\epsilon p} + 
           (\epsilon +p-2\epsilon p)   \log_2 \frac{1}{\epsilon +p-2\epsilon p}
\\
  \iH(\rvY|\rvX)  &=&    (1-\epsilon ) \log_2 \frac{1}{1-\epsilon } +
           \epsilon      \log_2 \frac{1}{\epsilon   }
\\
  \iI(\rvX;\rvY)  &=&    (1-p-\epsilon +2\epsilon p) \log_2 \frac{1}{1-p-\epsilon +2\epsilon p} + 
           (\epsilon +p-2\epsilon p)   \log_2 \frac{1}{\epsilon +p-2\epsilon p} \\&&
           - (1-\epsilon ) \log_2 \frac{1}{1-\epsilon } +
           - \epsilon      \log_2 \frac{1}{\epsilon   }
\\
  \iC       &=&    1  + \epsilon \log_2 \epsilon  + (1-\epsilon ) \log_2 (1-\epsilon )
\end{array}}
\end{theorem}
\begin{proof}
\begin{eqnarray*}
  \pP{\rvX=1} &\eqd& p    \\
  \pP{\rvX=0} &=& 1-p     \\
  \pP{\rvY=1}
    &=& \pP{\rvY=1|\rvX=0}\pP{\rvX=0} + \pP{\rvY=1|\rvX=1}\pP{\rvX=1}
  \\&=& \epsilon(1-p) + (1-\epsilon)p
  \\&=& \epsilon-\epsilon p + p-\epsilon p
  \\&=& \epsilon+p-2\epsilon p
\\
  \pP{\rvY=0} 
    &=& \pP{\rvY=0|\rvX=0}\pP{\rvX=0} + \pP{\rvY=0|\rvX=1}\pP{\rvX=1}
  \\&=& (1-\epsilon )(1-p) + \epsilon p
  \\&=& 1-p-\epsilon +\epsilon p+\epsilon p
  \\&=& 1-p-\epsilon +2\epsilon p
\\
\\
  \iH(\rvX)
    &\eqd& \pEx \log_2 \frac{1}{\pp(\rvX)}
  \\&=&    \sum_{n=0}^1 \pP{\rvX=n} \log_2 \frac{1}{\pP{\rvX=n}}
  \\&=&    \pP{\rvX=0} \log_2 \frac{1}{\pP{\rvX=0}} +
           \pP{\rvX=1} \log_2 \frac{1}{\pP{\rvX=1}}
  \\&=&    p     \log_2 \frac{1}{p} +
           (1-p) \log_2 \frac{1}{(1-p)}
\\
\\
  \iH(\rvY)
    &\eqd& \pEy \log_2 \frac{1}{\pp(\rvY)}
  \\&=&    \sum_{n=0}^1 \pP{\rvY=n} \log_2 \frac{1}{\pP{\rvY=n}}
  \\&=&    \pP{\rvY=0} \log_2 \frac{1}{\pP{\rvY=0}} + 
           \pP{\rvY=1} \log_2 \frac{1}{\pP{\rvY=1}}
  \\&=&    (1-p-\epsilon +2\epsilon p) \log_2 \frac{1}{1-p-\epsilon +2\epsilon p} + 
           (\epsilon +p-2\epsilon p)   \log_2 \frac{1}{\epsilon +p-2\epsilon p}
\\
\\
  \iH(\rvY|\rvX)
    &\eqd& \pExy \log_2 \frac{1}{\pp(\rvY|\rvX)}
  \\&=&    \sum_{m=0}^1\sum_{n=0}^1 \pP{\rvX=m,\rvY=n} \log_2 \frac{1}{\pP{\rvY=n|\rvX=m}}
  \\&=&    \sum_{m=0}^1\sum_{n=0}^1 \pP{\rvY=n|\rvX=m}\pP{\rvX=m} 
           \log_2 \frac{1}{\pP{\rvY=n|\rvX=m}}
  \\&=&    \pP{\rvY=0|\rvX=0}\pP{\rvX=0} \log_2 \frac{1}{\pP{\rvY=0|\rvX=0}} + \\&&
           \pP{\rvY=0|\rvX=1}\pP{\rvX=1} \log_2 \frac{1}{\pP{\rvY=0|\rvX=1}} + \\&&
           \pP{\rvY=1|\rvX=0}\pP{\rvX=0} \log_2 \frac{1}{\pP{\rvY=1|\rvX=0}} + \\&&
           \pP{\rvY=1|\rvX=1}\pP{\rvX=1} \log_2 \frac{1}{\pP{\rvY=1|\rvX=1}} 
  \\&=&    (1-\epsilon ) (1-p) \log_2 \frac{1}{1-\epsilon } +
           \epsilon      p     \log_2 \frac{1}{\epsilon   } +
           \epsilon      (1-p) \log_2 \frac{1}{\epsilon   } +
           (1-\epsilon ) p     \log_2 \frac{1}{1-\epsilon } 
  \\&=&    (1-p-\epsilon +\epsilon p+p-\epsilon p) \log_2 \frac{1}{1-\epsilon } +
           (\epsilon p+\epsilon -\epsilon p)   \log_2 \frac{1}{\epsilon   }
  \\&=&    (1-\epsilon ) \log_2 \frac{1}{1-\epsilon } +
           \epsilon      \log_2 \frac{1}{\epsilon   }
\\
\\
  \iI(\rvX;\rvY)
    &=& \iH(\rvY) - \iH(\rvY|\rvX)
  \\&=&    (1-p-\epsilon +2\epsilon p) \log_2 \frac{1}{1-p-\epsilon +2\epsilon p} + 
           (\epsilon +p-2\epsilon p)   \log_2 \frac{1}{\epsilon +p-2\epsilon p}
           - (1-\epsilon ) \log_2 \frac{1}{1-\epsilon } +
           - \epsilon      \log_2 \frac{1}{\epsilon   }
\\
\\
  \iC
    &\eqd& \max_p \iI(\rvX;\rvY)
  \\&=&    \left. \iI(\rvX;\rvY) \right|_{p=\frac{1}{2}}
  \\&=&    \frac{1}{2} \log_2 \frac{1}{\frac{1}{2}} + 
           \frac{1}{2} \log_2 \frac{1}{\frac{1}{2}}
           - (1-\epsilon ) \log_2 \frac{1}{1-\epsilon } +
           - \epsilon      \log_2 \frac{1}{\epsilon   }
  \\&=&    1  + \epsilon \log_2 \epsilon  + (1-\epsilon ) \log_2 (1-\epsilon )
\end{eqnarray*}
\end{proof}

%---------------------------------------
\begin{remark}
%---------------------------------------
\mbox{}\\\rembox{%
  \begin{array}{MM}
      When $\epsilon =0$  &(noiseless channel), the channel capacity is $1$ bit (maximum capacity).
    \\When $\epsilon=1$   &(inverting channel), the channel capacity is still $1$ bit.
    \\When $\epsilon=1/2$ &(totally random channel), the channel capacity is $0$.
    \\When $p=1$ ($1$     &is always transmitted), the entropy of $\rvX$ is $0$.
    \\When $p=0$ ($0$     &is always transmitted), the entropy of $\rvX$ is $0$.
    \\When $p=1/2$        &(totally random transmission), the entropy of $\rvX$ is 1 bit (maximum entropy).
  \end{array}}
\end{remark}

%=======================================
\subsection{Gaussian Noise Channel}
%=======================================

\begin{figure}[ht] \color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.20mm}                  
\begin{picture}(700,150)(-100,-50) 
  \thicklines                                      
  %\graphpaper[10](0,0)(500,100)                  
  \put(-100 ,  60 ){\makebox( 100,0)[b]{$\su$} }
  \put(-100 ,  50 ){\vector(1,0){100} }

  \put(  00 ,  10 ){\makebox( 100, 80)[t]{transmit} }
  \put(  00 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put(  00 ,  10 ){\makebox( 100, 80)[b]{$\opT$} }
  \put(  00 ,  00 ){\framebox( 100,100){} }

  \put( 100 ,  60 ){\makebox( 100,0)[b]{$\rvX$} }
  \put( 100 ,  50 ){\vector(1,0){140} }


  \put( 200 ,  00 ){\makebox(100, 95)[t]{$Z$} }
  \put( 260,   50 ){\line  (1,0){ 45} }
  \put( 250 ,  80 ){\vector(0,-1){20} }
  \put( 250,   50) {\circle{20}                   }
  \put( 200 ,  00 ){\dashbox(100,100){$+$} }
  \put( 200 ,  10 ){\makebox(100, 90)[b]{channel $\opC$} }

  %\put( 200 ,  10 ){\makebox( 100, 80)[t]{channel} }
  %\put( 200 ,  10 ){\makebox( 100, 80)[c]{operation} }
  %\put( 200 ,  10 ){\makebox( 100, 80)[b]{\opC} }
  %\put( 200 ,  00 ){\framebox(100,100){} }

  \put( 300 ,  60 ){\makebox( 100,0)[b]{$\rvY$} }
  \put( 300 ,  50 ){\vector(1,0){100} }

  \put( 400 ,  00 ){\framebox(100,100){} }
  \put( 400 ,  10 ){\makebox( 100, 80)[t]{receive} }
  \put( 400 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put( 400 ,  10 ){\makebox( 100, 80)[b]{$\opR$} }

  \put( 500 ,  60 ){\makebox( 100,0)[b]{$\sue$} }
  \put( 500 ,  50 ){\vector(1,0){100} }

  %\put(- 90 , -10 ){\makebox( 0, 0)[tl]{$\vu\eqd\su$} }
  %\put( 110 , -10 ){\makebox( 0, 0)[tl]{$s(t;\vu)=\opT\vu$} }
  %\put( 310 , -10 ){\makebox( 0, 0)[tl]{$r(t;\vu)=\opC\opT\vu$} }
  %\put( 510 , -10 ){\makebox( 0, 0)[tl]{$\sue=\opR\opC\opT\vu$} }

\end{picture}                                   
\end{fsL}
\end{center}
\caption{
   Additive noise system model
   \label{fig:i_addNoise_model}
   }
\end{figure}


%--------------------------------------
\begin{theorem}
%--------------------------------------
Let $Z\sim\pN{0}{\sigma^2}$. Then
\thmbox{  \iH(Z) = \frac{1}{2}\log_2 2\pi e \sigma^2  }
\end{theorem}
\begin{proof}
\begin{align*}
  \iH(Z)
    &= \pEz \log \frac{1}{\pp(Z)}
  \\&= -\pEz \log \pp(z) 
  \\&= -\pEz
        \log \left[\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-z^2}{2\sigma^2}} \right] 
  \\&= -\pEz \left[
       -\frac{1}{2}\log(2\pi\sigma^2) 
       + \frac{-z^2}{2\sigma^2} \log e 
       \right] 
  \\&= \frac{1}{2} \pEz \left[
       \log(2\pi\sigma^2) 
       + \frac{\log e}{\sigma^2}z^2  
       \right] 
  \\&= \frac{1}{2} \left[
       \log(2\pi\sigma^2) + \frac{\log e}{\sigma^2}\pEz z^2  
       \right] 
  \\&= \frac{1}{2} \left[
       \log(2\pi\sigma^2) + \frac{\log e}{\sigma^2}(\sigma^2+0)
       \right] 
  \\&= \frac{1}{2} \left[
       \log(2\pi\sigma^2) + \log e
       \right] 
  \\&= \frac{1}{2} \log(2\pi e\sigma^2) 
\end{align*}
\end{proof}

%--------------------------------------
\begin{theorem}
\footnote{
  \citerp{cover}{241}
  }
%--------------------------------------
Let $\rvY=\rvX+Z$ be a Gaussian channel with $\pE \rvX^2=P$ and
$Z\sim\pN{0}{\sigma^2}$. Then
\thmbox{ 
  \iI(\rvX;\rvY) \le \frac{1}{2}\log\left( 1 + \frac{P}{\sigma^2}\right) = \iC 
  \hspace{1cm}\mbox{bits per usage}
  }
\end{theorem}

%--------------------------------------
\begin{theorem}
\footnote{
  \citerp{cover}{250}
  }
%--------------------------------------
Let $\rvY=\rvX+Z$ be a bandlimited Gaussian channel with $\pE \rvX^2=P$ and
$Z\sim\pN{0}{\sigma^2}$ and bandwidth $W$. Then
\thmbox{ 
  \iC = W \log\left( 1 + \frac{P}{\sigma^2 W}\right) 
  \qquad\mbox{bits per second}
  }
\end{theorem}
%\begin{proof}
%By \prefpp{thm:nst}, $R_c \le 2W$.
%No complete proof at this time. \attention
%
%\end{proof}




