%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%=======================================
\chapter{Moment Estimation}
%=======================================
%=======================================
\section{Mean Estimation}
%=======================================
%---------------------------------------
\begin{theorem}
\label{thm:mse_mean}
%---------------------------------------
Let $\ds\meanest\eqd\sum_{n=1}^\xN \lambda_n \rvx_n$ with $\ds\sum_{n=1}^{\xN}\lambda_n=1$
be the \fncte{arithmetic mean} \xref{def:am}.
\thmbox{%
  \brb{\begin{array}{FMD}
      (A).& $\seqn{\rvx_n}$ is \prope{wide sense stationary} & and
    \\(B).& $\mean\eqd\pE\rvx_n$                             & and
    \\(C).& $\seqn{\rvx_n}$ is \prope{uncorrelated}          & and
    \\(D).& \mc{2}{M}{$\ds\meanest\eqd\sum_{n=1}^\xN \lambda_n \rvx_n$\qquad(\fncte{arithmetic mean})}
  \end{array}}
  \implies
  \brb{\begin{array}{FrclDD}
      (1).& \pE\meanest    &=& \mean                               & (\prope{unbiased})   & and
    \\(2).& \var(\meanest) &=& \mc{2}{l}{\ds\pvar\sum_{n=1}^{\xN}\lambda_n^2 }         & and
    \\(3).& \mse(\meanest) &=& \mc{2}{l}{\ds\pvar\sum_{n=1}^{\xN}\lambda_n^2 }         &
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \pE\meanest
    &\eqd \pE \sum_{n\in\Z} \lambda_n\rvx_n
    && \text{by definition of \fncte{arithmetic mean}}
    && \text{\xref{def:am}}
  \\&=  \sum_{n\in\Z} \lambda_n\pE\rvx_n
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&=  \mean\sum_{n\in\Z}\lambda_n
    && \text{by \prope{WSS} hypothesis}
    && \text{(A)}
  \\&=  \mean
    && \text{by $\sum\lambda_n=1$ hypothesis}
    && \text{\xref{def:am}}
  \\
  \var(\meanest)
    &\eqd \pE\brp{\meanest-\pE\meanest}^2
    && \text{by definition of \fncte{variance}}
  \\&= \pE\brp{\meanest-\mean}^2
    && \text{by previous result}
  \\&= \pE\brp{\sum_{n=1}^{\xN}\lambda_n\rvx_n -\mean}^2
    && \text{by definition of $\meanest$}
  \\&= \pE\brs{\sum_{n=1}^{\xN}\lambda_n\rvx_n -\mean\mcom{\sum_{n=1}^{\xN}\lambda_n}{$1$}}^2
    && \text{by $\sum\lambda_n=1$ hypothesis}
    && \text{\xref{def:am}}
  \\&= \pE\brs{\sum_{n=1}^{\xN}\lambda_n(\rvx_n-\mean)}^2
  \\&= \pE\brs{\sum_{n=1}^{\xN}\lambda_n(\rvx_n-\mean)\sum_{m=1}^{\xN}\lambda_m(\rvx_m-\mean)}
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{(\rvx_n-\mean)(\rvx_m-\mean)}                                     }
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean\pE\brs{\rvx_n}-\mean\pE\brs{\rvx_m}+\mean^2   }
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean^2-\mean^2+\mean^2                             }
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean^2                                             }
  \\&= \mathrlap{
       \sum_{n=1}^{\xN} \lambda_n^2 \brp{\pE\brs{\rvx_n^2    }-\mean^2}
     + \sum_{n=1}^{\xN}\sum_{m\neq n }\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean^2 }
       }
  \\&= \mathrlap{
       \sum_{n=1}^{\xN} \lambda_n^2 \brp{\pE\brs{\rvx_n^2    }-\mean^2                                             }
     + \sum_{n=1}^{\xN}\sum_{m\neq n  }\lambda_n\lambda_m\brp{\pE\rvx_n\pE\rvx_m  -\mean^2}
       }
  \\&= \mathrlap{
       \sum_{n=1}^{\xN} \lambda_n^2 \pvar
     + \cancelto{0}{
       \sum_{n=1}^{\xN}\sum_{m\neq n  }\lambda_n\lambda_m\brp{\mean\mean-\mean^2}
       }}
    && \text{by \prope{WSS} hypothesis}
    && \text{(A)}
  \\&= \pvar\sum_{n=1}^{\xN} \lambda_n^2
  \\
  \mse(\meanest)
    &= \pE\brp{\meanest-\pE\meanest}^2
        +\brp{\pE\meanest-\mean}^2
    && \text{by \prefp{thm:mse}}
  \\&= \pvar\sum_{n=1}^{\xN}\lambda_n^2 + \brp{\mean-\mean}^2
    && \text{by previous results}
  \\&= \pvar\sum_{n=1}^{\xN}\lambda_n^2
\end{align*}
\end{proof}

%---------------------------------------
\begin{definition}
\label{def:average}
%---------------------------------------
\defboxt{
  The \fnctd{average} $\meanest$ of a length $\xN$ sequence $\tuplexN{\rvx_n}$ is defined as
  \qquad$\ds\meanest\eqd\frac{1}{\xN}\sum_{n=1}^\xN \rvx_n$
  }
\end{definition}

%---------------------------------------
\begin{corollary}
\footnote{
  \citerpgc{kay1988}{45}{8131733564}{\textsection\scshape``3.3 Estimation Theory"}
  }
\label{cor:mse_average}
%---------------------------------------
\corbox{
  \brb{\begin{array}{FMD}
      (A).& $\seqn{\rvx_n}$ is \prope{wide sense stationary} & and
    \\(B).& $\mean\eqd\pE\rvx_n$                             & and
    \\(C).& $\seqn{\rvx_n}$ is \prope{uncorrelated}          & and
    \\(D).& \mc{2}{M}{$\ds\meanest\eqd\frac{1}{\xN}\sum_{n=1}^\xN \rvx_n$\qquad(\fncte{average})}
  \end{array}}
  \implies
  \brb{\begin{array}{FrclDD}
      (1).& \pE\meanest    &=& \mean                & (\prope{unbiased})   & and
    \\(2).& \var(\meanest) &=& \frac{\pvar}{\xN} &                      & and
    \\(3).& \mse(\meanest) &=& \frac{\pvar}{\xN} & (\prope{consistent})
  \end{array}}
  }
\end{corollary}
\begin{proof}
These results follow from \prefpp{thm:mse_mean} with $\lambda_n=\frac{1}{\xN}$.
\end{proof}


The \fncte{arithmetic mean} estimator $\meanest\eqd\sum\lambda_n\rvx_n$
is \prope{unbiased} and \prope{consistent} for any $\sum\lambda_n=1$ and
yields \fncte{mean square error} $\mse(\meanest)=\pvar\sum\lambda_n^2$
\xref{thm:mse_mean}.
But\ldots
\begin{enume}
  \item Said qualitatively: ``What is the 'best' sequence $\seqn{\lambda_n}$ to use?"
  \item Said quantitatively: ``What sequence $\seqn{\lambda_n}$ yields the smallest $\mse(\meanest)$?"
\end{enume}
For example, would fashioning $\seqn{\lambda_n}$ to be a scaled version of a standard 
window function, like the \fncte{Hanning window}\footnote{
  \citerp{abdaheer2009}{130}
  }
illustrated below, yield the best $\mse(\meanest)$?
\\\indentx\includegraphics{graphics/hanninglp50.pdf}
\\
\prefpp{thm:lambda_sq} answers question (2) stating
that the best sequence in terms of minimal $\mse$
is the sequence $\seqn{\lambda_n}\eqd\frac{1}{\xN}\seqn{\ldots,1,1,1,\ldots}$,
which is the \fncte{average} estimator,
which yields  $\mse(\meanest)= \frac{\pvar}{\xN}$ \xref{cor:mse_average}.
\\\indentx\includegraphics{graphics/rectangular_1N.pdf}
\\
That is, it turns out that $\frac{1}{\xN}\le\sum\lambda_n^2$ for all possible sequences $\seqn{\lambda_n}$.
This fact is demonstrated by \pref{lem:lambda_sq} (next),
which in turn follows more or less directly from the ubiquitous 
\ineqe{Cauchy-Schwarz Inequality} \xxref{thm:seq_cs}{thm:cs}.

Even further strengthening the average as choice estimator is \prefpp{cor:mean_white_gaussian}
which demonstrates that in the case where $\seqn{\rvx_n$} is \prope{uncorrelated} and 
\prope{Gaussian}, then the optimal maximum likelihood estimator is the average.

%---------------------------------------
\begin{lemma}
\label{lem:lambda_sq}
%---------------------------------------
\lembox{
  \brb{\sum_{n=1}^{\xN} \lambda_n=1}
  \qquad\implies\qquad
  \brb{\frac{1}{\xN} \le \sum_{n=1}^{\xN} \lambda_n^2}
  }
\end{lemma}
\begin{proof}
\begin{enumerate}
  \item Let the sequence $\seqn{a_n}$ be defined as $\seqn{a_n}\eqd\seqn{\ldots,1,1,1,\ldots}$
        \label{idef:lambda_sq_an}
  \item Let \fncte{inner product} $\inprod{a_n}{b_n}$ be defined as $\inprod{a_n}{b_n}\eqd\sum_{n=1}^{\xN}a_n b_n$
        \label{idef:lambda_sq_inprod}
  \item Let \fncte{norm} $\norm{a_n}$ be defined as $\norm{a_n}\eqd\sum_{n=1}^{\xN}a_n^2$
        \label{idef:lambda_sq_norm}
  \item Proof of lemma:
        \begin{align*}
          \boxed{\frac{1}{\xN}}
            &=    \frac{1}{\xN}\brp{\sum_{n=1}^{\xN} \lambda_n}^2
            &&    \text{by $\sum_{n=1}^{\xN} \lambda_n=1$ hypothesis}
          \\&=    \frac{1}{\xN}
                  \brp{\sum_{n=1}^{\xN} a_n\lambda_n}^2
            &&    \text{by $\seqn{a_n}\eqd\seqn{\ldots,1,1,1,\ldots}$ definition}
            &&    \text{\xref{idef:lambda_sq_an}}
          \\&\boxed{\le} \frac{1}{\xN}
                  \brp{\sum_{n=1}^{\xN} a_n^2} 
                  \brp{\sum_{n=1}^{\xN} \lambda_n^2}
            &&    \text{by \ineqe{Cauchy-Schwartz inequality}}
            &&    \text{\xref{thm:seq_cs}}
          \\&\eqd \frac{1}{\xN}\brp{\sum_{n=1}^{\xN} 1^2} \brp{\sum_{n=1}^{\xN} \lambda_n^2}
            &&    \text{by definition of $\seqn{a_n}$}
            &&    \text{\xref{idef:lambda_sq_an}}
          \\&=    \boxed{\sum_{n=1}^{\xN} \lambda_n^2}
        \end{align*}
\end{enumerate}
\end{proof}

%---------------------------------------
\begin{theorem}
\label{thm:lambda_sq}
%---------------------------------------
Let $\mse(\text{average mean})$ be the mean square error of the \fncte{average} estimator \xref{cor:mse_average}
and $\mse(\text{arithmetic mean})$ be the mean square error of the \fncte{arithmetic} estimator \xref{thm:mse_mean}.
\propbox{
  \mse(\text{average mean}) \le \mse(\text{arithmetic mean})
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \mse(\text{average mean}) 
    &= \pvar\frac{1}{\xN}
    && \text{by \prefp{cor:mse_average}}
  \\&\le \pvar \sum_{n=1}^{\xN} \lambda_n^2
    && \text{by \prefp{lem:lambda_sq}}
  \\&= \mse(\text{arithmetic mean})
    && \text{by \prefp{thm:mse_mean}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{corollary}
\label{cor:mean_white_gaussian}
%---------------------------------------
\corbox{
  \brb{\begin{array}{FMD}
      (A). & $\seqn{\rvx_n}$ is \prope{uncorrelated} & and
    \\(B). & $\rvx_n$ is \prope{Gaussian}
  \end{array}}
  \implies
  \brb{\begin{array}{FMD}
      (1). & $\ds\estML[\mu] = \frac{1}{\xN}\sum_{n=1}^{\xN} \rvx_n$ & and
    \\(2). & $\estML[\mu]$ is \prope{consistent}              & and
    \\(3). & $\estML[\mu]$ is \prope{efficient}               &
  \end{array}}
  }
\end{corollary}
\begin{proof}
This result follows directly from \prefpp{thm:estML-CR} with
\\\indentx$\begin{array}{rc>{\ds}lM}
   \rvy(t)          &\eqd& \rvx(t;\theta) + \fv(t) & where $\fv(t)$ is a \prope{zero-mean} white Gaussian noise process
   \\\rvx(t;\theta) &\eqd& \fg(\theta)
   \\               &\eqd& \theta
   \\               &\eqd& \mu
   \\\rvx_n         &\eqd& \fdoty_n
   \\               &\eqd& \inprod{\fy(t)}{\fpsi_n(t)}
   \\               &\eqd& \inprod{\fy(t)}{\delta(t-n\tau)}
   \\               &\eqd& \int_{t\in\R} \fy(t) \delta(t-n\tau) \dt
   \\               &=&    \fy(n\tau)
\end{array}$
\end{proof}

%=======================================
\section{Variance Estimation}
%=======================================
If we know the true \vale{mean} $\mean$ of a stationary random process $\seqn{\rvx_n}$, 
then a reasonable estimate of the variance might be 
$\frac{1}{\xN}\sum_{n=1}^{\xN} \brp{\rvx_n-\mean}^2$.
This estimate has the highly touted property of being \prope{unbiased}:
\\\indentx$\ds
 \pE\brs{\frac{1}{\xN}\sum_{n=1}^{\xN} \brp{\rvx_n-\mean}^2}
   =     \frac{1}{\xN}\sum_{n=1}^{\xN} \pE\brs{\brp{\rvx_n-\mean}^2}
   =     \frac{1}{\xN}\sum_{n=1}^{\xN} \pvar
   =     \pvar
$\\
Very good. 
However, in many cases we don't know the \textbf{true mean} $\mean$,
but rather only have an \textbf{estimated mean} $\meanest\eqd\frac{1}{\xN}=\sum_{n=1}^{\xN}\rvx_n$.
In this case, substituting in the estimated mean for the true mean as in 
$\varest_B\brp{\seqn{\rvx_n}}$ (next definition) yields a \prope{biased} variance estimate (next theorem).

%---------------------------------------
\begin{definition}
\footnote{
  \citerpc{wilks1962}{199}{\textsection ``8.2 \scshape Means and Variances of Mean, Variance,\ldots"},
  \citerpc{wilks1963}{199}{\textsection ``(b) Mean and Variance of Sample Variance"},
  \citerpc{kenney1942}{125}{``Bessel's correction"},
  \citerpc{bajpai1967}{509}{???}
  }
\label{def:varest}
%---------------------------------------
Let $\meanest$ be an estimate of the mean of a random sequence $\seqn{\rvx_n}$.
\defbox{\begin{array}{lrc>{\ds}l}
   \begin{array}{M}
     The \fnctd{sample variance} $\varest\brp{\seqn{\rvx_n}}$\\ 
     is defined as 
   \end{array}
     & \varest\brp{\seqn{\rvx_n}} &\eqd& \frac{1}{\xN-1}\sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2
     \\
   \begin{array}{M}
     The \fnctd{biased sample variance} $\varest_B\brp{\seqn{\rvx_n}}$\\
     is here defined as 
   \end{array}
     &  \varest_B\brp{\seqn{\rvx_n}} &\eqd& \frac{1}{\xN}  \sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2
\end{array}}
\end{definition}

The factor $\frac{\xN}{\xN-1}$ such that 
$\varest\brp{\seqn{\rvx_n}} = \frac{\xN}{\xN-1}\varest_B\brp{\seqn{\rvx_n}}$ 
is known as ``\vale{Bessel's correction}".
Why such ``correction" would be useful at all is demonstrated by \pref{thm:varest} (next).
\pref{thm:varest} demonstrates that the \fncte{biased sample variance} $\varest_B\brp{\seqn{\rvx_n}}$
is \prope{biased}, and multiplication by $\frac{\xN}{\xN-1}$ makes 
it \prope{unbiased}.
%---------------------------------------
\begin{theorem}
\footnote{
  \citerpc{wilks1962}{199}{\textsection ``8.2 \scshape Means and Variances of Mean, Variance,\ldots"},
  \citerpgc{tucker1962}{111}{}{\textsection ``8.2 Unbiased and Consistent Estimates"},
  \citerpgc{stuart1991}{609}{9780340560235}{\textsection ``Unbiased estimators"}
  }
\label{thm:varest}
%---------------------------------------
Let $\meanest$ be the \fncte{average} \xref{def:average} of a sequence $\seqn{\rvx_n}$.
Let $\mu^4\eqd\pE\brs{\brp{\rvx_n-\mean}^4}$ be the \fncte{4th central moment} of $\rvx_n$.
\thmbox{
  \brbr{\begin{array}{FMD}
      (A).& $\seqn{\rvx_n}$ is \prope{WSS}          & and
    \\(B).& $\mean\eqd\pE\rvx_n$                    & and
    \\(C).& \mc{2}{M}{$\seqn{\rvx_n}$ is \prope{uncorrelated}}
  \end{array}}
  \implies
  \brbl{\begin{array}{FrclDD}
      (1).& \pE\varest_B\seqn{\rvx_n}  &=& \ds\frac{\xN-1}{\xN}\pvar            & (\prope{biased})   & and
    \\(2).& \pE\varest  \seqn{\rvx_n}  &=& \ds                 \pvar            & (\prope{unbiased}) & and
    \\(3).& \var\brs{\varest\seqn{\rvx_n}} &=& \mc{2}{l}{\ds\frac{1}{\xN}\brs{\mu^4 - \brp{\frac{\xN-3}{\xN-1}}\sigma^4}} & (\prope{consistent})
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item lemma: $\pE(\rvx_n\meanest) = \frac{1}{\xN}\pvar+\mean^2$. Proof: \label{ilem:varest_xmu}
    \begin{align*}
      \pE\brp{\rvx_n\meanest}
        &\eqd \pE\brp{\rvx_n\frac{1}{\xN}\sum_{m=1}^{\xN}\rvx_m}
        && \text{by definition of \fncte{average}}
        && \text{\xref{def:average}}
      \\&= \pE\brp{\frac{1}{\xN}\sum_{m=1}^{\xN}\rvx_n\rvx_m}
      \\&= \frac{1}{\xN}\sum_{m=1}^{\xN}\pE\brp{\rvx_n\rvx_m}
        && \text{by \prope{linearity} of $\pE$}
        && \text{\xref{thm:pE_linop}}
      \\&= \frac{1}{\xN}\brs{\pE\rvx_n^2 + \sum_{m\neq n}\pE\brp{\rvx_n\rvx_m}}
      \\&= \frac{1}{\xN}\brs{\pE\rvx_n^2 + \sum_{m\neq n}\brp{\pE\rvx_n}\brp{\pE\rvx_m}}
        && \text{by \prope{uncorrelated} hypothesis}
        && \text{(C)}
      \\&= \frac{1}{\xN}\brs{\brp{\pvar+\mean^2} + (\xN-1)\mean^2}
        && \text{by \prefpp{cor:pVar}}
      \\&= \frac{1}{\xN}\pvar + \mean^2
    \end{align*}

  \item Proof for (1): \label{item:varest_B}
        \begin{align*}
          \pE\varest_B\brp{\seqn{\rvx_n}}
            &\eqd \pE\brs{\frac{1}{\xN}  \sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2}
            && \text{by definition of $\varest_B$ 
                     \xref{def:varest}}
          \\&= \frac{1}{\xN} \pE\brs{ \sum_{n=1}^{\xN} \brp{\rvx_n \mcom{-\mean + \mean}{$0$} - \meanest}^2}
          \\&= \mathrlap{
               \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \mcom{\pE\brp{\rvx_n -\mean}^2}{$\pvar$} + 2\pE\brs{(\rvx_n -\mean)(\mean - \meanest)} + \pE\brp{\mean - \mcom{\meanest}{$\pE\mean$}}^2 }
               }
          \\&= \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \pvar + 2\pE\brs{\rvx_n\mean - \rvx_n\meanest -\mean^2 + \mean\meanest} + \frac{1}{\xN}\pvar }
          \\&= \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \pvar + 2\brs{\mean^2 - \pE(\rvx_n\meanest) -\mean^2 + \mean^2} + \frac{1}{\xN}\pvar }
            && \mathrlap{\text{by \prefp{cor:mse_average}}}
          \\&= \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \pvar + 2\brs{\mean^2 - \brp{\mean^2 + \frac{1}{\xN}\pvar}} + \frac{1}{\xN}\pvar }
            && \text{by \prope{unbiased} property of $\meanest$}
            && \text{\xref{ilem:varest_xmu}}
          \\&= \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \pvar - \frac{1}{\xN}\pvar} 
          \\&= \frac{\xN-1}{\xN}\pvar
        \end{align*}

  \item Proof for (2):  \label{item:varest}
    \begin{align*}
      \pE\varest\brp{\seqn{\rvx_n}}
        &\eqd \pE\brs{\frac{1}{\xN-1}  \sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2}
        && \text{by definition of $\varest$}
        && \text{\xref{def:varest}}
      \\&= \frac{\xN}{\xN-1}\pE\brs{  \frac{1}{\xN}  \sum_{n=1}^{\xN-1} \brp{\rvx_n - \meanest}^2}
        && \text{by \prope{linearity} of $\pE$}
        && \text{\xref{thm:pE_linop}}
      \\&= \frac{\xN}{\xN-1}\brs{\frac{\xN-1}{\xN}\pvar}
        && \text{by $\varest_B$ result}
        && \text{\xref{item:varest_B}}
      \\&= \pvar
    \end{align*}

  \item lemma: $\pE\brs{\brp{\varest}^2}=\frac{\mu^4}{\xN} + \frac{(\xN-1)^2+2}{\xN(\xN-1)} \sigma^4$. 
        \label{ilem:varest_varvar}
        Proof:
        No proof here at this time.
        The assertion is made by \citerp{wilks1962}{199} who also without there supplying a proof 
        says, 
        ``Carrying out similar mean value operations we find after some reduction that" the result follows. 
    {\begin{align*}
      \pE\brs{\brp{\varest}^2}
        &\eqd \pE\brs{\brp{\frac{1}{\xN-1}\sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2}^2}
        \qquad \text{by definition of $\varest$ \xref{def:varest}}
      \\&= \pE\brs{
           \brp{\frac{1}{\xN-1}\sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2}
           \brp{\frac{1}{\xN-1}\sum_{m=1}^{\xN} \brp{\rvx_m - \meanest}^2}
           }
      \\&= \brp{\frac{1}{\xN-1}}^2\pE\brs{
           \sum_{n=1}^{\xN} \sum_{m=1}^{\xN} 
           \brp{\rvx_n - \meanest}^2 
           \brp{\rvx_m - \meanest}^2
           }
      \\&= \brp{\frac{1}{\xN-1}}^2\pE\brs{
           \sum_{n=1}^{\xN} \sum_{m=1}^{\xN} 
           \brp{\rvx_n^2 -2\rvx_n\meanest + \meanest^2}
           \brp{\rvx_m^2 -2\rvx_m\meanest + \meanest^2}
           }
      \\&= \brp{\frac{1}{\xN-1}}^2\pE\brs{
           \sum_{n=1}^{\xN} \sum_{m=1}^{\xN} 
           \brs{\begin{array}{c@{\hspace{2pt}}l}
               & \brp{\rvx_n^2\rvx_m^2 -2\meanest\rvx_n^2\rvx_m + \meanest^2\rvx_n^2}
              +  \brp{-2\meanest\rvx_n\rvx_m^2 +4\meanest^2\rvx_n\rvx_m -2\rvx_n\meanest^3}
            \\+&\brp{\meanest^2\rvx_m^2 -2\meanest^3\rvx_m + \meanest^4}
           \end{array}}
           }
      \\&= \brp{\frac{1}{\xN-1}}^2
           \sum_{n=1}^{\xN} \sum_{m=1}^{\xN} 
           \brs{\begin{array}{c@{\hspace{2pt}}l}
                   &\pE\brs{          \rvx_n^2\rvx_m^2 }
                 -2 \pE\brs{\meanest  \rvx_n^2\rvx_m   }
                 -2 \pE\brs{\meanest  \rvx_n  \rvx_m^2 }
                 +  \pE\brs{\meanest^2\rvx_n^2         }
                 +  \pE\brs{\meanest^2\rvx_m^2         }
               \\+4&\pE\brs{\meanest^2\rvx_n\rvx_m     }
                 -2 \pE\brs{\meanest^3\rvx_n           }
                 -2 \pE\brs{\meanest^3\rvx_m           }
                 +  \pE\brs{\meanest^4                 }
           \end{array}}
      \\&= \brp{\frac{1}{\xN-1}}^2
           \sum_{n=1}^{\xN} \sum_{m=1}^{\xN} 
           \brs{\begin{array}{c@{\hspace{2pt}}l}
                   &\pE\brs{          \rvx_n^2\rvx_m^2 }
                 -4 \pE\brs{\meanest  \rvx_n^2\rvx_m   }
                 +2 \pE\brs{\meanest^2\rvx_n^2         }
               \\+4&\pE\brs{\meanest^2\rvx_n\rvx_m     }
                 -4 \pE\brs{\meanest^3\rvx_n           }
                 +  \pE\brs{\meanest^4                 }
           \end{array}}
      \\&\eqq \frac{\mu^4}{\xN} + \frac{(\xN-1)^2+2}{\xN(\xN-1)} \sigma^4
    \end{align*}}

  \item Proof for (3):
    {\begin{align*}
      \var\brs{\varest\seqn{\rvx_n}} 
        &\eqd \pE\brs{\brp{\varest-\pE\varest}^2}
        && \text{by definition of $\varest$}
        && \text{\xref{def:varest}}
      \\&= \pE\brs{\brp{\varest-\pvar}^2}
        && \text{by (2)}
        && \text{\xref{item:varest}}
      \\&= \pE\brs{\brp{\varest}^2 - 2\pvar\varest + \brp{\pvar}^2}
        && \text{by \thme{Binomial Theorem}}
        && \text{\ifxref{polynom}{thm:binomial}}
      \\&= \pE\brs{\brp{\varest}^2} - 2\pvar\pE\brs{\varest} + \pE\brs{\brp{\pvar}^2}
        && \text{by \prope{linearity} of $\pE$}
        && \text{\xref{thm:pE_linop}}
      \\&= \pE\brs{\brp{\varest}^2} - 2\brp{\pvar}^2 + \brp{\pvar}^2
        && \text{by (2)}
        && \text{\xref{item:varest}}
      \\&= \brs{\frac{\mu^4}{\xN} + \brp{\frac{(\xN-1)^2+2}{\xN(\xN-1)}} \sigma^4} - \sigma^4 
        && \text{by \pref{ilem:varest_varvar}}
      \\&= \mathrlap{\frac{1}{\xN}\brs{\mu^4 + \brp{\frac{(\xN^2-\xN+1)+2-\xN(\xN-1)}{\xN-1}} \sigma^4}}
      \\&= \frac{1}{\xN}\brs{\mu^4 - \brp{\frac{\xN-3}{\xN-1}}\sigma^4}
    \end{align*}}
\end{enumerate}
\end{proof}

%=======================================
\section{Estimates in terms of moment estimates}
%=======================================
%---------------------------------------
\begin{definition}
\label{def:momest}
%---------------------------------------
\defboxt{
  The \fnctd{order-k moment estimate} is here defined as
  \\\indentx$\ds
  \momest_k\seqn{\rvx_n} \eqd \frac{1}{\xN}\sum_{n=1}^{\xN} \rvx_n^k
  $
  }
\end{definition}

%---------------------------------------
\begin{proposition}
%---------------------------------------
\propbox{\begin{array}{rc>{\ds}l}
    \meanest\seqn{\rvx_n} &=& \momest_1
  \\\varest\seqn{\rvx_n}  &=& \frac{\xN}{\xN-1}\momest_2 - \frac{\xN}{\xN-1}\momest_1^2
\end{array}}
\end{proposition}
\begin{proof}
{\begin{align*}
  \meanest\seqn{\rvx_n} 
    &\eqd \frac{1}{\xN}\sum_{n=1}^{\xN} \rvx_n
    && \text{by definition of $\meanest$}
    && \text{\xref{def:average}}
  \\&\eqd \momest_1
    && \text{by definition of $\momest_1$}
    && \text{\xref{def:momest}}
    \\
  \\\varest\seqn{\rvx_n}  
    &\eqd \frac{1}{\xN-1}\sum_{n=1}^{\xN} \brp{\rvx_n-\meanest}^2
    && \text{by definition of $\varest$}
    && \text{\xref{def:varest}}
  \\&= \frac{1}{\xN-1}\sum_{n=1}^{\xN} \brp{\rvx_n^2 - 2\rvx_n\meanest +\brp{\meanest}^2}
  \\&=   \frac{1}{\xN-1}\sum_{n=1}^{\xN} \rvx_n^2 
      - 2\frac{1}{\xN-1}\sum_{n=1}^{\xN} \rvx_n\meanest 
      +  \frac{1}{\xN-1}\sum_{n=1}^{\xN} \brp{\meanest}^2
  \\&=   \frac{\xN}{\xN-1}\mcom{\frac{1}{\xN}\sum_{n=1}^{\xN} \rvx_n^2}{$\momest_2$}
      - 2\mcom{\meanest}{$\momest_1$}\frac{\xN}{\xN-1}\mcom{\frac{1}{\xN}\sum_{n=1}^{\xN} \rvx_n}{$\momest_1$}
      +  \frac{\xN}{\xN-1} \mcom{\brp{\meanest}^2}{$\momest_1^2$}
  \\&= \frac{\xN}{\xN-1}\momest_2 - \frac{\xN}{\xN-1}\momest_1^2
\end{align*}}
\end{proof}

%=======================================
\section{Recursive forms}
%=======================================
In software/firmware implementations, recursive forms are very useful and efficient.
%---------------------------------------
\begin{proposition}
\footnote{
  \citerppgc{candy2009}{11}{12}{9780470430576}{Example 1.3},
  \citerppgc{candy2016}{12}{13}{9781119125457}{Example 1.3}
  }
%---------------------------------------
\propbox{
  \mcom{\meanest_{\xN}}{new}
    = \mcom{\meanest_{\xN-1}}{previous} 
      \mcomr{\frac{1}{\xN}}{weight}\mcom{\brs{\rvy(\xN) - \meanest_{\xN-1}}}{error}
  }
\end{proposition}
\begin{proof}
\begin{align*}
  \mcom{\meanest_{\xN}}{new}
    &\eqd \frac{1}{\xN} \sum_{n=1}^{\xN} \rvx_n
    && \text{by definition of \fncte{average}}
    && \text{\xref{def:average}}
  \\&= \frac{1}{\xN}\rvx_{\xN} + \frac{1}{\xN} \sum_{n=1}^{\xN-1} \rvx_n
  \\&= \frac{1}{\xN}\rvx_{\xN} + \frac{\xN-1}{\xN}\brp{\frac{1}{\xN-1}} \sum_{n=1}^{\xN-1} \rvx_n
  \\&\eqd \frac{1}{\xN}\rvx_{\xN} + \frac{\xN-1}{\xN}\meanest_{\xN-1}
    && \text{by definition of \fncte{average}}
    && \text{\xref{def:average}}
  \\&= \frac{1}{\xN}\rvx_{\xN} + \meanest_{\xN-1} - \frac{1}{\xN}\meanest_{\xN-1}
  \\&= \mcom{\meanest_{\xN-1}}{previous} 
      \mcomr{\frac{1}{\xN}}{weight}\mcom{\brs{\rvy(\xN) - \meanest_{\xN-1}}}{error}
\end{align*}
\end{proof}