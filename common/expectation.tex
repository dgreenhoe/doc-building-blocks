%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%======================================
\chapter{Expectation operator}
\label{chp:stats}
%======================================
\qboxnps
  {Aristotle (384 BC -- 322 BC)
    \index{Aristotle}
    \index{quotes!Aristotle}
    \footnotemark
  }
  {../common/people/aristot.jpg}
  {A likely impossibility is always preferable to an
  unconvincing possibility.}
  \footnotetext{\begin{tabular}[t]{ll}
    quote: & \url{http://en.wikiquote.org/wiki/Aristotle} \\
    image: & \url{http://en.wikipedia.org/wiki/Aristotle}
  \end{tabular}}
%=======================================
\section{Definitions}
\label{sec:pE_defs}
\index{expectation operator}
%=======================================
In a \structe{probability space} $\ps$ \xref{def:ps}, all probability information
is contained in the \fncte{measure} $\psp$.
%(or equivalently in the pdf or cdf defined in terms of $\psp$).
Often times this information is overwhelming and a simpler statistic,
which does not offer so much information, is sufficient.
Some of the most common statistics can be conveniently expressed in terms
of the \ope{expectation operator} $\pE$.
%---------------------------------------
\begin{definition}
\label{def:pE}
%---------------------------------------
Let $\ps$ be a \structe{probability space} \xref{def:ps} and
$\rvX$ a \fncte{random variable} \xref{def:rv} on $\ps$ with
\fncte{probability density function} $\ppx$.
\defboxt{ 
  The \opd{expectation operator} $\pEx$ on $\rvX$ is defined as
  \\\indentx$\ds\pEx\rvX \eqd \int_{x\in\F} x \ppx(x) \dx$. 
}
\end{definition}

We already said that a \fncte{random variable} $\rvX$ is neither random nor a variable,
but is rather a function of an underlying process that does appear to be random.
However, because it is a function of a process that does appear random,
the \fncte{random variable} $\rvX$ also appears to be random.
That is, if we don't know the outcome of of the underlying experimental
process, then we also don't know for sure what $\rvX$ is, and so $\rvX$ does
indeed appear to be random.
However, eventhough $\rvX$ appears to be random,
the expected value $\pEx\rvX$  of $\rvX$ is {\bf not random}.
Rather it is a fixed value (like $0$ or $7.9$ or $-2.6$).

Two common statistics that are conveniently expressed in terms of the
expectation operator are the \hie{mean} and \hie{variance}.
The mean is an indicator of the ``middle" of a probability distribution and the
variance is an indicator of the ``spread".
%---------------------------------------
\begin{definition}
\label{def:Mx}
\label{def:pvar}
\label{def:pVar}
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} on the \structe{probability space} $\ps$.
\defbox{\begin{array}{FM>{\ds}rc>{\ds}l}
  (1).&The \fnctd{mean} $\pmeanx$ of $\rvX$ is 
      & \pmeanx  &\eqd& \pEx\rvX 
  \\
  (2).&The \fnctd{variance} $\pVar(\rvX)$ or $\pvarx$ of $\rvX$ is 
      & \pVar(\rvX) &\eqd& \pEx\left[(\rvX-\pEx\rvX)^2 \right]
\end{array}}
\end{definition}

%=======================================
\section{Expectation as a linear operator}
%=======================================
The next theorem demonstrates that the operator $\pE$ is a 
\ope{linear operator} \xref{def:linop}---which in turn
makes $\pE$ part of a distinguished club of operators along with fellow member operators
differentiation $\opDif$, integration $\int\dx$,
Laplace $\opLT$, Fourier $\opFT$, z-transform $\opZ$, etc.
Because $\pE$ is a linear operator, it immediately inherits all the properties 
that it's linear operator birthright grants it \xref{cor:pE_linop}. 
%---------------------------------------
\begin{theorem}[\thmd{Linearity of $\pE$}]
\footnote{
  \citerpgc{haykin2014}{107}{9781118476772}{``{\scshape Property} 1 Linearity"},
  \citerpc{wilks1963}{73}{\textsection 3.2 ``Mean value of a random variable"}
  }
\label{thm:pE}
\label{thm:pE_linop}
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} on a \structe{probability space} $\ps$.
\thmbox{\begin{array}{>{\ds}rc>{\ds}lCD}
    \pEx(a\rvX+b\rvY+c)&=& \brp{a\pEx\rvX} + \brp{b\pEx\rvY} + c& \forall a,b,c\in\R & (\prope{linear})
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \boxed{\pExy(a\rvX+b\rvY + c)}
    &\eqd \int_{x\in\R}\int_{y\in\R}\brs{ax+by+c} \pdfxy(x,y)  \dy\dx
    \qquad\text{by definition of $\pE$ \xref{def:pE}}
  \\&= \int_{x\in\R}\int_{y\in\R} ax \pdfxy(x,y)  \dy\dx
     + \int_{x\in\R}\int_{y\in\R} by \pdfxy(x,y)  \dy\dx
     + \int_{x\in\R}\int_{y\in\R} c  \pdfxy(x,y)  \dy\dx
     %&& \text{by \prope{lineararity} of $\int\int\dy\dx$}
     %&& \text{\xref{def:linop}}
  \\&=  \int_{x\in\R} ax \mcom{\int_{y\in\R}\pdfxy(x,y)  \dy}{$\pdfx(x)$}\dx
     +  \int_{y\in\R} by \mcom{\int_{x\in\R}\pdfxy(x,y)  \dx}{$\pdfy(y)$}\dy
     + c\mcom{\int_{y\in\R}\int_{x\in\R}\pdfxy(x,y)  \dx\dy}{$1$}
     %&& \text{by \prope{lineararity} of $\int\int\dy\dx$}
     %&& \text{\xref{def:linop}}
  \\&= a\mcom{\int_{x\in\R} x \pdfx(x) \dx}{$\pE\rvX$}
     + b\mcom{\int_{y\in\R} y \pdfy(y) \dy}{$\pE\rvY$}
     + c
  \\&= \boxed{\brp{a\pEx\rvX} + \brp{b\pEy\rvY} + c}
\end{align*}
\end{proof}

%---------------------------------------
\begin{corollary}
\label{cor:pE_linop}
%---------------------------------------
Let $\pE$ be the \ope{expectation operator} over a \structe{probability space} $\ps$.
Let $spLLF$ be a \structe{vector space of random variables} over $\ps$.
\corbox{
  \begin{array}{F>{\ds}lc>{\ds}lCD|}
      (1). & \pE\vzero    &=& \vzero               &                            & and
    \\(2). & \pE(-\rvX)    &=& -(\pE\rvX)          & \forall \rvX     \in\spLLF & and
    \\(3). & \pE(\rvX-\rvY) &=& \pE\rvX - \pE\rvY  & \forall \rvX,\rvY\in\spLLF & and
  \end{array}
  \begin{array}{F>{\ds}lc>{\ds}lCC}
      (4). & \pE\brp{\sum_{n=1}^\xN \alpha_n\rvX_n}  
           &=& \sum_{n=1}^\xN \alpha_n\brp{\pE\rvX_n} 
           &   \forall \alpha_n\in\F,
           &   \forall\rvX\in\spLLF
  \end{array}
  }
\end{corollary}
\begin{proof}
These all follow immediately from the fact that $\pE$ is a \ope{linear operator} 
and from \prefpp{thm:L_prop}.
\end{proof}

%---------------------------------------
\begin{remark}
%---------------------------------------
Projecting a stochastic process onto a basis often yields valuable insights 
into the nature of the underlying data. 
Typical projection operators include the Fourier operator $\opFT$, Laplace $\opLT$,
and z-transform $\opZT$ \ldots not to mention wavelet operators.
But note that any such projection on a random sequence simply produces another random sequence.
For example, the Fourier transform $\opFT\rvx(n)$ of a random sequence $\rvx(n)$ is another random 
sequence.

One way to overcome this difficulty is to simply invoke a \ope{sampling} operator $\opS\rvx(n)$\ifsxref{sampling}{chp:sampling},
yielding a deterministic sequence, and then take the Fourier transform of the resulting 
deterministic sequence. 
The problem here is that every time you resample the sequence, you will very likely get a 
different Fourier transform.

Arguably a better approach (and the standard one at that) 
is to first invoke the expectation operator $\pE\rvx(n)$, also yielding a deterministic sequence.

The good news here is that because $\pE$ and all the above mentioned operators are \prope{linear}, 
we can do all the standard arithmetic acrobatics associated with linear algebra operators (next corollary).
%such as $\opE\opFT\rvx(n)
\end{remark}

%---------------------------------------
\begin{corollary}
\label{cor:pE_opop}
%---------------------------------------
Let $\opM$ and $\opN$ be \ope{linear operator}s \xref{def:linop}.
\corbox{\begin{array}{>{\scy}rlcl@{\qquad}C@{\qquad}D}
    1. & \pE\brp{\opM\opN}      &=& \brp{\pE\opM}\opN                          & \forall \pE\in\clLzw,\, \opM\in\clLyz,\, \opN\in\clLxy  & (\prope{associative})
  \\2. & \pE\brp{\opM+\opN} &=& \brp{\pE\opM}+\brp{\pE\opN}            & \forall \pE\in\clLyz,\, \opM\in\clLxy,\, \opN\in\clLxy  & (\prope{left distributive})
  \\3. & \brp{\pE+\opM}\opN &=& \brp{\pE\opN}+\brp{\opM\opN}           & \forall \pE\in\clLyz,\, \opM\in\clLyz,\, \opN\in\clLxy  & (\prope{right distributive})
  \\4. & \alpha\brp{\pE\opM}    &=& \brp{\alpha\pE}\opM = \pE\brp{\alpha\opM}  & \forall \pE\in\clLyz,\, \opM\in\clLxy,\, \alpha\in\F    & (\prope{homogeneous})
\end{array}}
\end{corollary}
\begin{proof}
These all follow immediately from the fact that $\pE$ is a \ope{linear operator} \xref{thm:pE_linop}
and from properities of all linear operators \xref{thm:L_LMN}.
\end{proof}

%---------------------------------------
\begin{corollary}
\label{cor:pVar}
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} on a \structe{probability space} $\ps$.
\corbox{\begin{array}{>{\ds}rc>{\ds}lCD}
    \pVar(a\rvX+b)     &=& a^2\pVar(\rvX)                       & \forall a,b  \in\R &
  \\\pVar(\rvX)        &=& \pEx(\rvX^2) - (\pEx\rvX)^2          &                    &
\end{array}}
\end{corollary}
\begin{proof}
\begin{align*}
  \pVar(\rvX)
    &\eqd \pEx\brs{(\rvX-\pEx\rvX)^2}
    &&    \text{by definition of $\pVar$}
    &&    \text{\xref{def:pvar}}
  \\&=    \pEx\brs{\rvX^2-2\rvX\pEx\rvX + (\pEx\rvX)^2 }
    &&    \text{by \thme{Binomial Theorem}}
    &&    \text{\ifxref{binomial}{thm:binomial}}
  \\&=    \pEx\rvX^2  - \pEx\brs{2\rvX\pEx\rvX}  + \pEx (\pEx\rvX)^2
    &&    \text{by \prope{linearity} of $\pE$}
    &&    \text{\xref{thm:pE_linop}}
  \\&=    \pEx\rvX^2 - 2(\pEx\rvX)[\pEx\rvX] + (\pEx\rvX)^2
  \\&=    \pEx(\rvX^2) - (\pEx\rvX)^2
\\
  \pVar(a\rvX+b)
    &=    \pEx(a\rvX+b)^2  - [\pEx(a\rvX+b)]^2
  \\&=    \pEx(a^2\rvX^2+2ab\rvX+b^2)  - [a(\pEx\rvX)+b]^2
  \\&=    a^2 \pEx\rvX^2  +2ab\pEx\rvX + b^2 - \brs{a^2[\pEx\rvX]^2 + 2ab\pEx\rvX + b^2}
    &&    \text{by \prope{linearity} of $\pE$}
    &&    \text{\xref{thm:pE_linop}}
  \\&=    a^2\brs{ \pEx\rvX^2  - (\pEx\rvX)^2 }
  \\&\eqd a^2 \pVar(\rvX)
    &&    \text{by previous result}
\end{align*}
\end{proof}

\begin{figure}[ht]
\setlength{\unitlength}{0.3mm}%
\begin{center}%
\begin{picture}(200,110)(-50,-10)%
  \thicklines
  \color{axis}%
    \put(  0,  0){\line(1, 0){120}}%
    \put(  0,  0){\line(0, 1){100}}%
    \qbezier[20](0,60)(30,60)(60,60)%
    \qbezier[20](60,0)(60,30)(60,60)%
  \color{blue}%
    \qbezier(33,100)(85,0)(110,100)%
    \put( 60, 60){\circle*{5}}%
  \color{red}%
    \put( 20,100){\line(1,-1){80}}%
  \color{label}%
  \put(125,  0){\makebox(0,0)[l]{$x$}}%
  \put( -5, 60){\makebox(0,0)[r]{$\ff(\pE\rvX)$}}%
  \put( 60, -5){\makebox(0,0)[t]{$\pE\rvX$}}%
  \put(130,60){\vector(-1,0){32}}%
  \put(130,40){\vector(-1,0){47}}%
  \put(135,60){\makebox(0,0)[l]{$\ff(x)$ (convex function)}}%
  \put(135,40){\makebox(0,0)[l]{$mx+c$ (support line)}}%
\end{picture}
\end{center}
\caption{
  Jensen's inequality
  \label{fig:jensen}
  }
\end{figure}

\fncte{Jensen's inequality} is an extremely useful application of \prope{convex}ity \xref{def:convex} to the
\ope{expectation} operator.
Jensen's inequality is stated in \pref{cor:jensen} (next)
and illustrated in \prefpp{fig:jensen}.
%--------------------------------------
\begin{corollary}[\thmd{Jensen's inequality}]
\footnote{
  \citerpgc{shao2003}{31}{0387953825}{``1.3 Distributions and Their Characteristics"},
  \citerp{cover}{25},
  \citerpp{jensen1906}{179}{180}
  }
\label{cor:jensen}
%--------------------------------------
Let $\ff$ be a function in $\clFrr$ and $\rvX$ be a \fncte{random variable} on $\ps$.
\corbox{
  \brb{\text{$\ff$ is \prope{convex}}} 
  \quad\implies\quad 
  \brb{\ff(\pE\rvX) \le \pE\ff(\rvX)}
  }
\end{corollary}
\begin{proof}
\begin{enumerate}
  \item Proof 1:
Let $mx+c$ be a ``support line" under $\ff(x)$ \xref{fig:jensen} such that
\[
  \begin{array}{rcll}
    mx+c &<& \ff(x) & \mbox{for } x\ne \pE\rvX \\
    mx+c &=& \ff(x) & \mbox{for } x=\pE\rvX.
  \end{array}
\]
Then
\begin{align*}
  \ff(\pE\rvX)
    &=   m[\pE\rvX] + c
  \\&=   \pE[mX + c]
  \\&\le \pE\ff(\rvX)
\end{align*}

  \item Proof 2 (alternate proof):
    \begin{align*}
      \ff\brp{\pE\rvX}
        &\eqd \ff\brp{\sum_{x\in\pse} x \psp(x)}
      \\&\le \sum_{x\in\pse} \ff(x) \psp(x)
        && \text{by \thme{Jensen's inequality} for convex sets}
        && \text{\xref{thm:jensenineq}}
    \end{align*}
\end{enumerate}
\end{proof}

%--------------------------------------
\begin{example}
\footnote{
  \citerppgc{shao2003}{31}{32}{0387953825}{``Example 1.18"},
  \citerpgc{dekking2006}{110}{1846281687}{``8.5 Solutions to the quick exercises"}
  }
\label{ex:jensen}
%--------------------------------------
Some examples of \thme{Jensen's Inequality} \xref{cor:jensen} applied to the \ope{expectation operator}
are the following:
\exbox{\begin{array}{c|c|c}
    \ds \brp{\pE\rvX}^{-1}  <   \pE\brp{\rvX^{-1}}
   &\ds \pE\brp{\log\rvX}   <   \log\brp{\pE\rvX}
   &\ds e^{-\pE\rvX}        \le \pE\brs{e^{-\rvX}}
\end{array}}
\end{example}

%---------------------------------------
\begin{theorem}[\thmd{Law of the Unconscious Statistician}]
\footnote{
  \citerpgc{suhov2005}{145}{0521847664}{(2.69)},
  \citerpgc{allen2018}{490}{1447174208}{18.3.4 The Law of the Unconscious Statistician},
  \citerpgc{papoulis1990}{124}{0137116985}{Fundamental Theorem}
  }
%---------------------------------------
\thmbox{
  \pE\brs{\fg(\rvX)} = \int_{x\in\R} \fg(x) \pdfx(x) \dx
  }
\end{theorem}


%=======================================
\section{Expectation inequalities}
%=======================================
%---------------------------------------
\begin{theorem}[\thmd{Markov's inequality}]
\footnote{
  \citerp{ross}{395}
  }
\label{thm:markovineq}
%---------------------------------------
Let $\rvX:\Omega\to[0,\infty)$ be a non-negative valued \fncte{random variable} and
$a\in(0,\infty)$. Then
\thmbox{ \psp\setn{\rvX\ge a} \le \frac{1}{a} \pE\rvX }
\end{theorem}
\begin{proof}
\begin{align*}
  I &\eqd \left\{ \begin{array}{l@{\hspace{4ex}\mbox{for}\hspace{4ex}}l}
    1 &\rvX\ge a \\
    0 &\rvX < a
    \end{array}\right.
\\
  aI &\le\rvX           \\
   I &\le \frac{1}{a}\rvX \\
   \pE I &\le \pE\left(\frac{1}{a}\rvX\right) \\
\\
   \psp\setn{\rvX\ge a}
     &= 1\cdot\psp\setn{\rvX\ge a} + 0\cdot\psp\setn{\rvX<a}
   \\&= \pE I
   \\&\le \pE\left(\frac{1}{a}\rvX \right)
   \\&=   \frac{1}{a}\pE\rvX
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}[Chebyshev's inequality]
\index{Chebyshev's inequality}
\index{theorems!Chebyshev's inequality}
\footnote{
  \citerp{ross}{396}
  }
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} with mean $\mu$ and variance $\sigma^2$.
\thmbox{ \psp\setn{\abs{\rvX-\mu}\ge a} \le \frac{\sigma^2}{a^2}}
\end{theorem}
\begin{proof}
\begin{align*}
  \psp\setn{\abs{\rvX-\mu} \ge a}
    &=   \psp\setn{ (\rvX-\mu)^2 \ge a^2}
  \\&\le \frac{1}{a^2} \pE(\rvX-\mu)^2 
    && \text{by \thme{Markov's inequality}}
    && \text{\xref{thm:markovineq}}
  \\&=   \frac{\sigma^2}{a^2}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{Kolmogorov's inequality}]
\footnote{
  \citerpc{wilks1963}{107}{\textsection 4.5 ``Kolmogorov's inequality"}
  }
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} with mean $\mu$ and variance $\sigma^2$.
\thmbox{
  \brb{\begin{array}{FMD}
      (A).&$\seqn{\rvx_n}$ are \prope{independent} & and
    \\(B).&Each $\rvx_n$ has \prope{zero-mean}     & and
    \\(C).&Each $\rvx_n$ has variance $\pvar$
  \end{array}}
  \implies
  \psP\brs{\abs{\sum_{n=1}^{\xN} \rvx_n} < \lambda\sum_{n=1}^{\xN} \rvx_n^2} \ge 1 - \frac{1}{\lambda^2}
  }
\end{theorem}

%=======================================
\section{Conditional expectation}
%=======================================
Sometimes the problem of finding the expected value of a \fncte{random variable} $\rvX$
can be simplified by ``conditioning $\rvX$ on $\rvY$".
It has already been pointed out in \prefpp{sec:pE_defs} 
that the expected value $\pEx\rvX$  of $\rvX$ is \textbf{not random}.
On the other hand, 
note that $\pE(\rvX|\rvY)$ \textbf{is random}.
This is because $\pE(\rvX|\rvY)$ is a function of $\rvY$.
That is, once we know that $\rvY$ equals some fixed value $y$
(like $0$ or $2.7$ or $-5.1$) then $\pE(\rvX|\rvY=y)$ is also fixed.
However, if we don't know the value of $\rvY$,
then $\rvY$ is still a \fncte{random variable} and the expression $\pE(\rvX|\rvY)$
is also random (a function of \fncte{random variable} $\rvY$).

%---------------------------------------
\begin{theorem}
\footnote{
  \citerpc{jazwinski1970}{40}{``Theorem 2.9 (Conditional Expectations)"},
  \citerpg{jazwinski2007}{40}{9780486318196} %{``Theorem 2.9 (Conditional Expectations)"}
  }
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s. Then
\thmbox{\pEx{\rvX} = \pEy\pEx[x|y](\rvX|\rvY) }
\end{theorem}
\begin{proof}
\begin{align*}
   \pEy\pEx[x|y](\rvX|\rvY)
     &\eqd \pEy \brs{ \int_{x\in\R}x \pp(\rvX=x|\rvY) \dx}
     && \text{by definition of $\pE$}
     && \text{\xref{def:pE}}
   \\&\eqd \int_{y\in\R} \brs{\int_{x\in\R}x \pp(x|\rvY=y) \dx } \pp(y) \dy
     && \text{by definition of $\pE$}
     && \text{\xref{def:pE}}
   \\&=    \int_{y\in\R} \int_{x\in\R}x \pp(x|y)\pp(y) \dx   \dy
   \\&=    \int_{x\in\R}x \int_{y\in\R} \pp(x,y) \dy   \dx
     && \text{by \prefp{thm:pdf_cond}}
   \\&=    \int_{x\in\R}x \pp(x) \dx
     && \text{by \prefp{thm:pdf_cond}}
   \\&\eqd \pEx\rvX
     && \text{by definition of $\pE$}
     && \text{\xref{def:pE}}
\end{align*}
\end{proof}

%=======================================
\section{Expectation inner product space}
%=======================================
When possible, we like to generalize any given mathematical structure
to a more general mathematical structure and then take advantage of
the properties of that more general structure.
Such a generalization can be done with \fncte{random variable}s.
Random variables can be viewed as vectors in a vector space.
Furthermore, the expectation of the product of two \fncte{random variable}s
(e.g. $\pE(\rvX\rvY)$)
can be viewed as an \fncte{inner product} in an \structe{inner product space}.
Since we have an \fncte{inner product} space,
we can then immediately use all the properties of
\structe{inner product space}s, \fncte{norm}ed spaces, vector spaces, metric spaces,
and topological spaces.

%---------------------------------------
\begin{theorem}
\footnote{
  \citerppgc{lindquist2015}{25}{26}{3662457504}{2.1 Hilbert Space of Second-Order Random Variables. $\inprod{\xi}{\eta}=\pE\setn{\xi\bar{\eta}}$},
  \citerpgc{caines1988}{21}{0471081019}{$Exy=\int_\Omega x(\omega)y(\omega)dP(\omega)$},
  \citerpgc{caines2018}{21}{1611974712}{$Exy=\int_\Omega x(\omega)y(\omega)dP(\omega)$},
  \citerpp{moon2000}{105}{106}
  }
\label{thm:prb_vspace}
\label{thm:pEinprod}
%---------------------------------------
Let $R$ be a ring,
$\ps$ be a \structe{probability space}, $\pE$ the expectation operator, and
$\spV=\set{\rvX}{\rvX:\pso\to R}$ be the set of all random vectors
in \structe{probability space} $\ps$.
\thmbox{\begin{array}{FlM}
  (1). & \spV\eqd\set{\rvX}{\rvX:\pso\to R}        & is a \structe{vector space}. \\
  (2). & \inprod{\rvX}{\rvY}\eqd\pE(\rvX\rvY^\ast) & is an \fncte{inner product}. \\
  (3). & \norm{\rvX}\eqd\sqrt{\pE(\rvX\rvX^\ast)}  & is a \fncte{norm}. \\
  (4). & \opair{\spV}{\inprodn}                    & is an \structe{inner product space}.
\end{array}}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Proof that $\spV$ is a vector space:
    \[\begin{array}{lll@{\hs{1cm}}D}
   1) & \forall \rvX,\rvY, \rvZ\in\spV
      & (\rvX+\rvY)+ \rvZ = \rvX+(\rvY+ \rvZ)
      & \text{($+$ is associative)}
      \\
   2) & \forall \rvX,\rvY\in\spV
      & \rvX+\rvY = \rvY+\rvX
      & \text{($+$ is commutative)}
      \\
   3) & \exists  0 \in\spV \st \forall \rvX\in\spV
      & \rvX+ 0 = \rvX
      & \text{($+$ identity)}
      \\
   4) & \forall \rvX \in\spV \exists \rvY\in\spV \st
      & \rvX+\rvY =  0
      & \text{($+$ inverse)}
      \\
   5) & \forall \alpha\in S \text{ and } \rvX,\rvY\in\spV
      & \alpha\cdot(\rvX+\rvY) = (\alpha \cdot\rvX)+(\alpha\cdot\rvY)
      & \text{($\cdot$ distributes over $+$)}
      \\
   6) & \forall \alpha,\beta\in S \text{ and } \rvX\in\spV
      & (\alpha+\beta)\cdot\rvX = (\alpha\cdot \rvX)+(\beta\cdot \rvX)
      & \text{($\cdot$ pseudo-distributes over $+$)}
      \\
   7) & \forall \alpha,\beta\in S \text{ and } \rvX\in\spV
      & \alpha(\beta\cdot\rvX) = (\alpha\cdot\beta)\cdot\rvX
      & \text{($\cdot$ associates with $\cdot$)}
      \\
   8) & \forall \rvX\in\spV
      & 1\cdot \rvX = \rvX
      & \text{($\cdot$ identity)}
\end{array}\]

  \item Proof that $\inprod{\rvX}{\rvY}\eqd\pE(\rvX\rvY^\ast)$ is an \fncte{inner product}.
  \[\begin{array}{llllD}
   1) &  \pE(\rvX\rvX^\ast) &\ge 0
      &  \forall \rvX\in\spV
      &  \text{(non-negative)}
      \\
   2) &  \pE(\rvX\rvX^\ast) &= 0 \iff \rvX=0
      &  \forall \rvX\in\spV
      &  \text{(non-degenerate)}
      \\
   3) &  \pE(\alpha\rvX\rvY^\ast)    &= \alpha\pE(\rvX\rvY^\ast)
      &  \forall \rvX,\rvY\in\spV,\;\forall\alpha\in\C
      &  \text{(homogeneous)}
      \\
   4) &  \pE[(\rvX+\rvY)\rvZ^\ast] &= \pE(\rvX\rvZ^\ast) + \pE(Y\rvZ^\ast)
      &  \forall \rvX,\rvY, \rvZ\in\spV
      &  \text{(additive)}
      \\
   5) &  \pE(\rvX\rvY^\ast) &= \pE(\rvY\rvX^\ast)
      &  \forall \rvX,\rvY\in\spV
      &  \text{(conjugate symmetric)}.
  \end{array}\]

  \item Proof that $\norm{\rvX}\eqd\sqrt{\pE(\rvX\rvX^\ast)}$ is a \fncte{norm}:
    This \fncte{norm} is simply induced by the above \fncte{inner product}.
  \item Proof that $\opair{\spV}{\inprodn}$ is an \structe{inner product space}:
    Because $\spV$ is a vector space and $\inprodn$ is
    an \fncte{inner product}, $\opair{\spV}{\inprodn}$ is an \structe{inner product space}.
\end{enumerate}
\end{proof}

The next theorem gives some results that follow directly from vector space
properties:
%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $\ps$ be a \structe{probability space} with \ope{expectation} functional $\pE$.
\thmbox{\begin{array}{F >{\ds}r c >{\ds}l M}
  (1).&   \sqrt{\pE\left(\sum_{n=1}^{\xN}\rvX_n\right)}
      &\  \le& \sum_{n=1}^{\xN} \pE(\rvX_nX_n^\ast)
      &   (\ineqe{Generalized triangle inequality})
     \\
  (2).&   \abs{\pE(\rvX\rvY^\ast)}^2
      &   \le& \pE(\rvX\rvX^\ast)\:\pE(YY^\ast)
      &   (\ineqe{Cauchy-Schwartz inequality})
     \\
  (3).&   2\pE(\rvX\rvX^\ast) + 2\pE(YY^\ast)
      &=& \mc{2}{M}{$\ds\pE[(\rvX+\rvY)(\rvX+\rvY)^\ast] + \pE[(\rvX-Y)(\rvX-Y)^\ast]$}
    \\&&&(\thme{Parallelogram Law})
  \end{array}}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item $\opair{\clFor}{\pE(\rvx,\rvy)}$ is an \structe{inner product space}. Proof: \prefpp{thm:prb_vspace}.

  \item Because $\opair{\clFor}{\pE(\rvx,\rvy)}$ is an \structe{inner product space}, the other properties follow:
        \\\indentx\begin{tabular}{llll}
          1. & \ineqe{Generalized triangle inequality}:
             & \pref{thm:norm_tri}
             & \prefpo{thm:norm_tri}
             \\
          2. & \ineqe{Cauchy-Schwartz inequality}:
             & \pref{thm:cs}
             & \prefpo{thm:cs}
             \\
          3. & \thme{Parallelogram Law}:
             & \pref{thm:parallelogram}
             & \prefpo{thm:parallelogram}
        \end{tabular}
\end{enumerate}
\end{proof}

