%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%======================================
\chapter{Statistics}
\label{chp:stats}
%======================================
%=======================================
\section{Expectation operator}
\index{expectation operator}
%=======================================
In a probability space $\ps$, all proability information
is contained in the measure $\psp$ (or equivalently in the pdf or cdf
defined in terms of $\psp$).
Often times this information is overwhelming and a simpler statistic,
which does not offer so much information, is sufficient.
Some of the most common statistics can be conveniently expressed in terms
of the \hie{expectation operator} $\pE$.
%---------------------------------------
\begin{definition}
\label{def:Ex}
%---------------------------------------
Let $\ps$ be a \structe{probability space}\ifsxref{prob}{def:ps} and
$\rvX:\pso\to\R$ a random variable with
\fncte{probability density function} $\ppx:\pse\to\R$.
Then the \opd{expectation operator} on $\rvX$ is
\defbox{ 
  \pEx\rvX \eqd \int_x x \ppx(x) \dx
}
\end{definition}

We already said that a random variable $\rvX$ is neither random nor a variable,
but is rather a function of an underlying process that does appear to be random.
However, because it is a function of a process that does appear random,
the random variable $\rvX$ also appears to be random.
That is, if we don't know the outcome of of the underlying experimental
process, then we also don't know for sure what $\rvX$ is, and so $\rvX$ does
indeed appear to be random.
However, eventhough $\rvX$ appears to be random,
the expected value $\pEx\rvX$  of $\rvX$ is {\bf not random}.
Rather it is a fixed value (like $0$ or $7.9$ or $-2.6$).

On the other hand, eventhough $\pE X$ is {\bf not random},
note that $\pE(X|Y)$ {\bf is random}.
This is because $\pE(X|Y)$ is a function of $\rvY$.
That is, once we know that $\rvY$ equals some fixed value $y$
(like $0$ or $2.7$ or $-5.1$) then $\pE(X|Y=y)$ is also fixed.
However, if we don't know the value of $\rvY$,
then $\rvY$ is still a random variable and the expression $\pE(X|Y)$
is also random (a function of random variable $\rvY$).

Two common statistics that are conveniently expressed in terms of the
expectation operator are the \hie{mean} and \hie{variance}.
The mean is an indicator of the ``middle" of a probability distribution and the
variance is an indicator of the ``spread".
%---------------------------------------
\begin{definition}
\index{mean}
\index{variance}
\label{def:Mx}
\label{def:pVar}
%---------------------------------------
Let $\ps$ be a probability space and $\rvX:\pso\to\R$ a random variable.\\
The \fnctd{mean} $\pmeanx$ and \fnctd{variance} $\pVar(\rvX)$ of $\rvX$ are
\defbox{\begin{array}{rcl}
  \pmeanx  &\eqd& \pEx\rvX \\
  \pVar(\rvX)&\eqd& \pEx\left[(X-\pEx\rvX)^2 \right]
\end{array}}
\end{definition}

The next theorem gives some useful relations for simple statistics.
%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $\ps$ be a probability space with random variable $\rvX:\pso\to\R$
and let $a,b\in\R$.
\thmbox{\begin{array}{rcl}
  \pEx(a\rvX+b)  &=& a\pEx\rvX + b     \\
  \pVar(a\rvX+b) &=& a^2\pVar(\rvX)\\
  \pVar(\rvX) &=& \pEx\rvX^2 - (\pEx\rvX)^2
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \pEx(a\rvX+b)
    &\eqd \int_\R \brs{ax+b}\ppx(x) \dx
    &&    \text{by definition of $\pEx$ \xref{def:Ex}}
  \\&=    a\int_\R x \ppx(x)\dx + b\int_\R \ppx(x)\dx
    &&    \text{by \prope{linearity} of $\int$}
  \\&=    a\pEx\rvX + b
    &&    \text{by definition of $\pEx$ \xref{def:Ex}}
\\
  \pVar(\rvX)
    &\eqd \pEx\brs{(X-\pEx\rvX)^2}
    &&    \text{by definition of $\pVar$ \xref{def:pVar}}
  \\&=    \pEx\brs{\rvX^2-2\rvX\pEx\rvX + (\pEx\rvX)^2}
  \\&=    \pEx\rvX^2  - \pEx 2\rvX\pEx\rvX  + \pEx (\pEx\rvX)^2
  \\&=    \pEx\rvX^2 - 2(\pEx\rvX)[\pEx\rvX] + (\pEx\rvX)^2
  \\&=    \pEx\rvX^2  - (\pEx\rvX)^2
\\
  \pVar(a\rvX+b)
    &=    \pEx(a\rvX+b)^2  - [\pEx(a\rvX+b)]^2
    &&    \text{by previous result}
  \\&=    \pEx(a^2X^2+2ab\rvX+b^2)  - [a\pEx\rvX+b]^2
  \\&=    \brb{a^2 \pEx\rvX^2 + 2ab\pEx\rvX + b^2}  - \brb{a^2[\pEx\rvX]^2 + 2ab\pEx\rvX + b^2}
  \\&=    a^2\brs{\pEx\rvX^2  - (\pEx\rvX)^2}
  \\&\eqd a^2 \pVar(\rvX)
    &&    \text{by definition of $\pVar$ \xref{def:pVar}}
\end{align*}
\end{proof}


\begin{figure}[ht]
\setlength{\unitlength}{0.3mm}%
\begin{center}%
\begin{picture}(200,110)(-50,-10)%
  \color{axis}%
    \put(  0,  0){\line(1, 0){120}}%
    \put(  0,  0){\line(0, 1){100}}%
    \qbezier[20](0,60)(30,60)(60,60)%
    \qbezier[20](60,0)(60,30)(60,60)%
  \color{blue}%
    \qbezier(33,100)(85,0)(110,100)%
    \put( 60, 60){\circle*{5}}%
  \color{red}%
    \put( 20,100){\line(1,-1){80}}%
  \color{label}%
  \put(125,  0){\makebox(0,0)[l]{$x$}}%
  \put( -5, 60){\makebox(0,0)[r]{$\ff(\pE\rvX)$}}%
  \put( 60, -5){\makebox(0,0)[t]{$\pE X$}}%
  \put(130,60){\vector(-1,0){32}}%
  \put(130,40){\vector(-1,0){47}}%
  \put(135,60){\makebox(0,0)[l]{$\ff(x)$ (convex function)}}%
  \put(135,40){\makebox(0,0)[l]{$mx+c$ (support line)}}%
\end{picture}
\end{center}
\caption{
  Jensen's inequality for $\pE\rvX$ \xref{thm:jensen}
  \label{fig:jensen}
  }
\end{figure}

And now for an extremely useful application of convexity to the
expectation operator: \hie{Jensen's inequality}.
Jensen's inequality is stated in \prefpp{thm:jensen}
and illustrated in \prefpp{fig:jensen}.
%--------------------------------------
\begin{theorem}[\thmd{Jensen's inequality for $\pE\rvX$}]
\footnote{
  \citerpgc{chow1978}{103}{1468400622}{Corollary 1, (11)},
  \citerp{cover}{25},
  \citerpp{jensen1906}{179}{180}
  }
\label{thm:jensen}
%--------------------------------------
Let $\ff$ be a function and $\rvX$ a \fncte{random variable}\ifsxref{prob}{def:rvX}. Then
\thmbox{
  \brb{\begin{array}{M}
    $\ff$ is \prope{convex}\\
    \xref{def:convex}
  \end{array}}
  \implies 
  \brb{\ff(\pE\rvX) \le \pE\ff(\rvX)}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item This follows directly from \thme{Jensen's Inequality} \xref{thm:jensenineq}
        with $\xN\eqd1$, $\lambda_1\eqd1$, and $\vx_1\eqd\pE\rvX$.

  \item An alternative proof is as follows: 
        As illustrated in \prefpp{fig:jensen}, 
        let $mx+c$ be a ``support line" under $\ff(x)$ such that
        \\\indentx
          $\brb{\begin{array}{rcll}
            mx+c &<& \ff(x) & \mbox{for } x\ne \pE\rvX \\
            mx+c &=& \ff(x) & \mbox{for } x=\pE X.
          \end{array}}$.
        \qquad
        Then
        $\brb{\begin{array}{rcl}
          \ff(\pE\rvX)
            &=&   m[\pE X] + c
          \\&=&   \pE[mX + c]
          \\&\le& \pE\ff(\rvX)
        \end{array}}$
\end{enumerate}
\end{proof}


%=======================================
\section{Upper bounds on probability}
%=======================================
%---------------------------------------
\begin{theorem}
\footnote{
  \citerpgc{chow1978}{105}{1468400622}{\thme{Tchebychev inequality}},
  \citerpp{ross}{395}{396}
  }
\label{thm:markovineq}
\label{thm:chebyineq}
%---------------------------------------
%Let $\rvX$ be a random variable with mean $\mu$ and variance $\sigma^2$.
Let $\rvX:\Omega\to[0,\infty)$ be a non-negative valued random variable. % and
%$a\in(0,\infty)$. Then
\thmbox{\begin{array}{Frc>{\ds}llMD}
  1. & \pP{\rvX\ge a}               &\le& \frac{1}{a}\pE\rvX     & \forall a>0 & (\thmd{Markov's inequality})     & and\\
  2. & \pP{\abs{\rvX-\pE\rvX}\ge a} &\le& \frac{1}{a^2}\pVar\rvX & \forall a>0 & (\thmd{Chebyshev's inequality})
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  I &\eqd \left\{ \begin{array}{l@{\hspace{4ex}\mbox{for}\hspace{4ex}}l}
    1 & X\ge a \\
    0 &\rvX < a
    \end{array}\right.
\\
  aI &\le\rvX           \\
   I &\le \frac{1}{a}\rvX \\
   \pE I &\le \pE\left(\frac{1}{a} X\right) 
\\
   \pP{X\ge a}
     &= 1\cdot\pP{X\ge a} + 0\cdot\pP{X<a}
   \\&= \pE I
   \\&\le \pE\left(\frac{1}{a}\rvX \right)
   \\&=   \frac{1}{a}\pE X
  \\\\
  \pP{\abs{X-\mu} \ge a}
    &=   \pP{ (X-\mu)^2 \ge a^2}
  \\&\le \frac{1}{a^2} \pE(X-\mu)^2
    &&   \text{by \thme{Markov's inequality}}
    &&   \text{\xref{thm:markovineq}}
  \\&=   \frac{\pVar\rvX}{a^2}
    &&   \text{by definition of $\pVar$}
    &&   \text{\xref{def:pVar}}
\end{align*}
\end{proof}

%=======================================
\section{Joint and conditional expectation}
%=======================================
Sometimes the problem of finding the expected value of a random variable $\rvX$
can be simplified by ``conditioning $\rvX$ on $\rvY$".
%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $\rvX$ and $\rvY$ be random variables. Then
\thmbox{\pEx{X} = \pEy\pEx[x|y](X|Y) }
\end{theorem}
\begin{proof}
\begin{align*}
   \pEy\pEx[x|y](X|Y)
     &\eqd \pEy \left[ \int_x x \pp(X=x|Y) \dx \right]
   \\&\eqd \int_y \left[\int_x x \pp(x|Y=y) \dx \right] \pp(y) \dy
   \\&=    \int_y \int_x x \pp(x|y)\pp(y) \dx   \dy
   \\&=    \int_x x \int_y \pp(x,y) \dy   \dx
   \\&=    \int_x x \pp(x) \dx
   \\&\eqd \pEx\rvX
\end{align*}
\end{proof}




When possible, we like to generalize any given mathematical structure
to a more general mathematical structure and then take advantage of
the properties of that more general structure.
Such a generalization can be done with random variables.
Random variables can be viewed as vectors in a vector space.
Furthermore, the expectation of the product of two random variables
(e.g. $\pE(XY)$)
can be viewed as an innerproduct in an innerproduct space.
Since we have an inner product space,
we can then immediately use all the properties of
innerproduct spaces, normed spaces, vector spaces, metric spaces,
and topological spaces.
%\footnote{
%  \hie{spaces:} Chapter/Appendix~\ref{chp:space} page~\pageref{chp:space}
%  }




%---------------------------------------
\begin{theorem}
\citetbl{
  \citerpp{moon2000}{105}{106}
  }
\label{thm:prb_vspace}
%---------------------------------------
Let $R$ be a ring,
$\ps$ be a probability space, $\pE$ the expectation operator, and
$V=\set{X}{X:\pso\to R}$ be the set of all random vectors
in probability space $\ps$.
\thmbox{\begin{array}{FM}
  1. & $V$ is a vector space. \\
  2. & $\inprod{X}{Y}\eqd\pE(XY^\ast)$ is an inner product. \\
  3. & $\norm{X}\eqd\sqrt{\pE(XX^\ast)}$ is a norm. \\
  4. & $(V,\inprod{\cdot}{\cdot})$ is an innerproduct space.
\end{array}}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Proof that $V$ is a vector space:
    \[\begin{array}{lll@{\hs{1cm}}D}
   1) & \forall  X, Y, Z\in V
      & ( X+ Y)+ Z =  X+( Y+ Z)
      & \text{($+$ is associative)}
      \\
   2) & \forall  X, Y\in V
      &  X+ Y =  Y+ X
      & \text{($+$ is commutative)}
      \\
   3) & \exists  0 \in V \st \forall  X\in V
      &  X+ 0 =  X
      & \text{($+$ identity)}
      \\
   4) & \forall \rvX \in V \exists  Y\in V \st
      &  X+ Y =  0
      & \text{($+$ inverse)}
      \\
   5) & \forall \alpha\in S \text{ and }  X, Y\in V
      & \alpha\cdot( X+ Y) = (\alpha \cdot\rvX)+(\alpha\cdot Y)
      & \text{($\cdot$ distributes over $+$)}
      \\
   6) & \forall \alpha,\beta\in S \text{ and }  X\in V
      & (\alpha+\beta)\cdot\rvX = (\alpha\cdot \rvX)+(\beta\cdot \rvX)
      & \text{($\cdot$ pseudo-distributes over $+$)}
      \\
   7) & \forall \alpha,\beta\in S \text{ and }  X\in V
      & \alpha(\beta\cdot\rvX) = (\alpha\cdot\beta)\cdot X
      & \text{($\cdot$ associates with $\cdot$)}
      \\
   8) & \forall  X\in V
      & 1\cdot \rvX =  X
      & \text{($\cdot$ identity)}
\end{array}\]

  \item Proof that $\inprod{X}{Y}\eqd\pE(XY^\ast)$ is an inner product.
  \[\begin{array}{llllD}
   1) &  \pE(XX^\ast) &\ge 0
      &  \forall  X\in V
      &  \text{(non-negative)}
      \\
   2) &  \pE(XX^\ast) &= 0 \iff  X=0
      &  \forall  X\in V
      &  \text{(non-degenerate)}
      \\
   3) &  \pE(\alpha XY^\ast)    &= \alpha\pE(XY^\ast)
      &  \forall  X, Y\in V,\;\forall\alpha\in\C
      &  \text{(homogeneous)}
      \\
   4) &  \pE[(X+Y)Z^\ast] &= \pE(XZ^\ast) + \pE(YZ^\ast)
      &  \forall  X, Y, Z\in V
      &  \text{(additive)}
      \\
   5) &  \pE(XY^\ast) &= \pE(YX^\ast)
      &  \forall  X, Y\in V
      &  \text{(conjugate symmetric)}.
  \end{array}\]

  \item Proof that $\norm{X}\eqd\sqrt{\pE(XX^\ast)}$ is a norm:
    This norm is simply induced by the above innerproduct.
  \item Proof that $(V,\inprod{\cdot}{\cdot})$ is an innerproduct space:
    Because $V$ is a vector space and $\inprod{\cdot}{\cdot}$ is
    an innerproduct, $(V,\inprod{\cdot}{\cdot})$ is an innerproduct space.
\end{enumerate}
\end{proof}




The next 2 theorems give some results that follow directly from vector space properties.
%---------------------------------------
\begin{theorem}[vector space equalities]
%---------------------------------------
Let $\ps$ be a probability space with expectation functional $\pE$.
\thmbox{\begin{array}{F >{\ds}r c >{\ds}l D}
  1. & \sqrt{\pE[(\rvX+\rvY)(\rvX+\rvY)^\ast]}
     &=&  \sqrt{\pE(\rvX\rvX^\ast)}
       +  2\Re\pE(\rvX\rvY^\ast)
       +  \sqrt{\pE(\rvY\rvY^\ast)}
     & (\thme{polar identity})
     \\
  2. & 2\pE(\rvX\rvX^\ast) + 2\pE(\rvY\rvY^\ast)
     &=& \pE[(\rvX+\rvY)(\rvX+\rvY)^\ast] + \pE[(\rvX-\rvY)(\rvX-\rvY)^\ast]
     &   (\thme{Parallelogram Law})
  \end{array}}
\end{theorem}
\begin{proof}
\begin{tabular}[t]{llll}
  1. & \thme{polar identity}:
     & \ifdochaselse{vsnorm}{\pref{lem:polarid}}{not included in this document}
     & \ifdochas{vsnorm}{page~\pageref{lem:polarid}}
     \\
  2. & \thme{Parallelogram Law}:
     & \ifdochaselse{vsnorm}{\pref{thm:parallelogram}}{not included in this document}
     & \ifdochas{vsnorm}{page~\pageref{thm:parallelogram}}
\end{tabular}
\end{proof}

%---------------------------------------
\begin{theorem}[vector space inequalities]
\citetbl{
  \citerpgc{chow1978}{104}{1468400622}{\thme{H/:older Inequality}: Theorem 2}
  }
%---------------------------------------
Let $\ps$ be a probability space with expectation functional $\pE$.
\thmbox{\begin{array}{F >{\ds}r c >{\ds}l D}
  1. & \sqrt{\pE\left(\sum_{n=1}^\xN \rvX_n\right)}
     &\le& \sum_{n=1}^\xN \sqrt{\pE(\rvX_n\rvX_n^\ast)}
     & (\thme{triangle inequality})
     \\
  2. & \abs{\sqrt{\pE(\rvX\rvX^\ast)}-\sqrt{\pE(\rvX\rvX^\ast)}}
     &\le&  \sqrt{\pE[(\rvX-\rvY)(\rvX-\rvY)^\ast]} 
     & (\thme{reverse triangle inequality})
     \\
  3. & \sqrt{\pE[(\rvX+\rvY)(\rvX+\rvY)^\ast]}
     &\le& \sqrt{\pE(\rvX\rvX^\ast)} + \sqrt{\pE(\rvY\rvY^\ast)}
     & (\thme{Minkowski's inequality})
     \\
  4. & \left| \pE(\rvX\rvY^\ast)\right|^2
     &\le& \pE(\rvX\rvX^\ast)\:\pE(\rvY\rvY^\ast)
     & (\thme{Cauchy-Schwartz inequality})
  \end{array}}
\end{theorem}
\begin{proof}
These follow directly from vector space results: 
%in Chapter/Appendix~\ref{chp:space}:

\begin{tabular}{llll}
  1. & \thme{triangle inequality}:
     & \ifdochaselse{vsnorm}{\pref{thm:norm_tri}}{not included in this document}
     & \ifdochas{vsnorm}{page~\pageref{thm:norm_tri}}
     \\
  2. & \thme{reverse triangle inequality}:
     & \ifdochaselse{vsnorm}{\pref{thm:rti}}{not included in this document}
     & \ifdochas{vsnorm}{page~\pageref{thm:rti}}
     \\
  3. & \thme{Minkowski's inequality}:
     & \ifdochaselse{vsnorm}{\pref{thm:minkowineq}}{not included in this document}
     & \ifdochas{vsnorm}{page~\pageref{thm:minkowineq}}
     \\
  4. & \thme{Cauchy-Schwartz inequality}:
     & \ifdochaselse{vsnorm}{\pref{thm:cs}}{not included in this document}
     & \ifdochas{vsnorm}{page~\pageref{thm:cs}}
\end{tabular}
\end{proof}

