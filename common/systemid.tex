%============================================================================
% Daniel J. Greenhoe
% LaTeX file
%============================================================================
%============================================================================
\chapter{System Identification}
%============================================================================
\qboxnpq{
  Karl R. Popper (1902--1994)
  \index{Popper, Karl}
  \index{quotes!Popper, Karl}
  \footnotemark
  }
  {../common/graphics/portraits/popper_karl_wkp_pdomain.jpg}
  %{Mathematics compares the most diverse phenomena and discovers the secret analogies that unite them.}
  {%
  I can therefore gladly admit that falsificationists like myself much
  prefer an attempt to solve an interesting problem by a bold conjecture,
  even (and especially) if it so turns out to be false,
  to any recital of a sequence of irrelevant truisms.
  We prefer this because we believe that this is the way in which we can
  learn from our mistakes and that in finding that our conjecture was false
  we shall have learned much about the truth,
  and shall have got nearer to the truth.
  }
  \footnotetext{\begin{tabular}[t]{ll}
    quote: & \citerp{popper1962}{231}, \citerpg{popper1963}{313}{1135971307}\\
    image: & \url{https://en.wikipedia.org/wiki/File:Karl_Popper.jpg}, ``no known copyright restrictions"
  \end{tabular}}

%=======================================
\section{Estimation techniques}
%=======================================
Let $\opS$ be a \structe{system} with \fncte{impulse response} $\fh(n)$ with
with \ope{DTFT} $\FH(\omega)$,
input $\rvx(n)$, and output $\rvy(n)$.
Often in the field of ``digital signal processing" (DSP), $\opS$ is a ``filter"
with known $\fh(n)$ and $\FH(\omega)$ because the filter $\opS$ was
designed by a designer who had direct control over $\fh(n)$.

However in many other practical situations, $\opS$ is some other system
for which $\fh(n)$ and $\FH(\omega)$ is \emph{not} known\ldots but which we may
want to \ope{estimate}. Examples of such $\opS$ is a
device on an industrial shaker table, a communication channel, or the entire earth.

Determining $\fh(n)$ and/or $\FH(\omega)$ is part of an operation called ``\ope{system identication}".
Determining $\FH(\omega)$ in particular is referred to as
``\ope{Frequency Response Identification}"\footnote{\citerpg{shin2008}{292}{0470725648}}
or as
``\ope{Frequency Response Function}" (``\ope{FRF}") estimation.\footnote{\citePpc{cobb1988}{1}{FRF ``measurement"}}
\ope{FRF} estimation is a challenging problem and one that
many people have devoted much effort to.
This chapter describes some of that effort.

In the early days, people used a rather obvious technique for determining $\FH(\omega)$---the simple
\fncte{sine sweep}. That is, they made the input a sine wave with slowly increasing (or decreasing)
frequency while measuring the resulting output.
This technique, although effective, was ``very slow".\footnote{
  \citeP{leuridan1986}{911}{``Stepped Sine Testing"},
  \citePpc{cobb1988}{1}{Chapter 1---Introduction},
  \citerppgc{ewins1986}{125}{140}{0863800173}{3.7 \scshape Use of different excitation types}
  }
\emph{And} there is another problem---we don't always have control over the input signal.
Examples of this include earthquake and volcanic activity analysis.

An alternative to the sine-sweep input is \fncte{random sequence} input.
All the techniques that follow in this chapter are of this type.
A problem with using random sequences directly for estimating $\FH(\omega)$ is that the
estimate $\estH(\omega)$ is itself also random.
This is not what we want. We want an estimate that we can actually write down
on paper or at least plot on paper.

A solution to this is to not use the random sequences directly to estimate $\FH(\omega)$,
but instead to first use the \ope{expectation} operator $\pE$ \xref{def:pE}.
The expectation operator takes a quantity $\rvX$ that is inherently ``random"
(with some probability distribution $\pdfp(x)$) and
turns it into a deterministic ``constant" $\pE\rvX$.

The operator $\pE$ is also used by the spectral density functions
$\Swxx(\omega)$ and $\Swxy(\omega)$ \xref{def:Swxy}.
And $\Swxx(\omega)$ and $\Swxy(\omega)$ are what are typically used to calculate
an estimate $\estH(\omega)$.

%=======================================
\section{Additive noise system models}
%=======================================
\begin{figure}
  \centering
  \begin{tabular}{|c|c|}
    \hline
     \tbox{\includegraphics{graphics/opT_cnoise.pdf}}
    &\tbox{\includegraphics{graphics/opT_mnoise.pdf}}
    \\
      (A) \opd{communications additive noise model}
     &(B) \opd{measurement    additive noise model}
    \\
      The ``input signal" is $\rvx(n)$.
     &The ``input signal" is $\rvp(n)$.
    \\\hline
      \mc{2}{|l|}{In each model, $\rvx(n)$ and $\rvy(n)$ are ``known", and $\rvu(n)$, $\rvv(n)$, and $\rvw(n)$ are \emph{not}.}
    \\\mc{2}{|l|}{In definition, the two models are \textbf{equivalent} under the relation $\rvu(n)=-\rvw(n)$.}
    \\\mc{2}{|l|}{In practice, they are \textbf{different}:}
    \\\mc{2}{|l|}{\indentx\imarks in (A), $\rvx$ and $\rvu$ would be typically \prope{uncorrelated};}
    \\\mc{2}{|l|}{\indentx\imarks in (B), $\rvx$ and $\rvw=-\rvu$ are very much \prope{correlated} ($\rvx$ is a function of $\rvu$).}
    \\\hline
  \end{tabular}
  \caption{Additive noise systems with \prope{linear}/\propb{non-linear} operator $\opT$\label{fig:addnoise}}
\end{figure}

Consider the additive noise systems illustrated in \prefpp{fig:addnoise}.
\begin{liste}
  \item The illustration on the left is suitable for modeling a communications system where
$\rvx$ is the transmitted signal, $\rvy$ is the received signal, $\rvu$ and $\rvv$ are thermal noise,
and the ``transfer function" $\opH$ is the communications channel (air, water, wires, etc.) that one wishes to estimate.

  \item The illustration on the right is suitable for modeling a testing system where
$\rvp$ is an input test signal (from an industrial shaker or from a naturally occurring signal originating
from geophysical activity), $\rvw$ is measurement noise, $\rvx$ is the measured input contaminated by noise,
and $\opH$ is the device under test (a piece of equipment, a building, or the entire earth).
\end{liste}

Note that the two models are an equivalent system $\opS$ under the relation $\rvu=-\rvw$.
But although one might expect such a sign difference to wreak mathematical havoc in resulting equations,
this is simply not the case here because
\\\indentx$\ds\Swww = \opFT\pE\brs{\rvw(m)\rvw\ast(0)}
                    = \opFT\pE\brs{\brp{-\rvu(m)}\brp{-\rvu^\ast(0)}}
                    = \opFT\pE\brs{\brp{\rvu(m)}\brp{\rvu^\ast(0)}}
                    = \Swuu$
\\
So the sign difference is not that big of a difference after all.
But there are some key differences in practice:
\begin{liste}
  \item In the communications model (on the left), the ``input signal" is $\rvx(n)$
      and the frequency-domain input \fncte{signal-to-noise ratio} (\fncte{SNR}) is $\ffrac{\Swxx(\omega)}{\Swuu(\omega)}$.
      In the measurement model (on the right), the ``input signal" is $\rvp(n)$
      and the frequency-domain input \fncte{signal-to-noise ratio} (\fncte{SNR}) is
         $\ffrac{\Swpp(\omega)}{\Swww(\omega)} = \ffrac{\Swpp(\omega)}{\Swuu(\omega)}$.

  \item On the left, $\rvx$ and $\rvu$ would be typically \prope{uncorrelated};
        on the right, $\rvx$ and $\rvw=-\rvu$ are very much \prope{correlated} ($\rvx$ is a function of $\rvu$).
\end{liste}

%=======================================
\section{Transfer function estimate definitions}
%=======================================
As a first attempt at estimating the transfer function $\opH$ of $\opS$,
or at least the magnitude squared of $\opH$,
we might assume $\opH$ to be \prope{LTI}, take a cue from the relation
$\Swyy=\Swxx\abs{\FH}^2$ of \prefpp{cor:Swxy},
%somehow deem the squaring operation to be of little consequence,
and arrive at a function called ``\fncte{transmissibility}"
(next definition).
%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{bendat2011}{469}{1118210824}{$\abs{H(f)}=\brs{G_{yy}(f)/G_{xx}(f)}^{1/2}$},
  \citePpc{yan2012}{204}{(1) $\brs{G_{YY}(s)}=\brs{H(s)}\brs{G_{FF}(s)}\brs{H^\ast(s)}^T$},
  \citerpgc{goldman1999}{179}{0831130881}{Transmissibility \ldots $H_{ab}'=\ffrac{G_{bb}}{G_{aa}}$ (note: differs by $\sqrt{\cdot}$ from Bendat and Piersol},
  \citeP{zhang2016},
  \citePp{zhou2018}{824},
  \url{https://link.springer.com/chapter/10.1007/978-3-319-54109-9_4}
  }
\label{def:Txy}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
  \fnctd{transmissibility $\Twxy(\omega)$} is defined as
  \qquad$\ds
  \Twxy(\omega) \eqd \sqrt{\frac{\Swyy(\omega)}{\Swxx(\omega)}}
  $
  }
\end{definition}

Transmissibility is in essence the ratio of ``\fncte{spectral power}" \xref{rem:spower}
output to \fncte{spectral power} input.
Note that it is a real-valued function (because $\Swxx$ and $\Swyy$ are real-valued).
We might suspect that we could attain better estimates of $\opH$ by allowing the estimates to be complex-valued. %\ldots
%not to mention the idea of taking seriously the squaring operation.
And in fact, all the remaining estimates in this section are in general complex-valued.

And so to start (again), and in the very special (a.k.a unrealistic) case of $\opS$ having
\prope{zero measurement noise} (\prope{zero measurement error})
($\rvv=\rvu=\rvw=0$),
$\fh(n)$ being \prope{linear time invariant} (\prope{LTI}),
and input $\rvx(n)$ being \prope{wide sense stationary}\ldots
then we can determine (a.k.a ``identify") $\fh(n)$ or $\FH(\omega)$
exactly by $\FH(\omega)=\ffrac{\Swyx(\omega)}{\Swxx(\omega)}$ \xref{cor:Swxy}.

However, in practical situations, there is measurement noise/error. % \xref{fig:xhyplusv}.
Examples may include
``road noise" from a test being performed in a moving vehicle or \fncte{quantization noise} from an
\hie{analog-to-digital converter} (\hie{ADC}).

\begin{minipage}{\tw-70mm}
If the measurement error is at the output only (and under the assumptions of \prope{LTI} and \prope{WSS})
then $\estHa$ (next definition) is the ideal estimator in the sense that $\estHa=\FH$ \xref{cor:estH1}.
\end{minipage}
\hfill\tbox{\includegraphics{graphics/sysH_xy0v--.pdf}}
\\
%---------------------------------------
\begin{definition}[\fnctd{least squares estimate} of $\FH(\omega)$]
\footnote{
  \citerppgc{bendat1993}{106}{109}{0471570559}{5.1.1 Optimality of Calculations},
  \citerpgc{bendat2011}{185}{1118210824}{$H_1(f) = \ffrac{G_{xy}(f)}{G_{xx}(f)}$ (6.37)},
  \citerpgc{shin2008}{293}{0470725648}{
    $H_1(f)=\ffrac{\Swxy(f)}{\Swxx(f)}$ (9.63);
    which differs from \pref{def:H1}, but see \prefp{rem:Rxym}
    },
  \citeP{bendat1978}{cited by Cobb(1988)---variance estimate for $\estHa$},
  \citePc{allemang1979}{cited by Shin(2008)},
  \citePpc{leuridan1986}{910}{\fncte{Least Squares Technique}; (8) $[G_{xx}](H)=[G_{xy}]$},
  \citeP{abom1986}{cited by Cobb(1988)---variance estimate for $\estHa$},
  \citePppc{allemang1987}{54}{55}{5.3.1 $H_1$ Technique; $[H]=[G_{XF}][G_{FF}]^{-1}$ (11)},
  \citePpc{cobb1988}{2}{$^1\hat{H}(f)=\ffrac{\hat{G}_{yx}(f)}{\hat{G}_{xx}(f)}$ (1)},
  \citePpc{goyder1984}{438}{$H(i\omega)=\ffrac{S_{qp}}{S_{pp}}$ (3)},
  \citerpgc{pintelon2012}{233}{1118287398}{$\hat{G}(\Omega_k)=S_{yu}(j\omega_k)S_{uu}^{-1}(j\omega_k)$ (7-30)},
  \citePpc{white2006}{678}{$H_1(f)=\ffrac{\hat{S}_{x_my_m}(f)}{\hat{S}_{x_mx_m}(f)}$ (1) which differs by conjugate, references Bendat and Piersol},
  }
\label{def:H1}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defbox{\begin{array}{Mrc>{\ds}l}
  The \fnctd{transfer function estimate $\estHa(\omega)$} is defined as &
  \estHa(\omega) &\eqd& \frac{\Swyx(\omega)}{\Swxx(\omega)}
\end{array}}
\end{definition}

The estimator $\estHa$ is a good start.
However in the early 1980s, L. D. Mitchell pointed out that in the presence of input noise,
$\estHa$ is far from ideal in that it is \prope{biased} with respect to $\FH$;
in fact, $\estHa$ \prope{under estimates} $\FH$ \xref{cor:estH2}.
Mitchell proposed a new estimator $\estHb$ (next definition).


\begin{minipage}{\tw-70mm}
This estimator has the special property that when there is input noise but no
output noise (and under \prope{LTI}, \prope{WSS}, and \prope{uncorrelated} assumptions),
then it is ideal in the sense that $\estHb(\omega)=\FH(\omega)$ \xref{cor:estH2}.
\end{minipage}
\hfill\tbox{\includegraphics{graphics/sysH_xyu0--.pdf}}
\\
Note also that in the case of both no input and no output noise, then $\estHa=\estHb$ \xref{cor:Swxy}.
%---------------------------------------
\begin{definition}[\fnctd{Inverse Method}]
\footnote{
  \citerpgc{shin2008}{293}{0470725648}{$H_2(f)=\ffrac{\Swyy(f)}{\Swyx(f)}$ (9.65);
    which differs from \pref{def:H2}, but see \prefp{rem:Rxym}
    },
  \citerpgc{bendat2011}{186}{1118210824}{$H_2(f) = \ffrac{G_{yy}(f)}{G_{yx}(f)}$ (6.42)},
  \citePc{mitchell1980}{cited by Cobb(1988)},
  \citePpc{mitchell1982}{278}{``Define what will be called an inverse method for calculation of a FRF as\ldots";
    $\~H_2(f)=\ffrac{G_{yy}}{G_{yx}}$ (6);
    Note this differs with \pref{def:H2} by a conjugate, but note that Mitchell seems to follow Bendat (see his [3] and [4]),
    which would explain this difference \xref{rem:Rxym}},
  \citePpc{cobb1988}{3}{$^2\hat{H}(f)=\ffrac{\hat{G}_{yy}(f)}{\hat{G}_{xy}(f)}$ (1)},
  \citePpc{white2006}{678}{$H_2(f)=\ffrac{\hat{S}_{y_my_m}(f)}{\hat{S}_{y_mx_m}(f)}$ (2) which differs by conjugate, references Bendat and Piersol}
  }
\label{def:H2}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defbox{\begin{array}{Mrc>{\ds}l}
  The \fnctd{transfer function estimate $\estHb(\omega)$} is defined as
  & \estHb(\omega) &\eqd& \frac{\Swyy(\omega)}{\Swxy(\omega)}
\end{array}}
\end{definition}

Mitchell's $\estHb$ contribution ``generated a flurry of activity"\footnote{\citePp{cobb1988}{3}}
and soon more $\FH$ estimators appeared.
%As it turns out, in the presence of \emph{both} input and output measurement noise,
%the traditional estimator $\estHa$, although ideal when input noise = 0, tends to \prope{under estimate} $\FH$ \xref{cor:estH1}
%and Mitchell's $\estHb$, although ideal when output noise = 0,
%tends to \prope{over estimate} $\FH$ \xref{cor:estH2}.
%But what about the case when there is noise on both input and output?
%The next estimator is ideal when noise is present on both and the noise power of the two is equal.
So far we have
\begin{listi}
  \item $\estHa$ which is ideal when there is no input noise but
        \prope{under estimate}s $\FH$ when there is \xref{cor:estH1}
  \item $\estHb$ which is ideal when there is no output noise but
        \prope{over estimate}s $\FH$ when there is \xref{cor:estH2}.
\end{listi}
But what about estimators for when there is noise on both input and output?
Armed with two estimators that between them account for both input and output noise,
an ``ad hoc" solution might be to somehow take mean values of $\estHa$ and $\estHb$
to induce new estimators---this approach summarizes the next three definitions.
An arguably more mature approach is to find estimators that are optimal with respect to least squares measures---and
this approach summarizes \pref{def:Hv} -- \prefpp{def:Hkappa}.

%---------------------------------------
\begin{definition}
\label{def:Ham}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
    The \fnctd{Arithmetic mean transfer function estimate $\estHam(\omega)$}
    of $\estHa(\omega)$ and $\estHb(\omega)$ is defined as
  \\\indentx$
  \estHam(\omega) \eqd
    \frac{\ds\abs{\Swxy(\omega)}^2 + \Swxx(\omega)\Swyy(\omega)}
         {\ds 2\Swxx(\omega)\Swxy(\omega)}
  $}
\end{definition}

%---------------------------------------
\begin{proposition}
\footnote{
  \citePpc{mitchell1982}{279}{``Frequency Response Calculation: The Average Method"},
  \citePpc{zheng2002}{918}{``1.3 Arithmetic Mean Estimator $H_3$"}
  }
\label{prop:Ham}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\propbox{\begin{array}{rc>{\ds}lM}
  \estHam(\omega) &=& \frac{\ds\estHa(\omega) + \estHb(\omega)}{2}
                  & (\fnctb{arithmetic mean} of $\estHa$ and $\estHb$)
\end{array}}
\end{proposition}
\begin{proof}
\begin{align*}
  \estHam(\omega)
    &\eqd  \frac{\ds\abs{\Swxy(\omega)}^2 + \Swxx(\omega)\Swyy(\omega)}
                {\ds2\Swxx(\omega)\Swxy(\omega)}
    && \text{by definition of $\estHam$}
    && \text{\xref{def:Ham}}
  \\&=  \frac{\ds \Swxy(\omega) \Swxy^\ast(\omega)}
             {\ds2\Swxx(\omega) \Swxy     (\omega)}
      + \frac{\ds \Swxx(\omega) \Swyy     (\omega)}
             {\ds2\Swxx(\omega) \Swxy     (\omega)}
  \\&=  \frac{
        \frac{\ds\Swxy^\ast(\omega)} {\ds\Swxx(\omega)}
      + \frac{\ds\Swyy     (\omega)} {\ds\Swxy(\omega)}
        }{2}
  \\&= \frac{\ds\estHa(\omega) + \estHb(\omega)}{2}
    && \text{by definitions of $\estHa$ and $\estHb$}
    && \text{\xxref{def:H1}{def:H2}}
\end{align*}
\end{proof}

% _____________________________________________
%| |               ________      _________     |
%|D|              / Sxy*(w)     / Syy(w)       |
%|E| Hgm(w) = \  / -------- \  / ---------     |
%|F|           \/   Sxx(w)   \/   Sxy(w)       |
%|_|___________________________________________|
\begin{definition}                      %
\label{def:Hgm}                         %
%---------------------------------------%
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
    The \fnctd{Geometric mean transfer function estimate $\estHgm(\omega)$}
    of $\estHa(\omega)$ and $\estHb(\omega)$ is defined as
  \\\indentx$
  \estHgm(\omega) \eqd
    \sqrt{\frac{\ds\Swxy^\ast(\omega)}
               {\ds\Swxx(\omega)}}
    \sqrt{\frac{\ds\Swyy(\omega)}
               {\ds\Swxy(\omega)}}
  $}
\end{definition}

%---------------------------------------
\begin{proposition}
\footnote{
  \citePpc{zheng2002}{918}{``1.4 Geometric Mean Estimator $H_4$"}
  }
\label{prop:Hgm}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\propbox{\begin{array}{rc>{\ds}>{\ds}lM}
  \estHgm(\omega) &=& \sqrt{\estHa(\omega)\estHb(\omega)}
    & (\fnctb{geometric mean} of $\estHa$ and $\estHb$)
\end{array}}
\end{proposition}
\begin{proof}
\begin{align*}
  \estHgm(\omega)
    &= \sqrt{\frac{\Swyy(\omega)\Swxy^\ast(\omega)}{\Swxx(\omega)\Swxy(\omega)}}
    && \text{by definition of $\estHgm$}
    && \text{\xref{def:Hgm}}
  \\&= \sqrt{
         \frac{\Swxy^\ast(\omega)}
              {\Swxx(\omega)}
         \frac{\Swyy(\omega)}
              {\Swxy(\omega)}
            }
  \\&= \sqrt{\estHa(\omega)\estHb(\omega)}
    && \text{by definitions of $\estHa$ and $\estHb$}
    && \text{\xxref{def:H1}{def:H2}}
  \\&= \mathrlap{\text{\fnctb{Geometric mean} of $\estHa(\omega)$ and $\estHb(\omega)$}}
\end{align*}
\end{proof}

Note that the \fncte{geometric mean estimator} \xref{def:Hgm} and \fncte{transmissibility} \xref{def:Txy}
are closely related (next).
%---------------------------------------
\begin{proposition}
\label{prop:HgmTxy_mag}
%---------------------------------------
Let $\phi(\omega)$ be the \fncte{phase} of $\Swxy(\omega)$ such that
$\ds\Swxy(\omega) \eqd \abs{\Swxy(\omega)}e^{-i\phi(\omega)}$
\\
\propbox{\begin{array}{rc>{\ds}ll}
  \estHgm(\omega) &=& \Twxy(\omega)\,e^{-i\phi(\omega)} &
  \brp{\begin{array}{rclMD}
    \abs{ \estHgm(\omega)} &=& \Twxy(\omega)        & is the \fncte{magnitude} of $\estHgm(\omega)$ & and\\
    \angle\estHgm(\omega)  &=& -\angle\Swxy(\omega) & is the \fncte{phase}     of $\estHgm(\omega)$
  \end{array}}
\end{array}}
\end{proposition}
\begin{proof}
Let $\phi(\omega)$ be the \fncte{phase} of
\begin{align*}
  \estHgm(\omega)
    &\eqd \sqrt{\estHa(\omega)\estHb(\omega)}
    && \text{by definition of $\estHgm$}
    && \text{\xref{def:Hgm}}
  \\&\eqd \sqrt{\frac{\Swxy^\ast(\omega)}{\Swxx(\omega)}\frac{\Swyy(\omega)}{\Swxy(\omega)}}
    && \text{by definitions of $\estHa$ and $\estHb$}
    && \text{\xxref{def:H1}{def:H2}}
  \\&= \sqrt{\frac{\Swyy(\omega)}{\Swxx(\omega)}\frac{\Swxy^\ast(\omega)}{\Swxy(\omega)}}
  \\&= \Twxy(\omega)\sqrt{\frac{\Swxy^\ast(\omega)}{\Swxy(\omega)}}
    && \text{by definition of $\Twxy$}
    && \text{\xref{def:Txy}}
  \\&= \Twxy(\omega)\sqrt{\frac{\abs{\Swxy(\omega)}e^{-i\phi(\omega)}}{\abs{\Swxy(\omega)}e^{i\phi(\omega)}}}
    && \text{where $\Swxy(\omega)\eqd{\abs{\Swxy(\omega)}e^{-i\phi(\omega)}}$}
  \\&= \Twxy(\omega)\sqrt{e^{-i2\phi(\omega)}}
  \\&= \Twxy(\omega)\,e^{-i\phi(\omega)}
\end{align*}
\end{proof}

%---------------------------------------
\begin{remark}
\label{rem:HgmTxy_gen}
%---------------------------------------
\fncte{Transmissibility} $\Twxy$ is a kind of ``black sheep" of the system identification function family.
All the other members of this family ($\estHa$, $\estHb$, $\estHv$, $\estHs$) are \prope{complex-valued},
but $\Twxy$ is only \prope{real-valued}---a seemingly ordinary Joe born into a super-hero family.
But \pref{prop:HgmTxy_mag} suggests that $\Twxy$ is not simply a ``black sheep", but
rather a ``dark horse" with abilities that can easily be unleashed by slight redefinition.
In particular, \pref{prop:HgmTxy_mag} demonstrates that $\Twxy$ is the \hie{magnitude} of the geometric mean of
$\estHa$ and $\estHb$.
We can thus justifiably define a \fnctd{complex transmissibility} function as $\estHgm$\ldots
and the magnitude of this \fncte{complex transmissibility} function is the \fncte{ordinary transmissibility}
function of \prefpp{def:Txy}.
\rembox{\begin{array}{Mc>{\ds}l}
  \fnctd{complex transmissibility} $\Twxy'(\omega)$ &\eqd& \estHgm(\omega)
\end{array}}
\end{remark}

%---------------------------------------
\begin{definition}
\footnote{
  \citerppgc{liang2015}{363}{365}{1498702341}{7.4.2 Coherence function},
  \citerpgc{ewins1986}{131}{0863800173}{$\gamma^2=\ffrac{H_1(\omega)}{H_2(\omega)}$ (3.8)}
  }
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
  The \fnctd{complex coherence} function is defined as
  \\\indentx$\ds\Cxy(\omega) \eqd \frac{\Swyx(\omega)}{\Swxx(\omega)\Swyy(\omega)}$
  }
\end{definition}

%---------------------------------------
\begin{remark}
\label{rem:HgmTxyCxy}
%---------------------------------------
Note that the \fncte{complex transmissibility} $\Twxy'$ of \pref{rem:HgmTxy_gen}
provides a nice mathematical symmetry (always a good sign of good direction) with coherence
in the system identification family tree.
In particular, note that the following:
\remboxt{
  $\Cxy \eqd \sqrt{\frac{\estHa^\ast}{\estHb}}$ whereas $\Twxy' \eqd \sqrt{\estHa \estHb}$
  }
\end{remark}
\begin{proof}
\begin{align*}
  \sqrt{\frac{\estHa^\ast(\omega)}{\estHb(\omega)}}
    && \text{by definition of $\estHgm$}
    && \text{\xref{def:Hgm}}
  %\\&\eqd \sqrt{\frac{\Swxy^\ast(\omega)}{\Swxx(\omega)}\frac{\Swyy(\omega)}{\Swxy(\omega)}}
  %  && \text{by definitions of $\estHa$ and $\estHb$}
  %  && \text{\xxref{def:H1}{def:H2}}
  %\\&= \sqrt{\frac{\Swyy(\omega)}{\Swxx(\omega)}\frac{\Swxy^\ast(\omega)}{\Swxy(\omega)}}
  %\\&= \Twxy(\omega)\sqrt{\frac{\Swxy^\ast(\omega)}{\Swxy(\omega)}}
  %  && \text{by definition of $\Twxy$}
  %  && \text{\xref{def:Txy}}
  %\\&= \Twxy(\omega)\sqrt{\frac{\abs{\Swxy(\omega)}e^{-i\phi(\omega)}}{\abs{\Swxy(\omega)}e^{i\phi(\omega)}}}
  %  && \text{where $\Swxy(\omega)\eqd{\abs{\Swxy(\omega)}e^{-i\phi(\omega)}}$}
  %\\&= \Twxy(\omega)\sqrt{e^{-i2\phi(\omega)}}
  %\\&= \Twxy(\omega)\,e^{-i\phi(\omega)}
\end{align*}
\end{proof}

% ___________________________________________
%|D|                Syy(w) Sxy*(w)           |
%|E| Hhm(w) = ------------------------------ |
%|F|           Sxx(w) Syy(w) + | Sxy(w) |^2  |
%|_|_________________________________________|
\begin{definition}                           %
\label{def:Hhm}                              %
%--------------------------------------------%
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
    The \fnctd{Harmonic mean transfer function estimate $\estHhm(\omega)$}
    of $\estHa(\omega)$ and $\estHb(\omega)$ is defined as
  \\\indentx$
  \estHhm(\omega) \eqd
    \frac{\ds2\Swyy(\omega)\Swxy^\ast(\omega)}
         {\ds\Swxx(\omega)\Swyy(\omega) + \abs{\Swxy(\omega)}^2}
  $}
\end{definition}

% ________________________________________________________
%| |                  2                                   |
%|P|          -----------------                           |
%|R| Hhm(w) =    1         1                              |
%|O|           ------ + ------                            |
%|P|            H1(w)    H2(w)                            |
%|_|______________________________________________________|
\begin{proposition}                                       %
\footnote{                                                %
  \citePc{carne2006}{$H_C = [H_A^{-1} + H_B^{-1}]^{-1}$}  %
  %\citergc{ewins1985}{086380036X}{cited by Carne (2006)} %
  %\citerpgc{ewins1986}{???}{0863800173}{...}             %
  }                                                       %
\label{prop:Hhm}                                          %
%---------------------------------------------------------%
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\propbox{\begin{array}{rc>{\ds}lM}
  \estHhm(\omega) &=& \frac{2}{\frac{1}{\ds\estHa(\omega)} + \frac{1}{\ds\estHb(\omega)}}
                  & (\fnctb{Harmonic mean} of $\estHa$ and $\estHb$)
\end{array}}
\end{proposition}
\begin{proof}
\begin{align*}
  \estHhm(\omega)
    &\eqd \frac{\ds2\Swyy(\omega)\Swxy^\ast(\omega)}
               {\ds\Swxx(\omega)\Swyy(\omega) + \abs{\Swxy(\omega)}^2}
    && \text{by definition of $\estHhm$}
    && \text{\xref{def:Hhm}}
  \\&= \frac{2}{\frac{\ds\Swxx(\omega)\Swyy(\omega) + \abs{\Swxy(\omega)}^2}
                     {\ds\Swyy(\omega)\Swxy^\ast(\omega)}}
  \\&= \frac{2}{                                     %
         \frac{\ds\Swxx(\omega)}                     %            2
              {\Swxy^\ast(\omega)}                   %  -------------------
         +                                           %   Sxx(w)     Sxy(w)
         \frac{\ds\Swxy(\omega)}                     %  -------- + --------
              {\ds\Swyy(\omega)}                     %   Sxy*(w)    Syy(w)
               }                                     %
  \\&= \frac{2}{                                     %           2
         \frac{1}{\estHa(\omega)}                    %   -----------------
        +\frac{1}{\estHb(\omega)}}                   %      1         1
    && \text{by definitions of $\estHa$ and $\estHb$}%    ------ + ------
    && \text{\xxref{def:H1}{def:H2}}                 %     H1(w)    H2(w)
  \\&= \mathrlap{\text{\fnctb{Harmonic mean} of $\estHa(\omega)$ and $\estHb(\omega)$}}
\end{align*}
\end{proof}

% ______________________________________
%|T|                                    |
%|H| H1 <= Hhm <= Hgm <= Ham <= H2      |
%|M|                                    |
%|_|____________________________________|
\begin{theorem}                         %
\label{thm:H_ineq}                      %
%---------------------------------------%
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\thmbox{
  \abs{\estHa(\omega)}  \le
  \abs{\estHhm(\omega)} \le
  \abs{\estHgm(\omega)} \le
  \abs{\estHam(\omega)} \le
  \abs{\estHb(\omega)}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item lemma: \label{ilem:H1H2} $\estHa(\omega)\le\estHb(\omega)$. Proof:
    \begin{align*}
      \abs{\estHa}
        &\eqd \abs{\frac{\Swyx(\omega)}{\Swxx(\omega)}}
        && \text{by definition of $\estHa$}
        && \text{\xref{def:H1}}
      \\&= \abs{\frac{\inprod{\rvy}{\rvx}}{\norm{\rvx}^2}}
      \\&= \frac{\abs{\inprod{\rvy}{\rvx}}}{\norm{\rvx}^2}
      \\&\le \frac{\abs{\inprod{\rvy}{\rvx}}}{\norm{\rvx}^2}
             \abs{\frac{\norm{\rvx}\norm{\rvy}}{\inprod{\rvy}{\rvx}}}^2
        && \text{by \thme{Cauchy Schwartz inequality}}
        && \text{\prefp{thm:cs}}
      \\&= \frac{\norm{\rvy}^2}{\abs{\inprod{\rvy}{\rvx}}}
      \\&= \abs{\frac{\norm{\rvy}^2}{\inprod{\rvx}{\rvy}}}
      \\&= \abs{\frac{\Swyy(\omega)}{\Swxy(\omega)}}
      \\&= \abs{\estHb}
        && \text{by definition of $\estHb$}
        && \text{\xref{def:H2}}
    \end{align*}
  \item remainder of the proof:
    \begin{align*}
      \abs{\estHa(\omega)}
        &= \min\setn{\estHa(\omega),\,\estHb(\omega)}
        && \text{by \pref{ilem:H1H2}}
      \\&\le \abs{\estHhm(\omega)}
        && \text{by \prefp{cor:hm_gm_am}}
		&& \text{with $\lambda_1=\lambda_2=\sfrac{1}{2}$}
      \\&\le \abs{\estHgm(\omega)}
        && \text{by \prefp{cor:hm_gm_am}}
		&& \text{with $\lambda_1=\lambda_2=\sfrac{1}{2}$}
      \\&\le \abs{\estHam(\omega)}
        && \text{by \prefp{cor:hm_gm_am}}
		&& \text{with $\lambda_1=\lambda_2=\sfrac{1}{2}$}
      \\&\le \max\setn{\estHa(\omega),\,\estHb(\omega)}
        && \text{by \prefp{cor:hm_gm_am}}
		&& \text{with $\lambda_1=\lambda_2=\sfrac{1}{2}$}
      \\&= \abs{\estHb(\omega)}
        && \text{by \pref{ilem:H1H2}}
    \end{align*}
\end{enumerate}
\end{proof}

% _______________________________________________________________________
%| |                               ____________________________________  |
%|D|                           \  /[               ]2      |        |2   |
%|E|          Syy(w) - Sxx(w) + \/ [Syy(w) - Sxx(w)]   + 4 | Sxy(w) |    |
%|F| Hv(w) = ----------------------------------------------------------- |
%| |                                2 Sxy(w)                             |
%|_|_____________________________________________________________________|
\begin{definition}                                                       %
\footnote{                                                               %
  \citePpc{white2006}{679}{(6)},                                         %
  \citerpgc{shin2008}{294}{0470725648}{(9.69)}                           %
  }                                                                      %
\label{def:Hv}                                                           %
%------------------------------------------------------------------------%
\defboxt{
  The \fnctd{Total Least Squares transfer function estimate $\estHv(\omega)$} is defined as
  \\\indentx$\ds
  \estHv(\omega)
    = \frac{\ds\Swyy(\omega)- \Swxx(\omega) +
            \sqrt{\brs{\Swyy(\omega) -  \Swxx(\omega)}^2 + 4 \abs{\Swxy(\omega)}^2}
           }{\ds2\Swxy(\omega)}
  $}
\end{definition}

The estimate $\estHv$ and it's refusal to assume zero input or output noise
was definitely a step in the right direction.
But it still has the limiting assumption that the input and output noise power are equal.
And what if it's not?
Enter Wicks and Vold who in 1986 introduced such an estimator (next definition)
that makes no such assumption.
It features a \vale{scaling parameter} $s$ in the range $\intco{0}{\infty}$
that can be adjusted depending on the ratio of output and input noise.
And as it turns out, $\estHs = \estHb$ when $s=0$, $\estHs=\estHv$ when $s=1$, and
$\estHs\to\estHa$ as $s\to\infty$. % \xref{thm:Hs}.


% ________________________________________________________________________________
%| |                                  __________________________________________  |
%|D|                    2         \  /[          2       ]2       2 |        |2   |
%|E|          Syy(w) - s  Sxx(w) + \/ [Syy(w) - s  Sxx(w)]   + 4 s  | Sxy(w) |    |
%|F| Hs(w) = -------------------------------------------------------------------- |
%| |                                      2 Sxy(w)                                |
%|_|______________________________________________________________________________|
\begin{definition}
\footnote{                                                                        %
  \citerpgc{shin2008}{293}{0470725648}{(9.67) with $\kappa(\omega)=s^2$},         %
  \citePpc{white2006}{679}{(6) with $\kappa(\omega)=s^2$},                        %
  \citePc{leclere2012}{(10) $\kappa(f)=1/s^2$ and $x$ and $y$ swapped},           %
  \citePc{zheng2002}{918}{10), seems to differ}                                   %
  }                                                                               %
\label{def:Hs}                                                                    %
%---------------------------------------------------------------------------------%
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
  The \fnctd{transfer function estimate $\estHs(\omega)$}
  with \vald{scaling parameter} $s\in\intco{0}{\infty}$ is defined as
  \\\indentx$\ds
  \estHs(\omega)
    = \frac{\Swyy(\omega)- s^2 \Swxx(\omega) +
            \sqrt{\brs{\Swyy(\omega) -  s^2 \Swxx(\omega)}^2 + 4 s^2 \abs{\Swxy(\omega)}^2}
           }{2\Swxy(\omega)}
  $}
\end{definition}
% _______________________________
%|P|                             |
%|R| Hs(w; s=0)     = H2(w)      |
%|O| Hs(w; s=1)     = Hv(w)      |
%|P| Hs(w; s=infty) = H1(w)      |
%|_|_____________________________|
\begin{proposition}              %
\label{prop:Hs_lim}              %
%--------------------------------%
Let $\estHs(\omega)$ be defined as in \prefpp{def:Hs}.
\propbox{\begin{array}{>{\ds}rc>{\ds}l}
    \brlr{\estHs(\omega; s)}_{s=0}     &=& \estHb(\omega)
  \\\brlr{\estHs(\omega; s)}_{s=1}     &=& \estHv(\omega)
  \\\lim_{s\to\infty}\estHs(\omega; s) &=& \estHa(\omega)
\end{array}}
\end{proposition}
\begin{proof}
\begin{align*}
  &\brlr{\estHs(\omega; s)}_{s=0}
  \\&\eqd \brlr{
       \frac{\Swyy(\omega)- s^2 \Swxx(\omega) +
             \sqrt{\brs{\Swyy(\omega) -  s^2 \Swxx(\omega)}^2 + 4 s^2 \abs{\Swxy(\omega)}^2}
            }{2\Swxy(\omega)}}_{s=0}
    && \text{by def. of $\estHs$}
    && \text{\xref{def:Hs}}
  \\&= \frac{\Swyy(\omega)- 0 + \sqrt{\brs{\Swyy(\omega) -  0}^2 + 0}}
            {2\Swxy(\omega)}
  \\&= \frac{\Swyy(\omega)}
            {\Swxy(\omega)}
  \\&\eqd \estHb(\omega)
    && \text{by def. of $\estHb$}
    && \text{\xref{def:H2}}
  \\
  \\
  &\brlr{\estHs(\omega; s)}_{s=1}
  \\&\eqd \brlr{
       \frac{\Swyy(\omega)- s^2 \Swxx(\omega) +
             \sqrt{\brs{\Swyy(\omega) -  s^2 \Swxx(\omega)}^2 + 4 s^2 \abs{\Swxy(\omega)}^2}
            }{2\Swxy(\omega)}}_{s=1}
    && \text{by def. of $\estHs$}
    && \text{by \xref{def:Hs}}
  \\&= \frac{\Swyy(\omega)- \Swxx(\omega) +
             \sqrt{\brs{\Swyy(\omega) -  \Swxx(\omega)}^2 + 4 \abs{\Swxy(\omega)}^2}
            }{2\Swxy(\omega)}
  \\&\eqd \estHv(\omega)
    && \text{by def. of $\estHv$}
    && \text{\xref{def:Hv}}
  \\
  \\
  &\lim_{s\to\infty}\estHs(\omega; s)
  \\&\eqd \lim_{s\to\infty}
       \frac{\Swyy(\omega)- s^2 \Swxx(\omega) + \sqrt{\brs{\Swyy(\omega) -  s^2 \Swxx(\omega)}^2 + 4 s^2 \abs{\Swxy(\omega)}^2}
            }{2\Swxy(\omega)}
    && \text{by def. of $\estHs$}
    && \text{by \xref{def:Hs}}
  \\&\eqd \lim_{p\to0}
       \frac{\Swyy(\omega)- \sfrac{1}{p} \Swxx(\omega) + \sqrt{\brs{\Swyy(\omega) -  \sfrac{1}{p} \Swxx(\omega)}^2 + 4 \sfrac{1}{p} \abs{\Swxy(\omega)}^2}
            }{2\Swxy(\omega)}
    && \text{where $p\eqd\sfrac{1}{s^2}$}
  \\&= \lim_{p\to0}
       \frac{p\Swyy(\omega)-  \Swxx(\omega) + \sqrt{\brs{p\Swyy(\omega) -  \Swxx(\omega)}^2 + 4p\abs{\Swxy(\omega)}^2}
            }{2p\Swxy(\omega)}
    && \text{by mult. by $1=\frac{p}{p}$}
  \\&= \lim_{p\to0}
       \frac{\ddp\brs{p\Swyy(\omega)-  \Swxx(\omega) + \sqrt{\brs{p\Swyy(\omega) -  \Swxx(\omega)}^2 + 4p\abs{\Swxy(\omega)}^2}}
            }{\ddp\brs{2p\Swxy(\omega)}}
    &&    \text{by \thme{l'H{/<o}pital's rule}}
  \\&= \lim_{p\to0}
       \frac{\ds\Swyy(\omega) +
            \frac{2\brs{p\Swyy(\omega) -  \Swxx(\omega)}\Swyy(\omega) + 4\abs{\Swxy(\omega)}^2}
                 {2\sqrt{\brs{p\Swyy(\omega) -  \Swxx(\omega)}^2 + 4p\abs{\Swxy(\omega)}^2}}
            }{\ds2\Swxy(\omega)}
  \\&= \frac{\ds\Swyy(\omega) +
            \frac{-2\Swxx(\omega)\Swyy(\omega) + 4\abs{\Swxy(\omega)}^2}
                 {2\sqrt{\brs{ -  \Swxx(\omega)}^2 }}
            }{\ds2\Swxy(\omega)}
  \\&= \frac{2\Swyy(\omega)\Swxx(\omega) - 2\Swxx(\omega)\Swyy(\omega) + 4\abs{\Swxy(\omega)}^2}
            {4\Swxx(\omega)\Swxy(\omega)}
  \\&= \frac{4\Swxy^\ast(\omega)\Swxy(\omega)}
            {4\Swxx(\omega)\Swxy(\omega)}
  \\&= \frac{\Swxy^\ast(\omega)}
            {\Swxx(\omega)}
  \\&\eqd \estHa(\omega)
    && \text{by def. of $\estHa$}
    && \text{\xref{def:H1}}
  %\\
  %\lim_{s\to\infty}\estHs(\omega; s)
  %  &\eqd \lim_{s\to\infty}
  %        \frac{2s^2\Swyx(\omega)}
  %             {s^2\Swxx(\omega)-\Swyy(\omega) + \sqrt{\brs{s^2\Swxx(\omega)-\Swyy(\omega)}^2 + 4s^2\abs{\Swxy(\omega)}^2}}
  %  && \text{by definition of $\estHs$}
  %  && \text{\xref{def:Hs}}
  %\\&= \lim_{s\to\infty}
  %     \frac{2\Swyx(\omega)}
  %          {\Swxx(\omega)-\frac{1}{s^2}\Swyy(\omega) + \sqrt{\frac{1}{s^4}\brs{s^2\Swxx(\omega)-\Swyy(\omega)}^2 + \frac{4}{s^4}s^2\abs{\Swxy(\omega)}^2}}
  %\\&= \lim_{s\to\infty}
  %     \frac{2\Swyx(\omega)}
  %          {\Swxx(\omega)-\frac{1}{s^2}\Swyy(\omega) + \sqrt{\brs{\frac{1}{s^2}s^2\Swxx(\omega)-\frac{1}{s^2}\Swyy(\omega)}^2 + \frac{4}{s^2}\abs{\Swxy(\omega)}^2}}
  %\\&= \frac{2\Swyx(\omega)}
  %           {\Swxx(\omega)-0 + \sqrt{\brs{\Swxx(\omega)-0}^2 + 0}}
  %\\&= \frac{\Swyx(\omega)}
  %          {\Swxx(\omega)}
  %\\&\eqd \estHa(\omega)
  %  && \text{by definition of $\estHa(\omega)$}
  %  && \text{\xref{def:H1}}
\end{align*}
\end{proof}

Wicks and Vold's $\estHs$ estimates $\FH$ using a single parameter $s$
that is \prope{constant} with respect to frequency.
This is suitable for additive white noise $\rvw(n)$, which has constant spectral density
$\Swww(\omega)=\pvar_w$.
However, not all noise is white noise.
And later in 2006, White, Tan and Hammond proposed a generalization (next definition) of $\estHs$ with a scaling
function $\kappa(\omega)$---that is, one that is not in general constant with respect to frequency
and is thus suitable in the presence of \prope{colored} noise.
% ___________________________________________________________________________________
%| |                                   ____________________________________________  |
%|D|                               \  /[                   ]2          |        |2   |
%|E|          Syy(w) - k(w)Sxx(w) + \/ [Syy(w) - k(w)Sxx(w)]   + 4 k(w)| Sxy(w) |    |
%|F| Hk(w) = --------------------------------------- ----------------- ------------- |
%| |                                       2 Sxy(w)                                  |
%|_|_________________________________________________________________________________|
\begin{definition}                                                                   %
\footnote{                                                                           %
  \citePpc{white2006}{679}{(6)},                                                     %
  \citerpgc{shin2008}{293}{0470725648}{(9.67)}                                       %
  }                                                                                  %
\label{def:Hkappa}                                                                   %
%------------------------------------------------------------------------------------%
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
  The \fnctd{transfer function estimate $\estHkp(\omega; \kappa)$}
  with \vald{scaling function} $\kappa(\omega)$ is defined as
  \\\indentx$\ds
  \estHkp(\omega; \kappa) \eqd
      \frac{\Swyy(\omega)-\kappa(\omega)\Swxx(\omega) +
            \sqrt{\brs{\Swyy(\omega) - \kappa(\omega)\Swxx(\omega)}^2 +
                  4\kappa(\omega)\abs{\Swxy(\omega)}^2}
           }
           {2\Swxy(\omega)}$
  }
\end{definition}

The previous estimators all assumed two signals: an input $\rvx(n)$ and an output $\rvy(n)$.
However, in many practical systems, there is a third signal that is ``driving" the system.
In 1984 Goyder proposed an estimator (next definition) that is based on three signals.
%---------------------------------------
\begin{definition}[Three channel estimate]
\footnote{
  \citerpgc{shin2008}{297}{0470725648}{$H_3(f)=\ffrac{S_{ry}(f)}{S_{rx}(f)}$ (9.78)},
  \citePpc{cobb1988}{4}{$^c\hat{H}(f)=\ffrac{\hat{G}_{ys}(f)}{\hat{G}_{xs}(f)}$ (1.4)},
  \citePpc{goyder1984}{440}{$H(i\omega)=\ffrac{S_{qz}}{S_{pz}}$ (5)},
  \citePpc{cobb1990}{450}{(1)},
  \citerpgc{pintelon2012}{241}{1118287398}{$\hat{G}(\Omega_k)=\hat{G}_{ry}(\Omega_k)\hat{G}_{ru}^{-1}(\Omega_k)$ (7-49)}
  }
\label{def:Hc}
%---------------------------------------
Let $\opS$ be a system with input $\rvx(n)$, output $\rvy(n)$, and a driver $\rvp(n)$.
\defboxt{
  The \fnctd{transfer function estimate $\estHc(\omega)$} is defined as
  \\\indentx$\ds
  \estHc(\omega) \eqd \frac{\Swxx[py](\omega)}{\Swxx[px](\omega)}
  $
  }
\end{definition}

%=======================================
\section{Least squares estimates}
%=======================================
\begin{minipage}{\tw-50mm}
Let $\opS$ be the \structe{system} illustrated to the right.
\textbf{If} there is no measurement noise on the input and output and \textbf{if}
$\opH$ is \prope{linear time invariant}, then
$\FH = \Swxy/\Swxx$ \xref{cor:Rxyh}.
But what if there is output measurement noise?
And what if $\opH$ is not \prope{LTI}?
What is the best least-squares estimate of $\FH$?
The answer is the estimator $\estHa$ \xref{def:H1},
as demonstrated in the next theorem.
\end{minipage}
\hfill\tbox{\includegraphics{graphics/sysHw_xy.pdf}}

\begin{figure}
  \centering
  \begin{tabular}{|c|c|}
    \hline
     \tbox{\includegraphics{graphics/opT_estH_cnoise.pdf}}
    &\tbox{\includegraphics{graphics/opT_estH_mnoise.pdf}}
    \\(A) LS estimation for communication  &(B) LS estimation for measurement
    \\    noise system \xref{cor:H1LSa}    &    noise system \xref{cor:H1LSb}
    \\\hline
  \end{tabular}
  \caption{Least Square estimation \xref{thm:estHls}\label{fig:estHls}}
\end{figure}
%---------------------------------------
\begin{theorem}[Optimal LTI least-squares estimate $\estH$ of $\opH$]
\label{thm:estHls}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated in \prefp{fig:estHls} (A) or (B).
Let $\norm{\rvX}^2\eqd\pE\brs{\rvX\rvX^\ast}$.
\thmbox{
  \brbr{\begin{array}{FMMD}
      (A).& $\rvx$, $\rvu$, and $\rvv$ are & \prope{WSS}          & and
    \\(B).& $\rvx$, $\rvu$, and $\rvv$ are & \prope{uncorrelated} & and
    \\(C).& $\pE\rvu=\pE\rvu=0$            & (\prope{zero-mean})  & and
    \\(D).& $\estHls$ is \prope{LTI}         &                      &
  \end{array}}
  \implies
  \brbl{\begin{array}{c}
    \ds\argmin_{\estH}\norm{\FR(\omega)-\FQ(\omega)}^2 = \frac{\Swyp(\omega)}{\Swpp(\omega)}
    \\
    \brp{\begin{array}{N}
      \scs The optimal \prope{LTI} \prope{least-squares} estimate of $\opT$ is
      \\$\estHls(\omega)=\ffrac{\Swyp(\omega)}{\Swpp(\omega)}$
    \end{array}}
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Define the \fncte{cost function} $\fCost\brp{\estH}$ for spectral \prope{least-squares} estimate as \label{item:H1LS_cost_def}
    $\fCost\brp{\estH} \eqd \norm{\FR(\omega)-\FQ(\omega)}^2$
    \\where $\FR(\omega)$ is the \ope{DTFT} \xref{def:dtft} of $\rvw(n)$
      and   $\FQ(\omega)$ is the \ope{DTFT} of $\rvq(n)$.

  \item lemma: $\fCost\brp{\estH}=\Swxx\abs{\estH}^2 - \estH(\omega)\Swxy - \brs{\estH\Swxy}^\ast + \Swqq$.
        Proof: \label{ilem:H1LS_cost}
    \begin{align*}
      \fCost\brp{\estH}
        &\eqd \norm{\FR(\omega)-\FQ(\omega)}^2
        && \text{by \pref{item:H1LS_cost_def}}
      \\&\eqd \inprod{\FR(\omega)-\FQ(\omega)}{\FR(\omega)-\FQ(\omega)}
      \\&\eqd \pE\brp{\brs{\FR(\omega)-\FQ(\omega)}\brs{\FR(\omega)-\FQ(\omega)}^\ast}
        && \text{by \prefp{thm:pEinprod}}
      \\&=\mathrlap{
              \pE\brs{\FR(\omega)\FR^\ast(\omega)}
            - \pE\brs{\FR(\omega)\FQ^\ast(\omega)}
            - \pE\brs{\FQ(\omega)\FR^\ast(\omega)}
            + \pE\brs{\FQ(\omega)\FQ^\ast(\omega)}
           }
      \\&= \Swrr(\omega) - \Swrq(\omega) - \Swrq^\ast(\omega) + \Swqq(\omega)
        && \text{by \thme{Wiener-Khinchin relationship}}
      \\&= \Swpp(\omega)\abs{\estH(\omega)}^2
         - \estH(\omega)\Swpy(\omega)
         - \brs{\estH(\omega)\Swpy(\omega)}^\ast
         + \Swqq(\omega)
        &&\text{by (A)--(D) and \prefpp{cor:GHz}}
      %\\&= \Swxx(\omega)\abs{\estH(\omega)}^2
      %   + \Swuu(\omega)\abs{\estH(\omega)}^2
      %   - \estH(\omega)\Swpy(\omega)
      %   - \brs{\estH(\omega)\Swpy(\omega)}^\ast
      %   + \Swqq(\omega)
      %  && \text{by (A)--(C) and \pref{cor:xvy}}
    \end{align*}

  \item lemma: $\estH_R = \ffrac{\Real\Swyp}{\Swpp}$. Proof: \label{ilem:H1LS_R}
    \begin{align*}
      0 &= \pderiv{}{\estH_R}\fCost\brp{\estH}
      \\&= \pderiv{}{\estH_R}\brp{
               \Swpp\abs{\estH}^2
            %+ \Swuu\abs{\estH}^2
             - \estH\Swpy
             - \estH^\ast\Swpy^\ast
             + \Swqq
             }
        && \text{by \prefp{ilem:H1LS_cost}}
      \\&= \pderiv{}{\estH_R}\brp{
               \Swpp\brs{\estH_R^2+\estH_I^2}
            %+ \Swuu\brs{\estH_R^2+\estH_I^2}
             - (\estH_R+i\estH_I)\Swpy
             - (\estH_R+i\estH_I)^\ast\Swpy^\ast
             + \Swqq
             }
      \\&= 2\estH_R\Swpp - \Swpy - \Swpy^\ast + \cancelto{0}{\pderiv{}{\estH_R}\Swqq}
        &&\text{because $\rvq$ does not vary with $\estH$}
      \\&= 2\estH_R\Swpp  - 2\Real\Swpy
      \\&= 2\estH_R\Swpp  - 2\Real\Swyp
        && \text{by \prefp{cor:Swxy_sym}}
      \\&\implies \boxed{\estH_R(\omega) = \frac{\Real\Swyp(\omega)}{\Swpp(\omega)}}
    \end{align*}

  \item lemma: $\estH_I = \ffrac{\Imag\Swyp}{\Swpp}$. Proof: \label{ilem:H1LS_I}
    \begin{align*}
      0
        &= \pderiv{}{\estH_I}\fCost\brp{\estH}
      \\&= \pderiv{}{\estH_I}
           \brp{\Swpp\abs{\estH}^2 - \estH\Swpy - \estH^\ast\Swpy^\ast + \Swqq}
        && \text{by \prefp{ilem:H1LS_cost}}
      \\&= \pderiv{}{\estH_I}\brs{
           \Swpp\brs{\estH_R^2+\estH_I^2}
        %+ \Swuu\brs{\estH_R^2+\estH_I^2}
         - (\estH_R+i\estH_I)\Swpy
         - (\estH_R-i\estH_I)\Swpy^\ast
         + \Swqq}
      \\&= \mathrlap{
           2\estH_I\Swpp - i\Swpy + i\Swpy^\ast + \cancelto{0}{\pderiv{}{\estH_R}\Swqq}
           \qquad\text{because $\rvq$ does not vary with $\estH$}
           }
      \\&= 2\estH_I\Swpp - 2i(i\Imag\Swpy)
      \\&= 2\estH_I\Swpp + 2i(i\Imag\Swyp)
        && \text{by \prefp{cor:Swxy_sym}}
      \\&= 2\estH_I\Swpp - 2\Imag\Swyp
      \\&\implies \boxed{\estH_I(\omega) = \frac{\Imag\Swyp(\omega)}{\Swpp(\omega)}}
     \end{align*}

  \item Proof that $\estH = \ffrac{\Swyx}{\Swpp} \eqd \estHa$:
    \begin{align*}
      \estH(\omega)
        &= \estH_R(\omega) + i\estH_I(\omega)
      \\&= \frac{ \Real\Swyp(\omega)}{\Swpp(\omega)}
         + \frac{i\Imag\Swyp(\omega)}{\Swpp(\omega)}
        && \text{by \pref{ilem:H1LS_R} and \pref{ilem:H1LS_I}}
      \\&= \frac{\Swyp(\omega)}{\Swpp(\omega)}
    \end{align*}
\end{enumerate}
\end{proof}

It follows immediately from \prefpp{thm:estHls} that, in the special case
of no input noise ($\rvu(n)=0$), $\estHa$ is the best
least-squares estimate of $\FH$ (next corollary).
Moreover, in the special case of $\opT=\FH$ being \prope{linear time invariant} (\prope{LTI}),
then $\estHa$ is not only the best least-squares estimate, but $\estHa$ actually exactly equals $\FH$
\xref{cor:estH1}.

%---------------------------------------
\begin{corollary}
\label{cor:H1LSa}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated in \prefp{fig:estHls} (A).
\corbox{
  \begin{array}{F>{\ds}rc>{\ds}l}
    (1).&\brb{\text{hypotheses of \prefp{thm:estHls}}}
        &\implies
        &\estHls(\omega) = \frac{\Swyx(\omega) + \Swyu(\omega)}{\Swxx(\omega)+\Swuu(\omega)}
    \\
    (2).&\brb{\text{hypotheses of \pref{thm:estHls} and $\vu(n)=0$}}
        &\implies
        &\estHls(\omega) = \frac{\Swyx(\omega)}{\Swxx(\omega)} = \estHa(\omega)
  \end{array}}
\end{corollary}
\begin{proof}
  \begin{align*}
    \text{(1):\quad}\estHls(\omega)
      &= \frac{\Swyp(\omega)}{\Swpp(\omega)}
      && \text{by \prefp{thm:estHls}}
    \\&= \frac{\Swyp(\omega)}{\Swxx(\omega)+\Swuu(\omega)}
    \\&= \frac{\Swyx(\omega)+\Swyu(\omega)}{\Swxx(\omega)+\Swuu(\omega)}
    \\
    \text{(2):\quad}\estHls(\omega)
      &= \frac{\Swyx(\omega)+\Swyu(\omega)}{\Swxx(\omega)+\Swuu(\omega)}
      && \text{by previous result}
      && \text{(1)}
    \\&= \frac{\Swyx(\omega)}{\Swxx(\omega)}
      && \text{by $\rvu(n)=0$ hypothesis}
    \\&\eqd \estHa(\omega)
      && \text{by definition of $\estHa$}
      && \text{\xref{def:H1}}
  \end{align*}
\end{proof}

%---------------------------------------
\begin{corollary}
\footnote{
  \citerppgc{bendat1980}{98}{100}{0471058874}{5.1.1 Optimal Character of Calculations; note: proof minimizing $\Swvv$ but yields same result},
  \citerppgc{bendat1993}{106}{109}{0471570559}{5.1.1 Optimality of Calculations}
  }
\label{cor:H1LSb}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated in \prefp{fig:estHls} (B).
\corbox{
  \begin{array}{F>{\ds}rc>{\ds}l}
    (1).&\brb{\text{hypotheses of \prefp{thm:estHls}}}
        &\implies
        &\estHls(\omega) = \frac{\Swyx(\omega)}{\Swxx(\omega)-\Swuu(\omega)}
    \\
    (2).&\brb{\text{hypotheses of \pref{thm:estHls} and $\vw(n)=0$}}
        &\implies
        &\estHls(\omega) = \frac{\Swyx(\omega)}{\Swxx(\omega)} = \estHa(\omega)
  \end{array}}
\end{corollary}
\begin{proof}
  \begin{align*}
    \text{(1):\quad}\estHls(\omega)
      &= \frac{\Swyp(\omega)}{\Swpp(\omega)}
      && \text{by \prefp{thm:estHls}}
    \\&= \frac{\Swyp(\omega)}{\Swxx(\omega)-\Swuu(\omega)}
    \\&= \frac{\Swyx(\omega)-\Swyu(\omega)}{\Swxx(\omega)-\Swuu(\omega)}
    \\&= \frac{\Swyx(\omega)-\cancelto{0}{\Swyu(\omega)}}{\Swxx(\omega)-\Swuu(\omega)}
    \\
    \text{(2):\quad}\estHls(\omega)
      &= \frac{\Swyx(\omega)}{\Swxx(\omega)-\Swuu(\omega)}
      && \text{by previous result}
      && \text{(1)}
    \\&= \frac{\Swyx(\omega)}{\Swxx(\omega)}
      && \text{by $\rvw(n)=0$ hypothesis}
    \\&\eqd \estHa(\omega)
      && \text{by definition of $\estHa$}
      && \text{\xref{def:H1}}
  \end{align*}
\end{proof}

%=======================================
\section{Alternate forms}
%=======================================
% ________________________________________________________________________________
%| |                                   2 s^2 Syx(w)                               |
%|P| Hs(w) = -------------------------------------------------------------------- |
%|R|                                  __________________________________________  |
%|O|           2                  \  /[ 2                ]2       2 |        |2   |
%|P|          s  Sxx(w) - Syy(w) + \/ [s  Sxx(w) - Syy(w)]   + 4 s  | Sxy(w) |    |
%|_|______________________________________________________________________________|
\begin{proposition}                                                               %
\footnote{                                                                        %
  \citerpgc{shin2008}{293}{0470725648}{(9.67)},                                   %
  %\citePpc{white2006}{679}{(6)},                                                 %
  \citePc{leclere2012}{(10) $\kappa(f)=1/s^2$ and $x$ and $y$ swapped}            %
  }                                                                               %
\label{prop:Hs}                                                                   %
%---------------------------------------------------------------------------------%
Let $\estHs(\omega; s)$ be defined as in \prefpp{def:Hs}.
\propbox{
  \estHs(\omega; s) =
  \frac{\ds 2s^2\Swyx(\omega)}
       {\ds  s^2\Swxx(\omega)-\Swyy(\omega) +
        \sqrt{\brs{s^2\Swxx(\omega)-\Swyy(\omega)}^2 + 4s^2\abs{\Swxy(\omega)}^2}
       }
  }
\end{proposition}
% _______________________________________________________________________
%| |                                   2 Syx(w)                          |
%|P| Hv(w) = ------------------------------------------------------------|
%|R|                               ____________________________________  |
%|O|                           \  /[               ]2      |        |2   |
%|P|          Sxx(w) - Syy(w) + \/ [Sxx(w) - Syy(w)]   + 4 | Sxy(w) |    |
%|_|_____________________________________________________________________|
\begin{proposition}                                                      %
\footnote{                                                               %
  \citePpc{white2006}{679}{(6)},                                         %
  \citerpgc{shin2008}{294}{0470725648}{(9.69)}                           %
  }                                                                      %
\label{prop:Hv}                                                          %
%------------------------------------------------------------------------%
Let $\estHv(\omega)$ be the \fnctd{Total Least Squares transfer function estimate} \xref{def:Hv}.
\propbox{
  \estHv(\omega) \eqd
  \frac{\ds2\Swyx(\omega)}
       {\ds\Swxx(\omega)-\Swyy(\omega) + \sqrt{\brs{\Swxx(\omega)-\Swyy(\omega)}^2 + 4\abs{\Swxy(\omega)}^2}}
  }
\end{proposition}
\begin{proof}
\begin{align*}
  \estHv(\omega)
    &= \brlr{\estHs(\omega; s)}_{s=1}
    && \text{by \prefp{prop:Hs_lim}}
  \\&= \brlr{
       \frac{\ds 2s^2\Swyx(\omega)}
         {\ds  s^2\Swxx(\omega)-\Swyy(\omega) +
          \sqrt{\brs{s^2\Swxx(\omega)-\Swyy(\omega)}^2 + 4s^2\abs{\Swxy(\omega)}^2}
         }}_{s=1}
    && \text{by \prefp{prop:Hs}}
  \\&= \frac{\ds2\Swyx(\omega)}
            {\ds\Swxx(\omega)-\Swyy(\omega) + \sqrt{\brs{\Swxx(\omega)-\Swyy(\omega)}^2 + 4\abs{\Swxy(\omega)}^2}}
\end{align*}
\end{proof}

%%---------------------------------------
%\begin{proposition}
%\footnote{
%  \citerpgc{shin2008}{293}{0470725648}{(9.67)},
%  %\citePpc{white2006}{679}{(6)},
%  }
%\label{prop:Hkappa}
%%---------------------------------------
%Let $\estHkp(\omega)$ be defined as in \prefpp{def:Hkappa}.
%\propboxt{
%  The \fnctd{transfer function estimate $\estHkp(\omega; \kappa)$} is defined as
%  \\\indentx$\ds
%  \estHkp(\omega) \eqd
%  \frac{2\kappa(\omega)\Swyx(\omega)}
%       {\kappa(\omega)\Swxx(\omega)-\Swyy(\omega) + \sqrt{\brs{\kappa(\omega)\Swxx(\omega)-\Swyy(\omega)}^2 + 4\kappa(\omega)\abs{\Swxy(\omega)}^2}}
%  $
%  }
%\end{proposition}

%=======================================
\section{Estimates for LTI systems}
%=======================================
\begin{figure}
  \centering
  \begin{tabular}{|c|c|}
    \hline
     \tbox{\includegraphics{graphics/opH_cnoise.pdf}}
    &\tbox{\includegraphics{graphics/opH_mnoise.pdf}}
    \\
      (A) \opd{communications LTI additive noise model}
     &(B) \opd{measurement    LTI additive noise model}
    \\
    \\\hline
  \end{tabular}
  \caption{Additive noise systems with \propb{LTI} operator $\opH$\label{fig:addnoise_LTI}}
\end{figure}

The previous section did assume the estimates $\estHa$ and $\estHb$  to be
\prope{linear time invariant} (\prope{LTI}), but it did \emph{not} assume that
the system transfer function $\opH$ itself to be \prope{LTI}.
But making the LTI assumption on $\opH$ yields some interesting and insightful
results, such as those in this section.

%---------------------------------------
\begin{theorem}[Estimating H in communication additive noise system]
\label{thm:estH1_cnoise}
\label{thm:estH2_cnoise}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated in \prefp{fig:addnoise_LTI} (A).
\thmboxt{
  $\brb{\begin{array}{FMMMD}
      (A).& $\opH$ is                               &\prope{linear time invariant} & (\prope{LTI}) & and
    \\(B).& $\rvx(n)$ is                            &\prope{wide-sense stationary} & (\prope{WSS}) & and
    \\(C).& $\rvx(n)$, $\rvu(n)$, and $\rvv(n)$ are &\prope{uncorrelated}          &               & 
  \end{array}}$
  \\\indentx$\implies
  \brb{\begin{array}{F >{\ds}rc>{\ds}l D}
    %\mc{3}{M}{\scs\prope{biased}:}
                           %  & \text{\scs\prope{over-estimated}}
                           %& & \text{\scs\prope{under-estimated}}
      (1).&\estHa(\omega) &=& \FH(\omega) &and
    \\(2).&\estHb(\omega) &=& \frac{\Swvv(\omega)}{\FH^\ast(\omega)\Swxx(\omega)}
                              +\FH(\omega)\brs{1 + \frac{\Swuu(\omega)}{\Swxx(\omega)}}
                           &
  \end{array}}$
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \estHa(\omega)
    &\eqd \frac{\Swyx(\omega)}{\Swxx(\omega)}
    && \text{by definition of $\estHa$}
    && \text{\xref{def:H1}}
  \\&= \frac{\FH(\omega)\Swxx(\omega)}{\Swxx(\omega)}
    && \text{by \prefp{cor:sysH_cnoise}}
  \\&= \FH(\omega)
  \\
  \estHb(\omega)
    &\eqd \frac{\Swyy(\omega)}
               {\Swxy(\omega)}
    && \text{by definition of $\estHb$}
    && \text{\xref{def:H2}}
  \\&= \frac{\Swyy(\omega)}
            {\FH^\ast(\omega)\Swxx(\omega)}
    && \text{by \prefp{cor:sysH_cnoise}}
  \\&= \frac{\Swvv(\omega)+\Swqq(\omega)}
            {\FH^\ast(\omega)\Swxx(\omega)}
    && \text{by \prefp{thm:xvy}}
  \\&= \frac{\Swvv(\omega)+\FH^\ast(\omega)\FH(\omega)\Swpp(\omega)}
            {\FH^\ast(\omega)\Swxx(\omega)}
    && \text{by \prefp{cor:Swxy}}
  \\&= \frac{\Swvv(\omega)}
            {\FH^\ast(\omega)\Swxx(\omega)}
      +\frac{\FH^\ast(\omega)\FH(\omega)\brs{\Swxx(\omega)+\Swuu(\omega)}}
            {\FH^\ast(\omega)\Swxx(\omega)}
  \\&= \frac{\Swvv(\omega)}
            {\FH^\ast(\omega)\Swxx(\omega)}
      +\FH(\omega)\brs{1 + \frac{\Swuu(\omega)}{\Swxx(\omega)}}
\end{align*}
\end{proof}


%---------------------------------------
\begin{theorem} [Estimating H in measurement additive noise system]
\footnote{
  \citerpgc{shin2008}{294}{0470725648}{$H_1(f)=H(f)$ (9.70); $H_2(f)=H(f)\brp{1+\ffrac{S_{n_yn_y}(f)}{S_{yy}(f)}}$ (9.71)},
  \citerpgc{shin2008}{294}{0470725648}{$H_1(f)=\ffrac{H(f)}{\brp{1+\ffrac{S_{n_xn_x}}{S_{xx}(f)}}}$ (9.72); $H_2(f)=H(f)$ (9.73)},
  \citePpc{mitchell1982}{277}{$\~H_1(f)=\ffrac{H_0(f)}{\brp{1+\ffrac{G_{nn}}{G_{uu}}}}$}
  \citePpc{mitchell1982}{278}{$\~H_2(f)=H_0(f)\brp{1+\ffrac{G_{mm}}{G_{vv}}}$}
  }
\label{thm:estH1_mnoise}
\label{thm:estH2_mnoise}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated in \prefp{fig:addnoise_LTI} (B).
\thmboxt{
  $\brb{\begin{array}{FMMD}
      (A).& $\opH$ is                               &\prope{linear time invariant} & and
    \\(B).& $\rvx(n)$ is                            &\prope{wide-sense stationary} & and
    \\(C).& $\rvx(n)$, $\rvu(n)$, and $\rvv(n)$ are &\prope{uncorrelated}          &
  \end{array}}$
  \\\indentx$\implies
  \brb{\begin{array}{F >{\ds}rc>{\ds}l DD}
     %(1).&\estHa(\omega) &=& \FH(\omega)\brs{\text{$\frac{1}{1+\frac{\ds\Swww(\omega)}{\ds\Swpp(\omega)}}$}}
      (1).&\estHa(\omega) &=& \FH(\omega)\brs{\abox{\frac{\ds1}{\ds1+\frac{\ds\Swww(\omega)}{\ds\Swpp(\omega)}}}}
                           &(\prope{under-estimated})
                           &and
   %\\(2).&\estHb(\omega) &=& \FH(\omega)\brs{1 + \frac{\Swvv(\omega)}{\Swqq(\omega)}}
    \\(2).&\estHb(\omega) &=& \FH(\omega)\brs{\abox{\ds1 + \frac{\ds\Swvv(\omega)}{\ds\Swqq(\omega)}}}
                           &(\prope{over-estimated})
                           &
  \end{array}}$
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \estHa(\omega)
    &\eqd \frac{\Swyx(\omega)}{\Swxx(\omega)}
    && \text{by definition of $\estHa$}
    && \text{\xref{def:H1}}
  \\&= \frac{\Swpp(\omega)\FH(\omega)}{\Swxx(\omega)}
    && \text{by \prefp{cor:sysH_addnoise_w}}
  \\&= \frac{\Swpp(\omega)\FH(\omega)}{\Swpp(\omega)+\Swww(\omega)}
    && \text{by hypothesis (A)}
    && \text{and \prefp{cor:Swxy}}
  \\&= \FH(\omega)\brs{\frac{1}{1+\frac{\Swww(\omega)}{\Swpp(\omega)}}}
  \\
  \estHb(\omega)
    &\eqd \frac{\Swyy(\omega)}{\Swxy(\omega)}
    && \text{by definition of $\estHb$}
    && \text{\xref{def:H2}}
  \\&= \frac{\Swqq(\omega) + \Swvv(\omega)}{\Swxy(\omega)}
    && \text{by hypothesis (C)}
    && \text{and \prefp{cor:xvy}}
  \\&= \frac{\Swqq(\omega) + \Swvv(\omega)}{\Swxq(\omega)}
    && \text{by hypothesis (C)}
    && \text{and \prefp{thm:sysT_addnoise_v}}
  \\&= \frac{\Swqq(\omega) + \Swvv(\omega)}{\Swpq(\omega)}
    && \text{by hypothesis (C)}
    && \text{and \prefp{thm:sysT_addnoise_w}}
  \\&= \frac{\Swqq(\omega) + \Swvv(\omega)}{\Swqq(\omega)/\FH(\omega)}
    && \text{by \prope{LTI} hypothesis (A)}
    && \text{and \prefp{cor:Swxy}}
  \\&= \FH(\omega)\brs{1 + \frac{\Swvv(\omega)}{\Swqq(\omega)}}
    && \text{by hypotheses (A) and (B)}
    && \text{and \prefp{cor:Swxy}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{corollary} %[LTI no output noise]
\label{cor:estH1}
\label{cor:estH2}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated in \prefpp{fig:addnoise_LTI}.
\corbox{\begin{array}{lcl}
  \brb{\begin{array}{FMM}
      (A).& \mc{2}{M}{hypotheses of \pref{thm:estH1_mnoise} \scs and}
    \\(B).& $\rvu(n)=\rvw(n)=0$               & (\prope{no input noise})
  \end{array}}
  &\implies&
  \brb{\begin{array}{>{\ds}rc>{\ds}lD}
    \estHa(\omega) &=& \FH(\omega) & (\prope{unbiased})
  \end{array}}
  \\
  \brb{\begin{array}{FMM}
      (A).& \mc{2}{M}{hypotheses of \pref{thm:estH1_mnoise} \scs and}
    \\(B).& $\rvv(n)=0$                       & (\prope{no output noise})
  \end{array}}
  &\implies&
  \brb{\begin{array}{>{\ds}rc>{\ds}lD}
    \estHb(\omega) &=& \FH(\omega) & (\prope{unbiased})
  \end{array}}
\end{array}}
\end{corollary}

%=======================================
%\section{Coherence}
%=======================================

%=======================================
\section{Beware of estimators}
%=======================================
Estimators yield, as the name implies, estimates.
These estimates in general contain some error.

%---------------------------------------
\begin{example}[The K=1 Welch estimate of coherence]
%---------------------------------------
Suppose we have two \prope{uncorrelated} stationary sequences $\rvx(n)$ and $\rvy(n)$. Then, there
CSD $\Sxy(\omega)$ should be $0$ because
\begin{align*}
  \Sxy(\omega)
    &\eqd \opDTFT\pE\Rxy(m)
  \\&=    \opDTFT\pE\brs{x(n)y[n+m]}
  \\&=    \opDTFT\brs{\pEx(n)} \brs{\pEy[n+m]}
  \\&=    \opDTFT\brs{0} \brs{0}
  \\&=    0
\end{align*}

This will give a coherence of $0$ also:
\[ C(\omega) = \frac{\Sxy}{\sqrt{\Sxx\Syy}} = 0\]

However, the Welch estimate with $K=1$ will yield
\begin{align*}
  \abs{C(\omega)}
    &= \abs{\frac{\ds\Sxy}{\sqrt{\ds\Sxx\Syy}}}
  \\&= \abs{\frac{\ds (\opFT x)(\opFT y)^\ast}{\sqrt{\ds\abs{\opFT x}^2\abs{\opFT y}^2}}}
  \\&= 1
\end{align*}

\end{example}


\section{Estimating noise}

\begin{liste}
  %\item \citeP{shoo1988}
  \item \citeP{thong2001}
  \item \citeP{kim2003}
  \item \citeP{kamel2004}
\end{liste}