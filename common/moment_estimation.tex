%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%=======================================
\chapter{Moment Estimation}
%=======================================
%=======================================
\section{Mean Estimation}
%=======================================
%---------------------------------------
\begin{theorem}
\label{thm:mse_mean}
%---------------------------------------
Let $\ds\meanest\eqd\sum_{n=1}^\xN \lambda_n \rvx_n$ with $\sum_{n=1}^{\xN}\lambda_n=1$
be the \fncte{arithmetic mean} \xref{def:am}.

\thmbox{
  \brb{\begin{array}{FMD}
      (A).& $\seqn{\rvx_n}$ is \prope{wide sense stationary} & and
    \\(B).& $\mu\eqd\pE\rvx_n$                               & and
    \\(C).& $\seqn{\rvx_n}$ is \prope{uncorrelated}          & and
    \\(D).& \mc{2}{M}{$\ds\meanest\eqd\sum_{n=1}^\xN \lambda_n \rvx_n$\qquad(\fncte{arithmetic mean})}
  \end{array}}
  \implies
  \brb{\begin{array}{FrclDD}
      (1).& \pE\meanest    &=& \mean                               & (\prope{unbiased})   & and
    \\(2).& \var(\meanest) &=& \mc{2}{l}{\ds\sigma^2\sum_{n=1}^{\xN}\lambda_n^2 }         & and
    \\(3).& \mse(\meanest) &=& \mc{2}{l}{\ds\sigma^2\sum_{n=1}^{\xN}\lambda_n^2 }         &
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \pE\meanest
    &\eqd \pE \sum_{n\in\Z} \lambda_n\rvx_n
    && \text{by definition of \fncte{arithmetic mean}}
    && \text{\xref{def:am}}
  \\&=  \sum_{n\in\Z} \lambda_n\pE\rvx_n
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&=  \mean\sum_{n\in\Z}\lambda_n
    && \text{by \prope{WSS} hypothesis}
    && \text{(A)}
  \\&=  \mean
    && \text{by $\sum\lambda_n=1$ hypothesis}
    && \text{\xref{def:am}}
  \\
  \var(\meanest)
    &\eqd \pE\brp{\meanest-\pE\meanest}^2
    && \text{by definition of \fncte{variance}}
  \\&= \pE\brp{\meanest-\mean}^2
    && \text{by previous result}
  \\&= \pE\brp{\sum_{n=1}^{\xN}\lambda_n\rvx_n -\mean}^2
    && \text{by definition of $\meanest$}
  \\&= \pE\brs{\sum_{n=1}^{\xN}\lambda_n\rvx_n -\mean\mcom{\sum_{n=1}^{\xN}\lambda_n}{$1$}}^2
    && \text{by $\sum\lambda_n=1$ hypothesis}
    && \text{\xref{def:am}}
  \\&= \pE\brs{\sum_{n=1}^{\xN}\lambda_n(\rvx_n-\mean)}^2
  \\&= \pE\brs{\sum_{n=1}^{\xN}\lambda_n(\rvx_n-\mean)\sum_{m=1}^{\xN}\lambda_m(\rvx_m-\mean)}
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{(\rvx_n-\mean)(\rvx_m-\mean)}                                     }
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean\pE\brs{\rvx_n}-\mean\pE\brs{\rvx_m}+\mean^2   }
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean^2-\mean^2+\mean^2                             }
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean^2                                             }
  \\&= \mathrlap{
       \sum_{n=1}^{\xN} \lambda_n^2 \brp{\pE\brs{\rvx_n^2    }-\mean^2}
     + \sum_{n=1}^{\xN}\sum_{m\neq n }\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean^2 }
       }
  \\&= \mathrlap{
       \sum_{n=1}^{\xN} \lambda_n^2 \brp{\pE\brs{\rvx_n^2    }-\mean^2                                             }
     + \sum_{n=1}^{\xN}\sum_{m\neq n  }\lambda_n\lambda_m\brp{\pE\rvx_n\pE\rvx_m  -\mean^2}
       }
  \\&= \mathrlap{
       \sum_{n=1}^{\xN} \lambda_n^2 \sigma^2
     + \cancelto{0}{
       \sum_{n=1}^{\xN}\sum_{m\neq n  }\lambda_n\lambda_m\brp{\mean\mean-\mean^2}
       }}
    && \text{by \prope{WSS} hypothesis}
    && \text{(A)}
  \\&= \sigma^2\sum_{n=1}^{\xN} \lambda_n^2
  \\
  \mse(\meanest)
    &= \pE\brp{\meanest-\pE\meanest}^2
        +\brp{\pE\meanest-\mean}^2
    && \text{by \prefp{thm:mse}}
  \\&= \sigma^2\sum_{n=1}^{\xN}\lambda_n^2 + \brp{\mean-\mean}^2
    && \text{by previous results}
  \\&= \sigma^2\sum_{n=1}^{\xN}\lambda_n^2
\end{align*}
\end{proof}

%---------------------------------------
\begin{definition}
\label{def:average}
%---------------------------------------
\defboxt{
  The \fnctd{average} of a length $\xN$ sequence $\seqn{\rvx_n}$ is defined as
  \\\indentx$\ds\meanest\eqd\frac{1}{\xN}\sum_{n=1}^\xN \rvx_n$
  }
\end{definition}

%---------------------------------------
\begin{corollary}
\label{cor:mse_average}
\footnote{
  \citerpgc{kay1988}{45}{8131733564}{\textsection\scshape``3.3 Estimation Theory"}
  }
%---------------------------------------
\corbox{
  \brb{\begin{array}{FMD}
      (A).& $\seqn{\rvx_n}$ is \prope{wide sense stationary} & and
    \\(B).& $\mu\eqd\pE\rvx_n$                               & and
    \\(C).& $\seqn{\rvx_n}$ is \prope{uncorrelated}          & and
    \\(D).& \mc{2}{M}{$\ds\meanest\eqd\frac{1}{\xN}\sum_{n=1}^\xN \rvx_n$\qquad(\fncte{average})}
  \end{array}}
  \implies
  \brb{\begin{array}{FrclDD}
      (1).& \pE\meanest    &=& \mean                & (\prope{unbiased})   & and
    \\(2).& \var(\meanest) &=& \frac{\sigma^2}{\xN} &                      & and
    \\(3).& \mse(\meanest) &=& \frac{\sigma^2}{\xN} & (\prope{consistent})
  \end{array}}
  }
\end{corollary}
\begin{proof}
These results follow from \prefpp{thm:mse_mean} with $\lambda_n=\frac{1}{\xN}$.
\end{proof}


The \fncte{arithmetic mean} estimator $\meanest\eqd\sum\lambda_n\rvx_n$
is \prope{unbiased} and \prope{consistent} for any $\sum\lambda_n=1$ and
yields \fncte{mean square error} $\mse(\meanest)=\sigma^2\sum\lambda_n^2$
\xref{thm:mse_mean}.
But\ldots
\begin{enume}
  \item Said qualitatively: ``What is the 'best' sequence $\seqn{\lambda_n}$ to use?"
  \item Said quantitatively: ``What sequence $\seqn{\lambda_n}$ yields the smallest $\mse(\meanest)$?"
\end{enume}
For example, would fashioning $\seqn{\lambda_n}$ to be a scaled version of a standard 
window function, like the \fncte{Hanning window}\footnote{
  \citerp{abdaheer2009}{130}
  }
illustrated below, yield the best $\mse(\meanest)$?
\\\indentx\includegraphics{graphics/hanninglp50.pdf}
\\
\prefpp{prop:lambda_sq} answers question (2) stating
that the best sequence in terms of minimal $\mse$
is the sequence $\seqn{\lambda_n}\eqd\frac{1}{\xN}\seqn{\ldots,1,1,1,\ldots}$,
which is the \fncte{average} estimator,
which yields  $\mse(\meanest)= \frac{\sigma^2}{\xN}$ \xref{cor:mse_average}.
\\\indentx\includegraphics{graphics/rectangular_1N.pdf}
\\
That is, it turns out that $\frac{1}{\xN}\le\sum\lambda_n^2$ for all possible sequences $\seqn{\lambda_n}$.
This fact is demonstrated by \pref{lem:lambda_sq} (next),
which in turn follows more or less directly from the ubiquitous 
\ineqe{Cauchy-Schwarz Inequality} \xxref{thm:seq_cs}{thm:cs}.

%---------------------------------------
\begin{lemma}
\label{lem:lambda_sq}
%---------------------------------------
\lembox{
  \brb{\sum_{n=1}^{\xN} \lambda_n=1}
  \qquad\implies\qquad
  \brb{\frac{1}{\xN} \le \sum_{n=1}^{\xN} \lambda_n^2}
  }
\end{lemma}
\begin{proof}
\begin{enumerate}
  \item Let the sequence $\seqn{a_n}$ be defined as $\seqn{a_n}\eqd\seqn{\ldots,1,1,1,\ldots}$
        \label{idef:lambda_sq_an}
  \item Let \fncte{inner product} $\inprod{a_n}{b_n}$ be defined as $\inprod{a_n}{b_n}\eqd\sum_{n=1}^{\xN}a_n b_n$
        \label{idef:lambda_sq_inprod}
  \item Let \fncte{norm} $\norm{a_n}$ be defined as $\norm{a_n}\eqd\sum_{n=1}^{\xN}a_n^2$
        \label{idef:lambda_sq_norm}
  \item Proof of lemma:
        \begin{align*}
          \boxed{\frac{1}{\xN}}
            &=    \frac{1}{\xN}\brp{\sum_{n=1}^{\xN} \lambda_n}^2
            &&    \text{by $\sum_{n=1}^{\xN} \lambda_n=1$ hypothesis}
          \\&=    \frac{1}{\xN}
                  \brp{\sum_{n=1}^{\xN} a_n\lambda_n}^2
            &&    \text{by $\seqn{a_n}\eqd\seqn{\ldots,1,1,1,\ldots}$ definition}
            &&    \text{\xref{idef:lambda_sq_an}}
          \\&\boxed{\le} \frac{1}{\xN}
                  \brp{\sum_{n=1}^{\xN} a_n^2} 
                  \brp{\sum_{n=1}^{\xN} \lambda_n^2}
            &&    \text{by \ineqe{Cauchy-Schwartz inequality}}
            &&    \text{\xref{thm:seq_cs}}
          \\&\eqd \frac{1}{\xN}\brp{\sum_{n=1}^{\xN} 1^2} \brp{\sum_{n=1}^{\xN} \lambda_n^2}
            &&    \text{by definition of $\seqn{a_n}$}
            &&    \text{\xref{idef:lambda_sq_an}}
          \\&=    \boxed{\sum_{n=1}^{\xN} \lambda_n^2}
        \end{align*}
\end{enumerate}
\end{proof}

%---------------------------------------
\begin{proposition}
\label{prop:lambda_sq}
%---------------------------------------
Let $\mse(\text{average mean})$ be the mean square error of the \fncte{average} estimator \xref{cor:mse_average}
and $\mse(\text{arithmetic mean})$ be the mean square error of the \fncte{arithmetic} estimator \xref{thm:mse_mean}.
\propbox{
  \mse(\text{average mean}) \le \mse(\text{arithmetic mean})
  }
\end{proposition}
\begin{proof}
\begin{align*}
  \mse(\text{average mean}) 
    &= \sigma^2\frac{1}{\xN}
    && \text{by \prefp{cor:mse_average}}
  \\&\le \sigma^2 \sum_{n=1}^{\xN} \lambda_n^2
    && \text{by \prefp{lem:lambda_sq}}
  \\&= \mse(\text{arithmetic mean})
    && \text{by \prefp{thm:mse_mean}}
\end{align*}
\end{proof}

%=======================================
\section{Variance Estimation}
%=======================================
%---------------------------------------
\begin{definition}
\label{def:varest}
%---------------------------------------
\defbox{\begin{array}{rc>{\ds}l}
    \varest_B &\eqd& \frac{1}{\xN}  \sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2
  \\\varest   &\eqd& \frac{1}{\xN-1}\sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2
\end{array}}
\end{definition}

%---------------------------------------
\begin{theorem}
\label{thm:varest}
%---------------------------------------
Let $\meanest$ be the \fncte{average} \xref{def:average} of a sequence $\seqn{\rvx_n}$.
\thmbox{
  \brb{\begin{array}{FMD}
      (A).& $\seqn{\rvx_n}$ is \prope{wide sense stationary} & and
    \\(B).& $\mu\eqd\pE\rvx_n$                               & and
    \\(C).& $\seqn{\rvx_n}$ is \prope{uncorrelated}          & and
  \end{array}}
  \implies
  \brb{\begin{array}{FrclDD}
      (1).& \pE\varest_B  &=& \frac{\xN-1}{\xN}\sigma^2            & (\prope{biased})   & and
    \\(2).& \pE\varest    &=&                  \sigma^2            & (\prope{unbiased}) &
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item lemma: $\pE(\rvx_n\meanest) = \frac{1}{\xN}\sigma^2+\mean^2$. Proof: \label{ilem:varest}
    \begin{align*}
      \pE\brp{\rvx_n\meanest}
        &\eqd \pE\brp{\rvx_n\frac{1}{\xN}\sum_{m=1}^{\xN}\rvx_m}
        && \text{by definition of \fncte{average}}
        && \text{\xref{def:average}}
      \\&= \pE\brp{\frac{1}{\xN}\sum_{m=1}^{\xN}\rvx_n\rvx_m}
      \\&= \frac{1}{\xN}\sum_{m=1}^{\xN}\pE\brp{\rvx_n\rvx_m}
      \\&= \frac{1}{\xN}\brs{\pE\rvx_n^2 + \sum_{m\neq n}\pE\brp{\rvx_n\rvx_m}}
      \\&= \frac{1}{\xN}\brs{\pE\rvx_n^2 + \sum_{m\neq n}\brp{\pE\rvx_n}\brp{\pE\rvx_m}}
      \\&= \frac{1}{\xN}\brs{\brp{\sigma^2+\mean^2} + (\xN-1)\mean^2}
      \\&= \mean^2 - \brp{\frac{1}{\xN}\sigma^2+\mean^2}
    \end{align*}

  \item Proof for \prope{biased} result: 
        \begin{align*}
          \pE\varest_B
            &\eqd \pE\brs{\frac{1}{\xN}  \sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2}
            && \text{by definition of $\varest_B$}
            && \text{\xref{def:varest}}
          \\&= \frac{1}{\xN} \pE\brs{ \sum_{n=1}^{\xN} \brp{\rvx_n \mcom{-\mean + \mean}{$0$} - \meanest}^2}
          \\&= \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \mcom{\pE\brp{\rvx_n -\mean}^2}{$\sigma^2$} + 2\pE\brs{(\rvx_n -\mean)(\mean - \meanest)} + \pE\brp{\mean - \mcom{\meanest}{$\pE\mean$}}^2 }
          \\&= \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \sigma^2 + 2\pE\brs{\rvx_n\mean - \rvx_n\meanest -\mean^2 + \mean\meanest} + \frac{1}{\xN}\sigma^2 }
          \\&= \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \sigma^2 + 2\brs{\mean^2 - \pE(\rvx_n\meanest) -\mean^2 + \mean^2} + \frac{1}{\xN}\sigma^2 }
          \\&= \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \sigma^2 + 2\brs{\mean^2 - \brp{\frac{1}{\xN}\sigma^2+\mu^2} } + \frac{1}{\xN}\sigma^2 }
            && \text{by \xref{ilem:varest}}
          \\&= \frac{1}{\xN}  \sum_{n=1}^{\xN} \brs{ \sigma^2 - \frac{1}{\xN}\sigma^2} 
          \\&= \frac{\xN-1}{\xN}\sigma^2
        \end{align*}

  \item Proof for \prope{unbiased} result: 
        \begin{align*}
          \pE\varest
            &\eqd \pE\brs{\frac{1}{\xN-1}  \sum_{n=1}^{\xN} \brp{\rvx_n - \meanest}^2}
            && \text{by definition of $\varest$}
            && \text{\xref{def:varest}}
          \\&= \frac{\xN}{\xN-1}\pE\brs{  \frac{1}{\xN}  \sum_{n=1}^{\xN-1} \brp{\rvx_n - \meanest}^2}
          \\&= \frac{\xN}{\xN-1}\frac{\xN-1}{\xN}\sigma^2
            && \text{by \prope{biased} result}
          \\&= \sigma^2
        \end{align*}
\end{enumerate}
\end{proof}

