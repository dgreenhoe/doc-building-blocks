%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================

%======================================
\chapter{Estimation Theory }
\label{app:est}
\index{estimation theory}
%======================================
%======================================
\section{Estimation types}
%======================================
%--------------------------------------
\paragraph{Estimation types.}
%--------------------------------------
Let $\rvx(t;\theta)$ be a waveform with parameter $\theta$.
There are three basic types of estimation on $s$:

\begin{enume}
   \item \ope{detection}:
      \begin{liste}
         \item The waveform $\rvx(t;\theta_n)$ is known except for the value of parameter $\theta_n$.
         \item The parameter $\theta_n$ is one of a finite set of values.
         \item Estimate $\theta_n$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{parametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t;\theta)$ is known except for the value of parameter $\theta$.
         \item The parameter $\theta$ is one of an infinite set of values.
         \item Estimate $\theta$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{nonparametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t)$ is unknown.
         \item Estimate $\rvx(t)$.
      \end{liste}
\end{enume}

%--------------------------------------
\paragraph{Estimation criterion.}
%--------------------------------------
Optimization requires a criterion against which the quality of an
estimate is measured.\footnote{\citergc{srv}{chapters 3, 5}{013125295X}.}
The most demanding and general criterion is the \prope{Bayesian} criterion.
The Bayesian criterion requires knowledge of the probability
distribution functions and the definition of a \fncte{cost function}.
Other criterion are special cases of the Bayesian criterion
such that the cost function is defined in a special way,
no cost function is defined, and/or the distribution is not known
\xref{fig:est-tech}.

%--------------------------------------
\paragraph{Estimation techniques.}
\label{ref:sec:parameter-est}
%--------------------------------------
Estimation techniques can be classified into
five groups \xref{fig:est-tech}:\footnote{%
  \citerpgc{nelles2001}{26}{3540673695}{``Fig 2.2 Overview of linear and nonlinear optimization techniques"},
  \citerpgc{nelles2001}{33}{3540673695}{``Fig 2.5 The Bayes method is the most general approach but\ldots"},
  \citerpgc{nelles2001}{63}{3540673695}{``Table 3.3 Relationship between linear recursive and nonlinear optimization techniques"},
  \citerpg{nelles2001}{66}{3540673695}
  }
\begin{enume}
   \item sequential decoding
   \item norm minimization
   \item gradient search
   \item inner product analysis
   \item direct search
\end{enume}

Sequential decoding is a non-linear estimation family.
Perhaps the most famous of these is the Veterbi algorithm which
uses a trellis to calculate the estimate.
The Verterbi algorithm has been shown to yield an optimal estimate
in the maximal likelihood (ML) sense.
Norm minimization and gradient search algorithms are all linear algorithms.
While this restriction to linear operations often simplifies calculations,
it often yields an estimate that is not optimal in the ML sense.

%=======================================
\section{Estimation criterion}
\label{sec:est_criterion}
%=======================================
\begin{figure}[h]
\centering%
\includegraphics{graphics/latestimation.pdf}
\caption{
   Estimation criterion
   \label{fig:est-criterion}
   }
\end{figure}

%--------------------------------------
\begin{definition}
\index{MAP}
\index{ML}
\index{maximum a-posteriori}
\index{maximum likelihood}
\label{def:MAP}
\label{def:ML}
\label{def:estB}
\label{def:estMS}
\label{def:estMM}
\label{def:estMAP}
\label{def:estML}
%--------------------------------------
Let\\
$\begin{array}{FlM}
    (A).& \rvx(t;\theta)            & be a random process with unknown parameter $\theta$
  \\(B).& \rvy(t)                   & an observed random process which is statistically dependent on $\rvx(t;\theta)$
  \\(C).& \fCost(\theta,\pdfp(x,y)) & be a cost function.
\end{array}$
\tbox{
\setlength{\unitlength}{0.15mm}
\begin{picture}(200,100)(-50,-50)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(- 50,  10){\makebox ( 50, 50)[b]{$x(t)$}            }
  \put(- 50,   0){\vector  (  1,  0)   {50}                }
  \put(   0, -50){\framebox(100,100)[c]{}                  }
  \put(   0, -20){\makebox (100, 50)[t]{stochastic}        }
  \put(   0, -20){\makebox (100, 50)[b]{operator}          }
  \put( 100,  10){\makebox ( 50, 50)[b]{$y(t)$}            }
  \put( 100,   0){\vector  (  1,  0)   {50}                }
\end{picture}
}
\\
Then the following \fnctd{estimate}s are defined as described here:
\defbox{\begin{array}{FMMlc>{\ds}l}
     (1).&\fnctd{Bayesian estimate}                         &                          & \estB   &\eqd& \argmin_{\theta} C(\theta,\pdfp(x,y))
   \\(2).&\fnctd{Mean square estimate}                      &(``\fnctd{MS  estimate}") & \estMS  &\eqd& \argmin_{\theta} \E\norm{C(\theta,\pdfp(x,y))}^2
   \\(3).&\fnctd{mini-max estimate}                         &(``\fnctd{MM  estimate}") & \estMM  &\eqd& \argmin_{\theta}\max_{\pdfp} C(\theta,\pdfp(x,y))
   \\(4).&\mc{2}{M}{\begin{tabular}[t]{@{}l}\fnctd{maximum a-posteriori probability estimate}\\
                                         (``\fnctd{MAP estimate}")
                    \end{tabular}}
         & \estMAP &\eqd& \argmax_{\theta} \psP\setn{x(t;\theta)|y(t)}
   \\(5).&\fnctd{maximum likelihood estimate}               &(``\fnctd{ML  estimate}") & \estML  &\eqd& \argmax_{\theta} \psP\setn{y(t)|x(t;\theta)}
\end{array}}
\end{definition}

%--------------------------------------
\begin{theorem}
\label{thm:map=ml}
%--------------------------------------
Let $\rvx(t;\theta)$ be a random process with unknown parameter $\theta$.
\thmbox{
  \brb{\text{$\psP\setn{\theta}=$\prope{constant}}}
  \quad\implies\quad
  \brb{\estMAP = \estML}
  }
\end{theorem}
\begin{proof}
\begin{align*}
   \estMAP
     &\eqd \argmax_{\theta} \psP\setn{s(t;\theta)|r(t)}
     &&    \text{by definition of $\estMAP$}
     &&    \text{\xref{def:estMAP}}
   \\&=    \argmax_{\theta} \frac{\psP\setn{s(t;\theta) \land \rvy(t)}}
                               {\psP\setn{r(t)}}
   \\&=    \argmax_{\theta} \frac{\psP\setn{r(t) | \rvx(t;\theta) }\psP\setn{s(t;\theta) }}
                               {\psP\setn{r(t)}}
   \\&=    \argmax_{\theta} \psP\setn{r(t) | \rvx(t;\theta) }\psP\setn{s(t;\theta) }
   \\&=    \argmax_{\theta} \psP\setn{r(t) | \rvx(t;\theta) }
   \\&\eqd \estML
     &&  \text{by definition of $\estML$}
     &&  \text{\xref{def:estML}}
\end{align*}
\end{proof}

%=======================================
\section{Measures of estimator quality}
\label{sec:quality}
%=======================================
%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{silverman1986}{35}{9780412246203}{\textsection ``1.3.2 Measures of discrepancy\ldots"},
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"},
  \citerp{bendat1966}{183}{\textsection ``5.3 Statistical Errors for Parameter Estimates"}
  }
\label{def:mse}
%---------------------------------------
\defboxt{
  The \fnctd{mean square error} $\mse(\estT)$ of an estimate $\estT$ of a parameter $\theta$ is defined as
  \\\indentx$\ds\mse(\estT) \eqd \pE\brs{\brp{\estT-\theta}^2}$
  }
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"}
  }
\label{def:nre}
%---------------------------------------
\defboxt{
  The \fnctd{normalized rms error} $\nre(\estT)$ of an estimate $\estT$ of a parameter $\theta$ is defined as
  \\\indentx$\ds\nre(\estT) \eqd \frac{\sqrt{\mse(\estT)}}{\theta} 
                            \eqd \frac{\sqrt{\pE\brs{\brp{\estT-\theta}^2}}}{\theta}$
  }
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{silverman1986}{35}{9780412246203}{\textsection ``1.3.2 Measures of discrepancy\ldots"},
  \citePpc{rosenblatt1956}{835}{``integrated mean square error"}
  }
\label{def:mise}
%---------------------------------------
\defboxt{
  The \fnctd{mean integrated square error} $\mise(\estT)$ of an estimate $\estT$ of a parameter $\theta$ is defined as
  \\\indentx$\ds\mise(\estT) \eqd \pE\int_{\theta\in\R}\brs{\brp{\estT-\theta}^2}$
  }
\end{definition}

The \fncte{mean square error} of $\estT$ can be expressed as the sum of two components:
the variance of $\estT$ and the bias of $\estT$ squared (next Theorem).
For an example of \pref{thm:mse} in action, see the proof for the $\mse(\meanest)$ of the 
\fncte{arithmetic mean estimate} as provided in \prefpp{thm:mse_mean}.
%---------------------------------------
\begin{theorem}
\footnote{
  \citerpgc{kay1988}{45}{8131733564}{\textsection\scshape``3.3 Estimation Theory"},
  \citerpgc{stuart1991}{629}{9780340560235}{``Minium mean-square-error estimation"},
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"},
  \citerp{bendat1966}{183}{\textsection ``5.3 Statistical Errors for Parameter Estimates"},
  \citerpgc{bendat1980}{39}{0471058874}{\textsection ``2.4.1 Bias versus Random Errors"}
  }
\label{thm:mse}
%---------------------------------------
\thmbox{\begin{array}{rc>{\ds}l}
  \mse(\estT) &=&
      \mcom{\pE\brs{\brp{\estT-\pE\estT}^2}}{variance of $\estT$}
    + \mcom{\brs{\pE\estT - \theta}^2}{bias of $\estT$ squared}
  \\
  \nre(\estT) &=&
    \frac{\sqrt{
    \pE\brs{\brp{\estT-\pE\estT}^2} + \brs{\pE\estT - \theta}^2
    }}{\theta}
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \mse(\estT)
    &\eqd \pE\brs{\brp{\estT-\theta}^2}
    && \text{by definition of $\mse$}
    && \text{\xref{def:mse}}
  \\&= \pE\brs{\brp{\estT\mcom{-\pE\estT+\pE\estT}{$0$}-\theta}^2}
    && \text{by \prope{additive identity} property of $\fieldC$}
  \\&= \pE\brs{
         \brp{\estT-\pE\estT}^2
        +\mcom{\brp{\pE\estT-\theta}^2}{constant}
        -2\brp{\estT-\pE\estT}\brp{\pE\estT-\theta}
       }
    && \text{by \thme{Binomial Theorem}}
    && \text{\ifxref{polynom}{thm:binomial}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
        -2\pE\brs{
         \estT\pE\estT
        -\estT\theta
        -\pE\estT\estT
        +\pE\estT\theta
        }
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
        -2\mcom{\brs{
         \pE\estT\pE\estT
        -\pE\estT\pE\theta
        -\pE\estT\pE\estT
        +\pE\estT\pE\theta
        }}{$0$}
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
\end{align*}
\end{proof}



%--------------------------------------
\section{Estimation techniques}
%--------------------------------------
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(1000,400)(-150,-300)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put( 325,  50){\framebox(100, 50)[c]{techniques}          }

  \put(  50,   0){\line    (  1,  0)   {750}                 }
 %\put( 375,   0){\circle*             { 10}                 }

  \put(-100,-225){\line    (  1,  0)   {300}                 }
  \put(  50,-225){\circle*             {  5}                 }
  \put(-100,-250){\line    (  0,  1)   { 25}                 }
  \put(  50,-250){\line    (  0,  1)   {150}                 }
  \put( 200,-250){\line    (  0,  1)   { 25}                 }

  \put(  50,- 50){\line    (  0,  1)   { 50}                 }
  \put( 200,- 50){\line    (  0,  1)   { 50}                 }
  \put( 350,- 50){\line    (  0,  1)   { 25}                 }
  \put( 500,- 50){\line    (  0,  1)   { 25}                 }
  \put( 425,- 25){\line    (  0,  1)   { 25}                 }
 %\put( 425,- 25){\circle*             { 10}                 }
  \put( 350,- 25){\line    (  1,  0)   {150}                 }
  \put( 650,- 50){\line    (  0,  1)   { 50}                 }
  \put( 800,- 50){\line    (  0,  1)   { 50}                 }
  \put( 375,   0){\line    (  0,  1)   { 50}                 }

  \put(  50,-150){\line    (  0,  1)   { 50}                 }
  \put( 200,-150){\line    (  0,  1)   { 50}                 }
  \put( 350,-150){\line    (  0,  1)   { 50}                 }
  \put( 500,-150){\line    (  0,  1)   { 50}                 }

  \put( 500,-250){\line    (  0,  1)   { 50}                 }

  \put(  60,- 45){\makebox (100, 40)[lt]{sequential}         }
  \put(  60,- 45){\makebox (100, 40)[lb]{decoding}           }
  \put( 210,- 45){\makebox (100, 35)[lt]{norm}       }
  \put( 210,- 45){\makebox (100, 35)[lb]{minimization}       }
  \put( 435,- 25){\makebox (100, 25)[l]{gradient search}     }
  \put( 360,- 50){\makebox (100, 25)[l]{use $\grad_\vp$ only}}
  \put( 510,- 50){\makebox (100, 25)[l]{$\grad_\vp,\grad^2_\vp$}}
  \put( 660,- 50){\makebox (100, 50)[l]{inner product}       }
  \put( 810,- 50){\makebox (100, 50)[l]{direct search}       }

  \put(   0,-100){\framebox(100, 50)[c]{}                    }
  \put(   0,- 90){\makebox (100, 30)[t]{code}                }
  \put(   0,- 90){\makebox (100, 30)[b]{tree}                }
  \put(   0,-300){\framebox(100, 50)[c]{}                    }

  \put(-150,-295){\makebox (100, 40)[t]{trellis}             }
  \put(-150,-295){\makebox (100, 40)[b]{(Viterbi)}           }
  \put(-150,-300){\framebox(100, 50)[c]{}                    }
  \put(   0,-295){\makebox (100, 40)[t]{Fano}                }
  \put(   0,-295){\makebox (100, 40)[b]{algorithm}           }
  \put(   0,-300){\framebox(100, 50)[c]{}                    }
  \put( 150,-295){\makebox (100, 40)[t]{Stack}               }
  \put( 150,-295){\makebox (100, 40)[b]{algorithm}           }
  \put( 150,-300){\framebox(100, 50)[c]{}                    }

  \put( 150,-100){\framebox(100, 50)[c]{}                    }
  \put( 150,- 90){\makebox (100, 30)[t]{min. mean}           }
  \put( 150,- 90){\makebox (100, 30)[b]{squares}             }
  \put( 150,-200){\framebox(100, 50)[c]{}                    }
  \put( 150,-190){\makebox (100, 30)[t]{least}               }
  \put( 150,-190){\makebox (100, 30)[b]{square}             }

  \put( 300,-100){\framebox(100, 50)[c]{}                    }
  \put( 300,- 90){\makebox (100, 30)[t]{steepest}            }
  \put( 300,- 90){\makebox (100, 30)[b]{descent}             }
  \put( 300,-200){\framebox(100, 50)[c]{}                    }
  \put( 300,-190){\makebox (100, 30)[t]{least mean}          }
  \put( 300,-190){\makebox (100, 30)[b]{square}             }

  \put( 450,-100){\framebox(100, 50)[c]{}                    }
  \put( 450,- 90){\makebox (100, 30)[t]{Newton}              }
  \put( 450,- 90){\makebox (100, 30)[b]{method}              }
  \put( 450,-200){\framebox(100, 50)[c]{Kalman}              }
  \put( 450,-190){\makebox (100, 30)[t]{}                    }
  \put( 450,-190){\makebox (100, 30)[b]{}                    }
  \put( 450,-295){\makebox (100, 40)[t]{recursive}           }
  \put( 450,-300){\framebox(100, 50)[c]{least}               }
  \put( 450,-295){\makebox (100, 40)[b]{square}             }

  \put( 600,-100){\framebox(100, 50)[c]{wavelets}            }
  \put( 600,- 90){\makebox (100, 30)[t]{}                    }
  \put( 600,- 90){\makebox (100, 30)[b]{}                    }

\end{picture}
\caption{
   Estimation techniques
   \label{fig:est-tech}
   }
\end{figure}

%\begin{figure}[ht]
%\center{\epsfig{file=estimate.eps, width=12cm, clip=}}
%%\center{\includegraphics[0,0][4in,4in]{df1.eps}}
%\caption{
%   Estimation techniques
%   \label{fig:estimate}
%   }
%\end{figure}

%--------------------------------------
\subsection{Sequential decoding}
%--------------------------------------
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(550,320)(-100,0)
  %\graphpaper[10](0,0)(300,300)
  \put( -10 , 300 ){\makebox(0,0)[r]{state $00$}}
  \put( -10 , 200 ){\makebox(0,0)[r]{state $01$}}
  \put( -10 , 100 ){\makebox(0,0)[r]{state $10$}}
  \put( -10 ,   0 ){\makebox(0,0)[r]{state $11$}}

  \thicklines
  \put(   0 ,   0 ){\circle*{10}}
  \put(   0 , 100 ){\circle*{10}}
  \put(   0 , 200 ){\circle*{10}}
  \put(   0 , 300 ){\circle*{10}}

\multiput(0,0)(100,0){5}{
  %\thicklines
  \linethickness{1mm}
  \put        (  0,300){\line( 1,-1){100}} % state0 path1
  \put        (  0,200){\line( 1,-1){100}} % state1 path1
  \put        (  0,100){\line( 1, 1){100}} % state2 path1
  \put        (  0,  0){\line( 1, 1){100}} % state3 path1

  %\thicklines
  \linethickness{0.1mm}
  %\put        (  0,300){\line( 1,-3){100}} % state0 path0
  %\put        (  0,200){\line( 1,-2){100}} % state1 path0
  %\put        (  0,100){\line( 1, 2){100}} % state2 path0
  %\put        (  0,  0){\line( 1, 3){100}} % state3 path0

  \qbezier[50](  0,300)(  0,300)(100,  0)  % state0 path0
  \qbezier[50](  0,200)(  0,200)(100,  0)  % state1 path0
  \qbezier[50](  0,100)(  0,100)(100,300)  % state2 path0
  \qbezier[50](  0,  0)(  0,  0)(100,300)  % state3 path0

  \put( 100 ,   0 ){\circle*{10}}
  \put( 100 , 100 ){\circle*{10}}
  \put( 100 , 200 ){\circle*{10}}
  \put( 100 , 300 ){\circle*{10}}
}
\end{picture}
\hspace{1cm}
\begin{tabular}{cl}
   $\cdots$ & $y_n=0$ \\
  ---     & $y_n=1$
\end{tabular}
\caption{
  Viterbi algorithm trellis
   \label{fig:est_trellis}
   }
\end{figure}

It has been shown that the Viterbi algorithm (trellis) produces
an optimal estimate in the maximal likelihood (ML) sense.
A Verterbi trellis is shown in \prefpp{fig:est_trellis}.

%======================================
\subsection{Norm minimization}
%======================================
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(-100,  10){\makebox ( 50,  0)[b]{$x$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{operator} }
  \put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vp$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn$}           }

  \put( 100,  85){\makebox ( 50,  0)[b]{$y$}               }
  \put( 100,  75){\line    (  1,  0)   { 50}               }
  %\put( 100,- 65){\makebox ( 50,  0)[b]{$U\estn$}            }
  \put( 100,- 75){\line    (  1,  0)   { 50}               }
  \put( 150,  75){\line    (  0, -1)   { 50}               }
  \put( 150,- 75){\line    (  0,  1)   { 50}               }
  \put( 150,  25){\vector  (  1,  0)   { 50}               }
  \put( 150, -25){\vector  (  1,  0)   { 50}               }

  \put( 200,- 50){\framebox(100,100)[c]{cost}    }
  \put( 300,  10){\makebox (100,  0)[b]{$C(\estn,x,y)$}      }
  \put( 300,   0){\vector  (  1,  0)   {100}               }
  \put( 400,- 50){\framebox(100,100)[c]{$\grad_\vp$}        }
  \put( 510,  10){\makebox (100,  0)[lb]{$\grad_\vp C(\vp,x,y)$}      }
  \put( 500,   0){\vector  (  1,  0)   {100}               }

\end{picture}
\caption{
   Estimation using gradient of cost function
   \label{fig:est-grad}
   }
\end{figure}

Norm minimization techniques are very powerful
in that an optimum solution can be computed
in one step without iteration or recursion.
In this section we present two types of norm minimization:
\footnote{
   The Least Squares algorithm is nothing new to mathematics.
   It was first developed in 1795 by Gauss who was also the first
   to discover the FFT.
   }

\begin{enume}
  \item minimum mean square estimation (MMSE): \\
        The MMS estimate is a \hie{stochastic} estimate.
        To compute the MMS estimate, the we do not need to know
        the actual data values, but we must know certain system statistics
        which are the
        input data autocorrelation and input/output crosscorrelation.
        The cost function is the expected value of the norm squared error.
   \item least square estimation (LSE): \\
        The LS estimate is a \hie{deterministic} estimate.
        To compute the LS estimate, we must know the actual data values
        (although these may be ``noisy" measurements).
        The cost function is the norm squared error.
\end{enume}

Solutions to both are given in terms of two matrices:

\begin{tabular}{lll}
   $\setY$: Autocorrelation matrix \\
   $W$: Crosscorrelation matrix.
\end{tabular}

%--------------------------------------
\subsubsection{Minimum mean square estimation}
\label{sec:est_mms}
%--------------------------------------
%--------------------------------------
\begin{definition}
\label{def:est_matrices}
%--------------------------------------
Let the following vectors, matrices, and functions
be defined as follows:

\defbox{\begin{tabular}{lcll}
   $\vx  $&$\in$&$ \vCm        $  & data vector                       \\
   $\vy  $&$\in$&$ \vCn        $  & processed data vector             \\
   $\vye $&$\in$&$ \vCn        $  & processed data estimate vector    \\
   $\ve  $&$\in$&$ \vCn        $  & error vector                     \\
   $\vp  $&$\in$&$ \vRm        $  & parameter vector                 \\
   $U    $&$\in$&$ \mCmn       $  & regression matrix                \\
   $R    $&$\in$&$ \mCmm       $  & autocorrelation matrix           \\
   $W    $&$\in$&$ \vCm        $  & cross-correlation vector         \\
   $C    $&$:  $&$ \vRm\to\Rnn $  & cost function
\end{tabular}}
\end{definition}

%--------------------------------------
\begin{theorem}[\thmd{Minimum mean square estimation}]
\label{thm:est_mms}
%--------------------------------------
Let
\begin{align*}
   \vye(\vp)   &\eqd U^H\vp    \\
   \ve(\vp)    &\eqd \vye-\vy \\
   \fCost(\vp) &\eqd \E\norm{\ve}^2 \eqd \Eb{\ve^H\ve} \\
   \estMS      &\eqd \argmin_\vp \fCost(\vp)  \\
   R           &\eqd \Eb{UU^H}   \\
   W           &\eqd \Eb{ U\vy}.
\end{align*}

Then
\thmbox{\begin{array}{rcl}
   \estMS                          &=& (\Real{\setY})^{-1}(\Real{W})  \\
   \fCost(\vp)                     &=& \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \E{\vy^H\vy} \\
   \grad_\vp \fCost(\vp)           &=& 2\Reb{\setY}\vp - 2\Real{W}  \\
   \fCost(\estMS)                  &=& (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy} \\
   \fCost(\estMS)|_{R\mbox{ real}} &=& \E{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W}).
\end{array}}
\end{theorem}
\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{align*}
   \fCost(\vp)
     &\eqd \E \norm{\ve}^2
   \\&=    \Eb{\ve^H\ve}
   \\&=    \Eb{ \left(\vye-\vy\right)^H\left(\vye-\vy\right) }
   \\&=    \Eb{ \left(U^H\vp-\vy\right)^H\left(U^H\vp-\vy\right) }
   \\&=    \Eb{ \left(\vp^HU-\vy^H\right)\left(U^H\vp-\vy\right) }
   \\&=    \Eb{ \vp^HUU^H\vp - \vp^HU\vy -\vy^HU^H\vp + \vy^H\vy }
   \\&=    \vp^H\Eb{UU^H}\vp - \vp^H\Eb{U\vy} -\Eb{\vy^HU^H}\vp + \E{\vy^H\vy}
   \\&=    \vp^H\Eb{UU^H}\vp - (\Eb{U\vy}^H\vp)^H -\Eb{U\vy}^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - (W^H\vp)^H -W^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - 2\Reb{W^H}\vp + \E{\vy^H\vy}
\\
\\
   \grad_\vp \fCost(\vp)
     &= \grad_\vp \left[ \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \E{\vy^H\vy} \right]
   \\&= R\vp + R^T\vp - [(W^H)^\ast]^T - [W^H]^T + 0
   \\&= R\vp + (R^H)^\ast\vp - W - W^\ast
   \\&= R\vp + R^\ast\vp - W - W^\ast
   \\&= (R + R^\ast)\vp - (W + W^\ast)
   \\&= 2(\Real{\setY})\vp - 2\Real{W}
\\
\\
   \vpo
     &= (\Real{\setY})^{-1}(\Real{W})
\\
\\
   \fCost(\vpo)
     &=    \vpo^H R \vpo - 2\Reb{W^H}\vpo + \E{\vy^H\vy}
   \\&=    [(\Real{\setY})^{-1}(\Real{W})]^H R [(\Real{\setY})^{-1}(\Real{W})] - 2\Reb{W^H}[(\Real{\setY})^{-1}(\Real{W})] + \E{\vy^H\vy}
   \\&=    (\Real{W^H})(\Real{\setY})^{-H} R (\Real{\setY})^{-1}(\Real{W}) - 2\Reb{W^H}(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    (\Real{W^H})(\Real{R^H})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2\Reb{W^H}(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy}
\\
\\
   \fCost(\vpo)|_{R\mbox{ real}}
     &=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    (\Real{W^H})R^{-1} R R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    (\Real{W^H}) R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    \E{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W})
\end{align*}
\end{proof}




\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  \put(-100,  10){\makebox ( 50,  0)[b]{$\vx$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{process} }
  %\put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vp$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn=R^{-1}W$}           }

  \put( 110,  85){\makebox ( 50,  0)[lb]{$y$}               }
  \put( 110, -65){\makebox ( 50,  0)[lb]{$\hat{y}=\vx^T\vp$}               }
  \put( 100,  75){\vector  (  1,  0)   {150}               }
  \put( 100,- 75){\vector  (  1,  0)   {150}               }
  \put( 250,   0){\circle{40} }
  \put( 250,   0){\makebox(0,0)[c]{$+$} }
  \put( 250,  75){\vector  (  0, -1)   { 60}               }
  \put( 250, -75){\vector  (  0,  1)   { 60}               }

  \put( 300,  10){\makebox (100,  0)[b]{$e=\hat{y}-y$}      }
  \put( 270,   0){\vector  (  1,  0)   {80}               }
  \put( 350,   0){\vector  (  0, -1)   {225}               }
  \put( 350,-225){\vector  ( -1,  0)   {250}               }
\end{picture}
\caption{
   Adative filter example
   \label{fig:est_adapt}
   }
\end{figure}

In many adaptive filter and equalization applications,
the autocorrelation matrix $U$ is simply the $m$-element
random data vector $\vx(k)$ at time $k$, as in the \thme{Wiener-Hopf equations} (next).
%--------------------------------------
\begin{corollary}[\thmd{Wiener-Hopf equations}]
\footnote{
  \citerppgc{ifeachor1993}{547}{549}{020154413X}{\textsection ``9.3 Basic Wiener filter theory"},
  \citerppgc{ifeachor2002}{651}{654}{0201596199}{\textsection ``10.3 Basic Wiener filter theory"}   % TODO: check 654
  }
%--------------------------------------
\corbox{
  \brb{
   U \eqd \vx(k) \eqd
   \brs{\begin{array}{l}
      x(k) \\
      x(k-1) \\
      x(k-2) \\
      \vdots \\
      x(k-m+1)
   \end{array}}}
  \implies
  \brb{\begin{array}{rcl}
    \estMS                          &=& R^{-1}W  \\
    \fCost(\estMS)                  &=& W^T R^{-1} R R^{-1}W - 2W^T R^{-1}W + \E{\vy^T\vy} \\
  \end{array}}
  }
\end{corollary}
\begin{proof}
This is a special case of the more general case discussed
in \prefpp{thm:est_mms}.
Here, the dimension of $U$ is $m\times1$ (n=1).
As a result,
$\vy$, $\vye$, and $\ve$ are simply scalar quantities (not vectors).
In this special case, we have the following results
\xref{fig:est_adapt}:

\begin{align*}
   \hat{y}(\vp)   &\eqd \vx^T \vp    \\
   e(\vp)    &\eqd \hat{y}-y \\
   \fCost(\vp) &\eqd \E\norm{\ve}^2 \eqd \Eb{e^2} \\
   \estMS      &\eqd \argmin_\vp \fCost(\vp)  \\
   R           &\eqd \Eb{\vx\vx^T}   \\
   W           &\eqd \Eb{ \vx y }    \\
    \fCost(\vp)                     &= \vp^T R \vp -2W^T\vp  + \Eb{\vy^T\vy} \\
    \grad_\vp \fCost(\vp)           &= 2R\vp - 2W  \\
    \fCost(\estMS)|_{R\mbox{ real}} &=    \E{\vy^T\vy} - W^T R^{-1}W.
\end{align*}
\end{proof}

%======================================
\subsubsection{Least squares}
\label{sec:ls}
%======================================
%--------------------------------------
\begin{theorem}[\thmd{Least squares}]
\label{thm:ls}
\index{least squares}
%--------------------------------------
Let
\begin{align*}
   \vye(\vp)   &\eqd U^H\vp                      \\
   \ve(\vp)    &\eqd \vye-\vy                    \\
   \fCost(\vp) &\eqd \norm{\ve}^2 \eqd \ve^H\ve  \\
   \estLS      &\eqd \argmin_\vp \fCost(\vp)    \\
   R           &\eqd UU^H                        \\
   W           &\eqd U\vy.
\end{align*}

Then
\thmbox{\begin{array}{rcl}
   \estLS                          &=& (\Real{\setY})^{-1}(\Real{W})  \\
   \fCost(\vp)                     &=& \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \E{\vy^H\vy} \\
   \grad_\vp \fCost(\vp)           &=& 2\Reb{\setY}\vp - 2\Real{W}  \\
   \fCost(\estLS)                  &=& (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy} \\
   \fCost(\estLS)|_{R\mbox{ real}} &=& \E{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W}).
\end{array}}
\end{theorem}

\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{align*}
   \fCost(\vp)
     &\eqd \norm{\ve}^2
   \\&=    e^H\ve
   \\&=    \left(\vye-\vy\right)^H\left(\vye-\vy\right)
   \\&=    \left(U^H\vp-\vy\right)^H\left(U^H\vp-\vy\right)
   \\&=    \left(\vp^HU-\vy^H\right)\left(U^H\vp-\vy\right)
   \\&=    \vp^HUU^H\vp - \vp^HU\vy -\vy^HU^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - (W^H\vp)^H -W^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - 2\Reb{W^H}\vp + \vy^H\vy
\\
\\
   \grad_\vp \fCost(\vp)
     &= \grad_\vp \left[ \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \vy^H\vy \right]
   \\&= R\vp + R^T\vp - [(W^H)^\ast]^T - [W^H]^T + 0
   \\&= R\vp + (R^H)^\ast\vp - W - W^\ast
   \\&= R\vp + R^\ast\vp - W - W^\ast
   \\&= (R + R^\ast)\vp - (W + W^\ast)
   \\&= 2(\Real{\setY})\vp - 2\Real{W}
\\
\\
   \vpo
     &= (\Real{\setY})^{-1}(\Real{W})
\\
\\
   \fCost(\vpo)
     &=    \vpo^H R \vpo - 2\Reb{W^H}\vpo + \vy^H\vy
   \\&=    [(\Real{\setY})^{-1}(\Real{W})]^H R [(\Real{\setY})^{-1}(\Real{W})] - 2\Reb{W^H}[(\Real{\setY})^{-1}(\Real{W})] + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{\setY})^{-H} R (\Real{\setY})^{-1}(\Real{W})     - 2\Reb{W^H}(\Real{\setY})^{-1}(\Real{W})       + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{R^H})^{-1} R (\Real{\setY})^{-1}(\Real{W})   - 2\Reb{W^H}(\Real{\setY})^{-1}(\Real{W})     + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W})     - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W})      + \vy^H\vy
\\
\\
   \fCost(\vpo)|_{R\mbox{ real}}
     &=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \vy^H\vy
   \\&=    (\Real{W^H})R^{-1} R R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \vy^H\vy
   \\&=    (\Real{W^H}) R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \vy^H\vy
   \\&=    \vy^H\vy - (\Real{W^H})R^{-1}(\Real{W})
\end{align*}
\end{proof}


%---------------------------------------
\begin{example}[Polynomial approximation]
\index{polynomial approximation}
\index{least squares}
\index{Vandermonde matrix}
%---------------------------------------
\mbox{}\\
Suppose we {\bf know} the locations
$\set{(x_n,y_n)}{n=1,2,3,4,5}$ of 5 data points.
Let $\vx$ and $\vy$ represent the locations of these points such that
\[
   \vx \eqd
   \left[\begin{array}{l}
      x_1  \\
      x_2  \\
      x_3  \\
      x_4  \\
      x_5
   \end{array}\right]
   \qquad\qquad
   \vy \eqd
   \left[\begin{array}{l}
      y_1  \\
      y_2  \\
      y_3  \\
      y_4  \\
      y_5
   \end{array}\right]
\]
Suppose we want to find a second order polynomial
  \[ c x^2 + bx + a \]
that best approximates
these 5 points in the least squares sense.
We define the matrix $U$ (known) and vector $\estn$ (to be computed)
as follows:
\[
   U^H \eqd
   \mcom{
   \left[\begin{array}{lll}
      1  & x_1 & x_1^2  \\
      1  & x_2 & x_2^2  \\
      1  & x_3 & x_3^2  \\
      1  & x_4 & x_4^2  \\
      1  & x_5 & x_5^2
   \end{array}\right]
   }{Vandermonde matrix \footnotemark}
   \addtocounter{footnote}{-1}\footnote{\citer{horn}{29}}
   \qquad\qquad
   \estn \eqd
   \left[\begin{array}{l}
      a  \\
      b  \\
      c  \\
   \end{array}\right]
\]
Then, using \prefpp{thm:ls}, the best coefficients $\estn$
for the polynomial are
\begin{align*}
  \estn
    &= \left[\begin{array}{l}
          a  \\
          b  \\
          c  \\
       \end{array}\right]
  \\&= R^{-1}W
  \\&= (UU^H)^{-1}\; (U\vy)
  \\&= \left(
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]^H
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]
       \right)^{-1}
       \left(
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]^H
       \left[\begin{array}{l}
          y_1  \\
          y_2  \\
          y_3  \\
          y_4  \\
          y_5
       \end{array}\right]
       \right)
\end{align*}
\end{example}

%======================================
\subsection{Gradient search techniques}
%======================================
One of the biggest advantages of using a gradient search technique is
that they can be implemented \hie{recursively} as shown in the next equation.
The general form of the gradient search parameter estimation techniques is\footnote{\citerp{nelles2001}{90}}
\thmbox{
   \vp_n = \vp_{n-1} - \eta_{n-1} R \;\left[\gradxy{\vp}{\fCost(\vp_n)}\right]
}
where at time $n$

\begin{tabular}{lll}
   $\vp_n$      & is the \hie{state    }   & (vector)  \\
   $\eta_n$     & is the \hie{step size}   & (scalar)  \\
   $\setY$          & is the \hie{direction}   & (matrix)  \\
   $\gradxy{\vp}{\fCost(\vp_n)}$      & is the \hie{gradient } of the cost function $\fCost(\vp_n)$   & (vector)
\end{tabular}

Two major categories of gradient search techniques are
\begin{liste}
   \item steepest descent (includes LMS)
   \item Newton's method (includes RLS and Kalman filters).
\end{liste}

The key difference between the two is that
{\bf \hie{steepest descent} uses only first derivative information},
while
{\bf \hie{Newton's method} uses both first and second derivative information}
making it converge much faster but with significantly higher
complexity.

%======================================
\subsubsection*{First derivative techniques}
\label{sec:1st-deriv}
%======================================
\paragraph{Steepest descent.}
In this algorithm, $R=I$ (identity matrix).
First derivative information is contained in $\grad\fCost$.
Second derivative information, if present, is contained in $\setY$.
Thus, steepest descent algorithms do not use second derivative information.
\thmbox{
  \vp_n = \vp_{n-1} - \eta_{n-1} \;\left[ \gradxy{\vp}{\fCost(\vp_n)} \right]
}
\paragraph{Least Mean Squares (LMS).}\footnote{\citerp{mik}{526}}
This is a special case of \hie{steepest descent}.
In minimum mean square estimation \xref{sec:est_mms},
the cost function $\fCost(\vp)$ is defined as a
\hie{statistical average} of the error vector such that
$\fCost(\vp) = \Eb{\ve^H\ve}$.
In this case the gradient $\grad\fCost$ is difficult to compute.
However, the LMS algorithm greatly simplifies the problem by
instead defining the cost function as a function of the
\hie{instantaneous error} such that
\begin{align*}
   \vy &= y(n)
\\
   \vye &= \hat{y}(n)
\\
   \fCost(\vp)
   &= \norm{e(n)}^2
 \\&= e^2(n)
 \\&= (\hat{y}(n)-y(n))^2
\end{align*}

Computing the gradient of this cost function is then
just a special case of \hie{least squares estimation} \xref{sec:ls}.
Using LS, we let $U=\vx^T$ and hence
\begin{align*}
   \gradxy{\vp}{\fCost(\vp)}
   &= 2U^TU \vp -2U^T\vy                   && \text{ by \prefp{thm:ls}}
\\ &= 2\vx\vx^T \vp -2\vx y               && \text{ by above definitions}
\\ &= 2\vx\hat{y} -2\vx y                    && \text{ }
\\ &= 2\vx(\hat{y} -y)                      && \text{ }
\\ &= 2\vx e(n)                && \text{ }
\end{align*}

The LMS algorithm uses this instantaneous gradient for $\grad\fCost$,
lets $R=I$, and uses a constant step size $\eta$ to give
\thmbox{
  \vp_n = \vp_{n-1} - 2\eta \vx_n e(n)
}
%--------------------------------------
\subsubsection*{Second derivative techniques}
%--------------------------------------
\paragraph{Newton's Method.}
This algorithm uses the \hie{Hessian} matrix $H$,
which is the second derivative of the cost function $\fCost(\vp)$,
and lets $R=H^{-1}$.
\begin{align*}
   H_n &\eqd& \grad_\vp\grad_\vp \fCost(\vp_n)
\\
   \vp_n &= \vp_{n-1} - \eta_{n-1} H_n^{-1} \left[\grad_\vp\fCost(\vp_n)\right]
\end{align*}


\paragraph{Kalman filtering}\footnote{\citerp{nelles2001}{66}}
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}

\if 0
\paragraph{RLS with forgetting}\footnote{\citerp{nelles2001}{64}}
This algorithm introduces a forgetting factor $\lambda$
to help the algorithm track non-stationary channels.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+\lambda}  P(k-1)x(k) \\
   P(k) &= \frac{1}{\lambda}(I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}
\fi

\paragraph{Recursive Least Squares (RLS)}\footnote{\citerp{nelles2001}{66}}
This algorithm is a special case of either the RLS with forgetting
or the Kalman filter.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}  P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1) \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}




%--------------------------------------
\subsection{Direct search}
%--------------------------------------
A direct search algorithm may be used in cases where the cost
function over $\vp$ has several local minimums, making convergence difficult.
Furthermore, direct search algorithms can be very computationally demanding.

