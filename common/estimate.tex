%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================

%======================================
\chapter{Estimation Theory }
\label{app:est}
\index{estimation theory}
%======================================
%--------------------------------------
\paragraph{Estimation types.}
%--------------------------------------
Let $\rvx(t;\theta)$ be a waveform with parameter $\theta$.
There are three basic types of estimation on $s$:

\begin{enume}
   \item \ope{detection}:
      \begin{liste}
         \item The waveform $\rvx(t;\theta_n)$ is known except for the value of parameter $\theta_n$.
         \item The parameter $\theta_n$ is one of a finite set of values.
         \item Estimate $\theta_n$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{parametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t;\theta)$ is known except for the value of parameter $\theta$.
         \item The parameter $\theta$ is one of an infinite set of values.
         \item Estimate $\theta$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{nonparametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t)$ is unknown.
         \item Estimate $\rvx(t)$.
      \end{liste}
\end{enume}

%--------------------------------------
\paragraph{Estimation criterion.}
%--------------------------------------
Optimization requires a criterion against which the quality of an
estimate is measured.\footnote{\citergc{srv}{chapters 3, 5}{013125295X}.}
The most demanding and general criterion is the \prope{Bayesian} criterion.
The Bayesian criterion requires knowledge of the probability
distribution functions and the definition of a \fncte{cost function}.
Other criterion are special cases of the Bayesian criterion
such that the cost function is defined in a special way,
no cost function is defined, and/or the distribution is not known
\xref{fig:est-tech}.

%--------------------------------------
\paragraph{Estimation techniques.}
\label{ref:sec:parameter-est}
%--------------------------------------
Estimation techniques can be classified into
five groups \xref{fig:est-tech}:\footnote{%
  \citerpgc{nelles2001}{26}{3540673695}{``Fig 2.2 Overview of linear and nonlinear optimization techniques"},
  \citerpgc{nelles2001}{33}{3540673695}{``Fig 2.5 The Bayes method is the most general approach but\ldots"},
  \citerpgc{nelles2001}{63}{3540673695}{``Table 3.3 Relationship between linear recursive and nonlinear optimization techniques"},
  \citerpg{nelles2001}{66}{3540673695}
  }
\begin{enume}
   \item sequential decoding
   \item norm minimization
   \item gradient search
   \item inner product analysis
   \item direct search
\end{enume}

Sequential decoding is a non-linear estimation family.
Perhaps the most famous of these is the Veterbi algorithm which
uses a trellis to calculate the estimate.
The Verterbi algorithm has been shown to yield an optimal estimate
in the maximal likelihood (ML) sense.
Norm minimization and gradient search algorithms are all linear algorithms.
While this restriction to linear operations often simplifies calculations,
it often yields an estimate that is not optimal in the ML sense.

%--------------------------------------
\section{Estimation criterion}
\label{sec:est_criterion}
%--------------------------------------
\begin{figure}[h]
\centering%
\includegraphics{graphics/latestimation.pdf}
\caption{
   Estimation criterion
   \label{fig:est-criterion}
   }
\end{figure}

%--------------------------------------
\begin{definition}
\index{MAP}
\index{ML}
\index{maximum a-posteriori}
\index{maximum likelihood}
\label{def:MAP}
\label{def:ML}
\label{def:estB}
\label{def:estMS}
\label{def:estMM}
\label{def:estMAP}
\label{def:estML}
%--------------------------------------
Let\\
$\begin{array}{FlM}
    (A).& x(t;\theta)          & be a function with unknown parameter $\theta$
  \\(B).& y(t)                 & a known function which is statistically dependent on $x(t;\theta)$
  \\(C).& C(\theta,\pdfp(x,y)) & be a cost function.
\end{array}$
\tbox{
\setlength{\unitlength}{0.15mm}
\begin{picture}(200,100)(-50,-50)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(- 50,  10){\makebox ( 50, 50)[b]{$x(t)$}            }
  \put(- 50,   0){\vector  (  1,  0)   {50}                }
  \put(   0, -50){\framebox(100,100)[c]{}                  }
  \put(   0, -20){\makebox (100, 50)[t]{stochastic}        }
  \put(   0, -20){\makebox (100, 50)[b]{operator}          }
  \put( 100,  10){\makebox ( 50, 50)[b]{$y(t)$}            }
  \put( 100,   0){\vector  (  1,  0)   {50}                }
\end{picture}
}
\\
Then the following \fnctd{estimate}s are defined as follows:
\defbox{\begin{array}{FMMlc>{\ds}l}
     (1).&\fnctd{Bayesian estimate}                         &                          & \estB   &\eqd& \argmin_{\theta} C(\theta,\pdfp(x,y))
   \\(2).&\fnctd{Mean square estimate}                      &(``\fnctd{MS  estimate}") & \estMS  &\eqd& \argmin_{\theta} \E\norm{C(\theta,\pdfp(x,y))}^2
   \\(3).&\fnctd{mini-max estimate}                         &(``\fnctd{MM  estimate}") & \estMM  &\eqd& \argmin_{\theta}\max_{\pdfp} C(\theta,\pdfp(x,y))
   \\(4).&\mc{2}{M}{\begin{tabular}[t]{@{}l}\fnctd{maximum a-posteriori probability estimate}\\
                                         (``\fnctd{MAP estimate}")
                    \end{tabular}}
         & \estMAP &\eqd& \argmax_{\theta} \psP\setn{x(t;\theta)|y(t)}
   \\(5).&\fnctd{maximum likelihood estimate}               &(``\fnctd{ML  estimate}") & \estML  &\eqd& \argmax_{\theta} \psP\setn{y(t)|x(t;\theta)}
\end{array}}
\end{definition}

%--------------------------------------
\begin{theorem}
\label{thm:map=ml}
%--------------------------------------
Let $\rvx(t;\theta)$ be a function with unknown parameter $\theta$.
\thmbox{
  \brb{\text{$\psP\setn{\theta}=$\prope{constant}}}
  \quad\implies\quad
  \brb{\estMAP = \estML}
  }
\end{theorem}
\begin{proof}
\begin{align*}
   \estMAP
     &\eqd \argmax_{\theta} \psP\setn{s(t;\theta)|r(t)}
     &&    \text{by definition of $\estMAP$}
     &&    \text{\xref{def:estMAP}}
   \\&=    \argmax_{\theta} \frac{\psP\setn{s(t;\theta) \land \rvy(t)}}
                               {\psP\setn{r(t)}}
   \\&=    \argmax_{\theta} \frac{\psP\setn{r(t) | \rvx(t;\theta) }\psP\setn{s(t;\theta) }}
                               {\psP\setn{r(t)}}
   \\&=    \argmax_{\theta} \psP\setn{r(t) | \rvx(t;\theta) }\psP\setn{s(t;\theta) }
   \\&=    \argmax_{\theta} \psP\setn{r(t) | \rvx(t;\theta) }
   \\&\eqd \estML
     &&  \text{by definition of $\estML$}
     &&  \text{\xref{def:estML}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{definition}
\label{def:mse}
%---------------------------------------
\defboxt{
  The \fnctd{mean square error} $\mse(\estT)$ of an estimate $\estT$ of a parameter $\theta$ is defined as
  \\\indentx$\ds\mse(\estT) \eqd \pE\brs{\brp{\estT-\theta}^2}$
  }
\end{definition}

The \fncte{mean square error} of $\estT$ can be expressed as the sum of two components:
the variance of $\estT$ and the bias of $\estT$ squared (next Theorem).
%---------------------------------------
\begin{theorem}
\label{thm:mse}
\footnote{
  \citerpgc{kay1988}{45}{8131733564}{\textsection\scshape``3.3 Estimation Theory"}
  }
%---------------------------------------
\thmbox{
  \mse(\estT) =
      \mcom{\pE\brs{\brp{\estT-\pE\estT}^2}}{variance of $\estT$}
    + \mcom{\brs{\pE\estT - \theta}^2}{bias of $\estT$ squared}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \mse(\estT)
    &\eqd \pE\brs{\brp{\estT-\theta}^2}
    && \text{by definition of $\mse$}
    && \text{\xref{def:mse}}
  \\&= \pE\brs{\brp{\estT-\pE\estT+\pE\estT-\theta}^2}
  \\&= \pE\brs{
         \brp{\estT-\pE\estT}^2
        +\mcom{\brp{\pE\estT-\theta}^2}{constant}
        -2\brp{\estT-\pE\estT}\brp{\pE\estT-\theta}
       }
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
        -2\pE\brs{
         \estT\pE\estT
        -\estT\theta
        -\pE\estT\estT
        +\pE\estT\theta
        }
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
        -2\brs{
         \pE\estT\pE\estT
        -\pE\estT\pE\theta
        -\pE\estT\pE\estT
        +\pE\estT\pE\theta
        }
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
        -0
\end{align*}
\end{proof}

%=======================================
\section{Sufficient statistics}
%=======================================
\prefpp{thm:sstat} (next) shows that the finite set
$\setY\eqd\set{\fdoty_n}{n=1,2,\ldots,N}$ provides just as
much information as having the entire $\rvy(t)$ waveform
(an uncountably infinite number of values)
with respect to the following cases:
\begin{enume}
   \item the conditional probability of $\rvx(t;u)$ given $\rvy(t)$
   \item the \fncte{MAP estimate} of the information sequence
   \item the \fncte{ML estimate} of the information sequence.
\end{enume}
That is, even with a drastic reduction in the amount of information
from uncountably infinite to finite $\xN$,
no information is lost with respect to the quantities listed above.

This amazing result is very useful in practical system implementation
and also for proving other theoretical results
(notably estimation and detection theorems which come later
in this chapter).


%---------------------------------------
\begin{theorem}[\thmd{Sufficient statistic theorem}]
\footnote{
  \citePpc{fisher1922}{316}{``Criterion of Sufficiency"}
  }
\label{thm:sstat}
\index{optimal receiver}
\index{MAP} \index{maximum a-posteriori probability estimation}
\index{ML}  \index{maximum likelihood estimation}
%---------------------------------------
Let $\opSys$ be an additive White Gaussian noise system and
$\Psi$ an orthonormal basis for $\rvx(t;u)$ such that
\\\indentx$\begin{array}{rcl}
     \rvy(t)&=&    \rvx(t;\estT) + \rvv(t)
   \\\Psi  &=&    \set{\psi_n}{n=1,2,\ldots,\xN}
   \\\setY &\eqd& \set{\fdoty_n}{n=1,2,\ldots,\xN}
\end{array}$
\thmbox{
  \brb{\begin{array}{M}
     $\rvv(t)$ is \prope{AWGN}
  \end{array}}
  \implies
  \brb{\begin{array}{Frc>{\ds}lc>{\ds}l}
     (1). & \mc{3}{l}{\psP\set{ \rvx(t;\estT)}{\rvy(t)}} &=& \psP\set{\rvx(t;\estT)}{\setY}
   \\(2). & \estMAP &\eqd& \argmax_{\estT} \psP\set{\rvx(t;\estT)}{\rvy(t)} &=& \argmax_{\estT} \psP\set{\rvx(t;\estT)}{\setY} 
   \\(3). & \estML  &\eqd& \argmax_{\estT} \psP\set{\rvy(t)}{\rvx(t;\estT)} &=& \argmax_{\estT} \psP\set{\setY}{\rvx(t;\estT)}
  \end{array}}
  }
\end{theorem}
\begin{proof}
Let $\ds\rvv'(t) \eqd \rvv(t) - \sum_{n=1}^\xN \fdotv_n \psi_n(t)$.
\begin{enumerate}
\item The relationship between $\setY$ and $\rvv'(t)$ is given by
\begin{align*}
   \rvy(t)
     &= \sum_{n=1}^\xN \inprod{\rvy(t)}{\psi_n(t)}\psi_n(t) +
        \left[\rvy(t)- \sum_{n=1}^\xN \inprod{\rvy(t)}{\psi_n(t)}\psi_n(t) \right]
   \\&= \sum_{n=1}^\xN \inprod{\rvy(t)}{\psi_n(t)}\psi_n(t) +
        \left[\rvy(t)- \sum_{n=1}^\xN \inprod{\rvx(t)+\rvv(t)}{\psi_n(t)}\psi_n(t) \right]
   \\&= \sum_{n=1}^\xN \fdoty_n\psi_n(t) +
        \left[\rvx(t)+\rvv(t) - \sum_{n=1}^\xN \inprod{\rvx(t)}{\psi_n(t)}\psi_n(t)
                        - \sum_{n=1}^\xN \inprod{\rvv(t)}{\psi_n(t)}\psi_n(t) \right]
   \\&= \sum_{n=1}^\xN \fdoty_n\psi_n(t) +
        \rvx(t)+\rvv(t) - \rvx(t) - \left[ \rvv(t) - \rvv'(t)\right]
   \\&= \sum_{n=1}^\xN \fdoty_n\psi_n(t) + \rvv'(t).
\end{align*}

\item Proof that the set of statistics $\setY$ and the random process $\rvv'(t)$ are \prope{uncorrelated}:
\begin{align*}
   \pE\brs{\fdoty_n \rvv'(t)}
     &= \pE\brs{\inprod{\rvy(t)}{\psi_n(t)}\left( \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\psi_n(t)}\psi_n(t)\right)}
   \\&= \pE\brs{\inprod{\rvx(t)+\rvv(t)}{\psi_n(t)}\left( \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\psi_n(t)}\psi_n(t)\right)}
   \\&= \pE\brs{\Bigg(\inprod{\rvx(t)}{\psi_n(t)}+\inprod{\rvv(t)}{\psi_n(t)}\Bigg)
            \left( \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\psi_n(t)}\psi_n(t)\right)}
   \\&= \pE\brs{\Bigg(\fdotx_n+\fdotv_n\Bigg)
            \left( \rvv(t)-\sum_{n=1}^\xN \fdotv_n\psi_n(t)\right)}
   \\&= \pE\brs{\fdotx_n \rvv(t) - \fdotx_n \sum_{n=1}^\xN \fdotv_n\psi_n(t)
            +\fdotv_n \rvv(t) - \fdotv_n \sum_{n=1}^\xN \fdotv_n\psi_n(t) }
   \\&= \pE\brs{\fdotx_n \rvv(t)} -
        \pE\brs{\fdotx_n \sum_{n=1}^\xN \fdotv_n\psi_n(t)} +
        \pE\brs{\inprod{\rvv(t)}{\psi_n(t)} \rvv(t)} -
        \pE\brs{\sum_{m=1}^\xN \fdotv_n \fdotv_m\psi_m(t)}
   \\&= \fdotx_n \cancelto{0}{\pE{\rvv(t)}} -
        \fdotx_n \sum_{n=1}^\xN \cancelto{0}{\pE\brs{\fdotv_n}}\psi_n(t) +
        \pE\brs{\inprod{\rvv(t)\rvv(\estT)}{\psi_n(\estT)} } -
        \sum_{m=1}^\xN \pE\brs{\fdotv_n \fdotv_m}\psi_m(t)
   \\&= 0 - 0 +
        \inprod{\pE\brs{\rvv(t)n(\estT)}}{\psi_n(\estT)} -
        \sum_{m=1}^\xN \xN_o\kdelta_{mn} \psi_m(t)
     \qquad\text{(because $\fdotv_n$ is white)}
   \\&= \inprod{\xN_o\delta(t-\estT)}{\psi_n(\estT)} - \xN_o\psi_n(t)
   \\&= \xN_o\psi_n(t) - \xN_o\psi_n(t)
   \\&= 0
\end{align*}

\item This implies $\fdoty_n$ and $\rvv'(t)$ are uncorrelated.
Since they are Gaussian processes (due to channel operator hypothesis),
they are also independent.

\item Proof that $P\set{\rvx(t;\estT)}{\rvy(t)}=P\set{\rvx(t;\estT)}{\fdoty_1,\;\fdoty_2,\ldots,\fdoty_{\xN}}$:
\begin{align*}
   \psP\set{\rvx(t;\estT)}{\rvy(t)}
     &= \psP\set{\rvx(t;\estT)}{\sum_{n=1}^\xN\fdoty_n \psi_n(t) + \rvv'(t)}
   \\&= \psP\set{\rvx(t;\estT)}{R, \rvv'(t)}
     && \text{because $\setY$ and $\rvv'(t)$ can be extracted by $\inprod{\cdots}{\fpsi_n(t)}$}
   \\&= \frac{\psP\set{R, \rvv'(t)}{\rvx(t;\estT)}  P\setn{\rvx(t;\estT)} }
             {\psP\setn{R,\rvv'(t)}}
   \\&= \frac{\psP\setn{ R|\rvx(t;\estT)}\psP\setn{ \rvv'(t)|\rvx(t;\estT)}\psP\setn{\rvx(t;\estT)}}
             {\psP\setn{\setY}\psP\setn{\rvv'(t)}}
     && \text{by \prope{independence} of $\setY$ and $\rvv'(t)$}
   \\&= \frac{\psP\setn{ R|\rvx(t;\estT)}\psP\setn{ \rvv'(t)}\psP\setn{\rvx(t;\estT)}}
             {\psP\setn{\setY}\psP\setn{\rvv'(t)}}
   \\&= \frac{\psP\setn{ R|\rvx(t;\estT)} \psP\setn{\rvx(t;\estT)}}
             {\psP\setn{\setY}}
   \\&= \frac{\psP\setn{ R,\rvx(t;\estT)}}
             {\psP\setn{\setY}}
   \\&= \psP\set{\rvx(t;\estT)}{\setY}
\end{align*}

\item Proof that $\setY$ is a sufficient statistic for the \vale{MAP estimate}:
\begin{align*}
   \estMAP
     &\eqd \argmax_{\estT} \psP\setn{\rvx(t;\estT)|\rvy(t)}
     &&    \text{by definition of \vale{MAP estimate}}
   \\&=    \argmax_{\estT} \psP\setn{\rvx(t;\estT)|R}
     &&    \text{by result 4.}
\end{align*}

\item Proof that $\setY$ is a sufficient statistic for the \vale{ML estimate}:
\begin{align*}
   \estML
     &\eqd \argmax_{\estT} \psP\setn{\rvy(t)|\rvx(t;\estT)}
     &&    \text{by definition of \fncte{ML estimate}}
   \\&=    \argmax_{\estT} \psP\setn{\sum_{n=1}^\xN\fdoty_n\psi_n(t)+\rvv'(t)|\rvx(t;\estT)}
   \\&=    \argmax_{\estT} \psP\setn{R,\rvv'(t)|\rvx(t;\estT)}
     &&    \text{because $\setY$ and $\rvv'(t)$ can be extracted by  $\inprod{\cdots}{\fpsi_n(t)}$}
   \\&=    \argmax_{\estT} \psP\setn{R|\rvx(t;\estT)}\psP\setn{\rvv'(t)|\rvx(t;\estT)}
     &&    \text{by \prope{independence} of $\setY$ and $\rvv'(t)$}
   \\&=    \argmax_{\estT} \psP\setn{R|\rvx(t;\estT)}\psP\setn{\rvv'(t)}
     &&    \text{by \prope{independence} of $\rvx(t)$ and $\rvv'(t)$}
   \\&=    \argmax_{\estT} \psP\setn{R|\rvx(t;\estT)}
     &&    \text{by \prope{independence} of $\rvv'(t)$ and $\estT$}
\end{align*}
\end{enumerate}
\end{proof}


Depending on the nature of the channel (additive, white, and/or Gaussian)
we can know certain characteristics of the noise and received statistics.
These are described in the next four theorems.
%======================================
%\subsection{Additive noise channel}
%\label{sec:opCan}
%======================================


%---------------------------------------
\begin{theorem}%[Additive noise projection statistics]
\label{thm:an_stats}
\index{projection statistics!Additive noise channel}
%---------------------------------------
Let $\opC=\opCan$ be an additive noise channel.
\thmbox{
\mcom{\opC=\opCan}{additive noise channel}
\implies
\left\{
\begin{array}{lrcl}
   \pE(\fdoty_n|\theta)       &= \fdotx_n(\theta) + \pE \fdotv_n
\end{array}
\right.
}
\end{theorem}
\begin{proof}
\begin{align*}
   \pE(\fdoty_n |\theta)
     &\eqd \brp{\inprod{\rvy(t)}{\psi_n(t)}  |\theta}
   \\&=    {\inprod{\rvx(t;\theta)+\fn(t)}{\psi_n(t)}}
   \\&=    {\inprod{\rvx(t;\theta)}{\psi_n(t)}} +   {\inprod{\fn(t)}{\psi_n(t)}}
   \\&=    \inprod{\sum_{k=1}^\xN \fdotx_k(\theta) \psi_k(t)}{\psi_n(t)} + \fdotv_n
   \\&=    \sum_{k=1}^\xN \fdotx_k(\theta) \inprod{\psi_k(t)}{\psi_n(t)} + \fdotv_n
   \\&=    \fdotx_n(\theta)  + \fdotv_n
\\ \\
   \pE({\fdoty_n} | \theta)
     &= \pE\brs{\fdotx_n(\theta)  + \fdotv_n}
   \\&= \pE{\fdotx_n(\theta) } +   \pE{\fdotv_n}
   \\&= \fdotx_n(\theta)
\end{align*}
\end{proof}


%======================================
%\subsection{Additive gaussian noise channel}
%\label{sec:opCagn}
%======================================
\begin{figure}[ht] \color{figcolor}
\centering%
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotv_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdoty_n|\theta_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdoty_n|\theta_2)$} }
\end{picture}
\caption{
  Additive Gaussian noise channel Statistics
   %\label{fig:awgn_stats}
   }
\end{figure}



%---------------------------------------
\begin{theorem}%[Additive Gaussian noise projection statistics]
\label{thm:agn_stats}
\index{projection statistics!Additive Gaussian noise channel}
%---------------------------------------
Let $\opC=\opCagn$ be an additive gaussian noise channel
with distribution $\fn(t)\sim\pN{0}{\sigma^2}$ for all $t$.
%(see \prefpp{fig:agn_stats})
\thmbox{
\mcom{\opC=\opCagn}{additive Gaussian channel}
\implies
\left\{
\begin{array}{rclD}
   \pE\fdotv_n           &=   & 0                               & \\%\text{\scriptsize(noise projection is zero-mean                )} \\
   \pE(\fdoty_n|\theta)  &=   & \fdotx_n(\theta)                & \\%\text{\scriptsize(expected receiver projection = transmitted projection )} \\
   \fdotv_n              &\sim& \pN{0}{\sigma^2}                & (noise projections are Gaussian        ) \\
   \fdoty_n|\theta       &\sim& \pN{\fdotx_n(\theta)}{\sigma^2} & (receiver projections are Gaussian     ) \\
\end{array}
\right.
}
\end{theorem}
\begin{proof}
\begin{align*}
   \pE\fdotv_{n}
     &= \pE\inprod{\fn(t)}{\psi_n(t)}
   \\&= \inprod{\pE\fn(t)}{\psi_n(t)}
   \\&= \inprod{0}{\psi_n(t)}
   \\&= 0
\\
\\
   (\fdoty_n |\theta)
     &\eqd {\inprod{\rvy(t)}{\psi_n(t)}}  |\theta
   \\&=    {\inprod{\rvx(t;\theta)+\fn(t)}{\psi_n(t)}}
   \\&=    {\inprod{\rvx(t;\theta)}{\psi_n(t)}} +   {\inprod{\fn(t)}{\psi_n(t)}}
   \\&=    \inprod{\sum_{k=1}^\xN \fdotx_k(\theta) \psi_k(t)}{\psi_n(t)} + \fdotv_n
   \\&=    \sum_{k=1}^\xN \fdotx_k(\theta) \inprod{\psi_k(t)}{\psi_n(t)} + \fdotv_n
   \\&=    \fdotx_n(\theta)  + \fdotv_n
\\ \\
   \pE({\fdoty_n} | \theta)
     &= \pE\brs{\fdotx_n(\theta)  + \fdotv_n}
   \\&= \pE{\fdotx_n(\theta) } +   \pE{\fdotv_n}
   \\&= \fdotx_n(\theta)
\end{align*}

The distributions follow because they are linear operations on
Gaussian processes.
\end{proof}




%======================================
%\subsection{Additive white noise channel}
%\label{sec:opCawn}
%======================================


%---------------------------------------
\begin{theorem}%[Additive white noise projection statistics]
\label{thm:awn_stats}
\index{projection statistics!Additive white noise channel}
%---------------------------------------
Let $\opC=\opCawn$ be an additive white noise channel.
\thmbox{
\mcom{\opC=\opCawn}{additive white channel}
\implies
\left\{
\begin{array}{lclD}
   \pE\fdotv_n                            &=& 0                     & (noise projection is zero-mean) \\
   \pE(\fdoty_n|\theta)                   &=& \fdotx_n(\theta)      & (expected receiver projection = transmitted projection) \\
   \cov{\fdotv_n}{\fdotv_m}               &=& \sigma^2 \kdelta_{nm} & (noise projections are uncorrelated) \\
   \cov{\fdoty_n|\theta}{\fdoty_m|\theta }&=& \sigma^2 \kdelta_{nm} & (receiver projections are uncorrelated)
\end{array}
\right.
}
\end{theorem}

\begin{proof}
Because the noise is additive (see \prefpp{thm:an_stats})
\begin{align*}
   \pE\fdotv_{n}           &= 0  \\
   (\fdoty_n |\theta)      &= \fdotx_n(\theta)  + \fdotv_n \\
   \pE({\fdoty_n} | \theta) &= \fdotx_n(\theta).
\end{align*}

Because the noise is also white,
\begin{align*}
   \cov{\fdotv_m}{\fdotv_n}
      &= \cov{\inprod{\fn(t)}{\psi_m(t)}}{\inprod{\fn(t)}{\psi_n(t)}}
    \\&= \pE\brs{\inprod{\fn(t)}{\psi_m(t)} \inprod{\fn(t)}{\psi_n(t)}}
    \\&= \pE\brs{\inprod{\fn(t)}{\psi_m(t)} \inprod{n(u)}{\psi_n(u)}}
    \\&= \pE\brs{ \inprod{n(u)\inprod{\fn(t)}{\psi_m(t)}}{\psi_n(u)}}
    \\&= \pE\brs{ \inprod{\inprod{n(u)\fn(t)}{\psi_m(t)}}{\psi_n(u)}}
    \\&= \inprod{\inprod{\pE\brs{ n(u)\fn(t)}}{\psi_m(t)}}{\psi_n(u)}
    \\&= \inprod{\inprod{\sigma^2 \delta(t-u)}{\psi_m(t)}}{\psi_n(u)}
    \\&= \sigma^2 \inprod{\psi_n(t)}{\psi_m(t)}
    \\&= \left\{
          \begin{tabular}{ll}
             $\sigma^2$ & for $n=m$ \\
             $0$   & for $n\ne m$.
          \end{tabular}
          \right.
\\
\\
   \cov{\fdoty_n|\theta}{\fdoty_m|\theta }
      &= \pE\brs{\fdoty_n \fdoty_m |\theta} - [\pE\fdoty_n|\theta][\pE\fdoty_m|\theta ]
    \\&= \pE\brs{(\fdotx_n(\theta) +\fdotv_n)(\fdotx_m(\theta) +\fdotv_m)} - \fdotx_n(\theta) \fdotx_m(\theta)
    \\&= \pE\brs{(\fdotx_n(\theta) +\fdotv_n)(\fdotx_m(\theta) +\fdotv_m)} - \fdotx_n(\theta) \fdotx_m(\theta)
    \\&= \pE\brs{\fdotx_n(\theta) \fdotx_m(\theta) +\fdotx_n(\theta) \fdotv_m+ \fdotv_n\fdotx_m(\theta) +\fdotv_n\fdotv_m } - \fdotx_n(\theta) \fdotx_m(\theta)
    \\&= \fdotx_n(\theta) \fdotx_m(\theta) + \fdotx_n(\theta) \pE\brs{\fdotv_m}+ \pE\brs{\fdotv_n}\fdotx_m(\theta) +\pE\brs{\fdotv_n\fdotv_m}  - \fdotx_n(\theta) \fdotx_m(\theta)
    \\&= 0 + \fdotx_n(\theta) \cdot0 + 0\cdot\fdotx_m(\theta) + \cov{\fdotv_n}{\fdotv_m}+[\pE\fdotv_n][\pE\fdotv_m]
    \\&= \sigma^2 \kdelta_{nm} + 0\cdot0
    \\&= \left\{
          \begin{tabular}{ll}
             $\sigma^2$ & for $n=m$ \\
             $0$   & for $n\ne m$.
          \end{tabular}
          \right.
\end{align*}
\end{proof}


%======================================
%\subsection{Additive white gaussian noise channel}
%\label{sec:opCawgn}
%======================================
\begin{figure}[ht] \color{figcolor}
\centering%
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotv_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdoty_n|\theta_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdoty_n|\theta_2)$} }
\end{picture}
\caption{
  Additive white Gaussian noise channel statistics
   \label{fig:awgn_stats}
   }
\end{figure}



%---------------------------------------
\begin{theorem}%[AWGN projection statistics]
\label{thm:awgn_stats}
\label{thm:ms_stats}
\index{projection statistics!Additive white Gaussian noise channel}
%---------------------------------------
Let $\opC=\opCawgn$ be an additive gaussian noise channel
with distribution $\fn(t)\sim\pN{0}{\sigma^2}$ for all $t$.
\thmbox{
\mcom{\opC=\opCawgn}{AWGN}
\implies
\left\{
\begin{array}{rclD}
   \fdotv_n                            &\sim& \pN{0}{\sigma^2}                     & (noise projections are Gaussian        ) \\
   \fdoty_n|\theta                     &\sim& \pN{\fdotx_n(\theta)}{\sigma^2}      & (receiver projections are Gaussian     ) \\
   \cov{\fdotv_n}{\fdotv_m}            &=   & \sigma^2 \kdelta_{nm}                & (noise projections are uncorrelated    ) \\
   \cov{\fdoty_n}{\fdoty_m }           &=   & \sigma^2 \kdelta_{nm}                & (receiver projections are uncorrelated ) \\
   \psp\{\fdotv_n=a \land \fdotv_m=b\} &=   & \psp\{\fdotv_n=a\}\psp\{\fdotv_m=b\} & (noise    projections are independent  ) \\
   \psp\{\fdoty_n=a \land \fdoty_m=b\} &=   & \psp\{\fdoty_n=a\}\psp\{\fdoty_m=b\} & (receiver projections are independent  )
\end{array}
\right.
}
\end{theorem}

\begin{proof}
The distributions follow because they are linear operations on
Gaussian processes.

By \prefpp{thm:awn_stats} (for AWN channel)
\begin{align*}
   \pE{\fdotv_{n}} &= 0
\\
   \cov{\fdotv_m}{\fdotv_n} &= \sigma^2 \kdelta_{mn}
\\
   \fdoty_n &= \fdotx_n  + \fdotv_n
\\
   \pE{\fdoty_n} &= \fdotx_n
\\
   \cov{\fdoty_n}{\fdoty_m } &= \sigma^2 \kdelta_{mn}
\end{align*}
Because the processes are Gaussian,
uncorrelatedness implies \prope{independence}.
\end{proof}




%======================================
\section{Optimal symbol estimation}
\label{sec:awgn_est}
\index{maximum likelihood estimation}
%======================================
The AWGN projection statistics provided by
\prefpp{thm:awgn_stats} help generate the optimal
ML-estimates for a number of communication systems.
These ML-estimates can be expressed in either of two standard forms:
\begin{liste}
  \item {\bf Spectral decompostion}:
     The optimal estimate is expressed in terms of \hie{projections}
     of signals onto orthonormal basis functions.
  \item {\bf Matched signal}:
     The optimal estimate is expressed in terms of the (noisy)
     received signal correlated with (``matched" with)
     the (noiseless) transmitted signal.
\end{liste}
\prefpp{thm:estML_general} (next) expresses the general
optimal \fncte{ML estimate} in both of these forms.

Parameter detection is a special case of parameter estimation.
In parameter detection, the estimate is a member of an finite set.
In parameter estimation, the estimate is a member of an infinite set
\xref{sec:awgn_est}.


%---------------------------------------
\begin{theorem}[\thmd{General ML estimation}]
\index{maximum likelihood estimation!general}
\label{thm:estML_general}
\label{thm:ml_est_det}
%\citepp{srv}{157}{158}
%---------------------------------------
Let $\Psi$ be an orthonormal set spanning $\rvx(t)$ such that
\\\indentx$\begin{array}{rcl}
    \Psi     &\eqd& \seqn{\psi_1(t), \psi_2(t), \ldots, \psi_n(t)}
  \\\fdoty_n &\eqd& \inprod{\rvy(t)}{\psi_n(t)}
  \\\fdotx_n &\eqd& \inprod{\rvx(t)}{\psi_n(t)}
  \\\rvy(t)  &=   & \rvx(t;\estT) + \fn(t).
\end{array}$
\\
Then the optimal ML-estimate $\estML$ of parameter $\theta$ is
\thmbox{\begin{array}{rc>{\ds}l@{\qquad}D}
   \estML
     &=& \argmin_{\estT} \left[ \sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
       & (spectral decomposition)
   \\&=& \argmax_{\estT}
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\estT)}-\norm{\rvx(t;\estT)}^2 \right]
       & (matched signal)
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML
     &= \argmax_{\estT} \psP\setn{\rvy(t)|\rvx(t;\estT)}
   \\&= \argmax_{\estT} \psP\setn{\fdoty_1,\fdoty_2,\ldots,\fdoty_n|\rvx(t;\estT)}
     && \text{by \prefpp{thm:sstat}}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN \psP\setn{\fdoty_n|\rvx(t;\estT)}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN \pdfpb{\fdoty_n|\rvx(t;\estT)}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN
         \frac{1}{\sqrt{2\pi\sigma^2}}
         \exp{\frac{[\fdoty_n-\fdotx_n(\estT)]^2}{-2\sigma^2} }
     && \text{by \prefpp{thm:awgn_stats}}
   \\&= \argmax_{\estT}
         \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
         \exp{\frac{-1}{2\sigma^2} \sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 }
   \\&= \argmax_{\estT}
         \left[ -\sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
\\ \\
   \\&= \argmax_{\estT}
         \left[ -\lim_{N\to\infty}\sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
     && \text{by \prefpp{thm:sstat}}
   \\&= \argmax_{\estT}
         \left[ -\norm{\rvy(t)-\rvx(t;\estT)}^2 \right]
     && \text{by \thme{Plancheral's formula}}
     && \text{\xref{thm:plancherel}}
   \\&= \argmax_{\estT}
         \left[ -\norm{\rvy(t)}^2 +2\Real\inprod{\rvy(t)}{\rvx(t;\estT)}-\norm{\rvx(t;\estT)}^2 \right]
   \\&= \argmax_{\estT}
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\estT)}-\norm{\rvx(t;\estT)}^2 \right]
     && \text{because $\rvy(t)$ \prope{independent} of $\estT$}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{ML amplitude estimation}]
\label{thm:estML_amplitude}
\footnote{
  \citerppg{srv}{158}{159}{013125295X}
  }
\index{maximum likelihood estimation!amplitude}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
\begin{align*}
   \rvy(t)     &=     [\opCawgn s](t) = \rvx(t;a) + \fn(t) \\
   \rvx(t;a)   &\eqd  a  \sym(t).
\end{align*}
Then
\thmbox{\begin{array}{rclM}
  \estML[a]    &=&  \frac{1}{\norm{\lambda(t)}^2} \inprod{\rvy(t)}{\lambda(t)}
               &    (optimal ML-estimate of $a$)
             \\&=&  \frac{1}{\norm{\sym(t)}^2} \sum_{n=1}^\xN \fdoty_n \fdotlam_n
               & %\text{(optimal ML-estimate of $a$)}
\\\pE\estML[a] &=& a
               &   ($\estML[a]$ is \propb{unbiased})
\\\var\estML[a]&=& \frac{\sigma^2}{\norm{\sym(t)}^2}
               &  (variance of estimate $\estML[a]$)
\\\var\estML[a]&=& \mbox{CR lower bound}
               &   ($\estML[a]$ is an {\bf \prope{efficient} estimate})
\end{array}}
\end{theorem}

\begin{proof}\\
\begin{enumerate}
\item \fncte{ML estimate} in ``matched signal" form:
\begin{align*}
   \estML[a]
     &= \argmax_a
         \left[ 2\inprod{\rvy(t)}{\rvx(t;u)}-\norm{\rvx(t;\phi)}^2 \right]
     && \text{by \prefpp{thm:estML_general}}
   \\&= \argmax_a
         \left[ 2\inprod{\rvy(t)}{a\lambda(t)}-\norm{a\lambda(t)}^2 \right]
     && \text{by hypothesis}
   \\&= \arg_a
         \left[ \pderiv{}{a}2a\inprod{\rvy(t)}{\lambda(t)}-\pderiv{}{a}a^2\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ 2\inprod{\rvy(t)}{\lambda(t)}-2a\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ \inprod{\rvy(t)}{\lambda(t)}=a\norm{\lambda(t)}^2 \right]
   \\&= \frac{1}{\norm{\lambda(t)}^2} \inprod{\rvy(t)}{\lambda(t)}
\end{align*}

\item \fncte{ML estimate} in ``spectral decomposition" form:
\begin{align*}
   \estML[a]
     &= \argmin_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}^2 }
     && \text{by \prefpp{thm:estML_general}}
   \\&= \arg_a
         \brp{ \pderiv{}{ a }\sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}^2=0 }
   \\&= \arg_a
         \brp{ 2\sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}\pderiv{}{ a }\fdotx_n( a )=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n - \inprod{ a \lambda(t)}{\psi_n(t)}}\pderiv{}{ a }\inprod{ a \lambda(t)}{\psi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \inprod{\lambda(t)}{\psi_n(t)}}\pderiv{}{ a }( a \inprod{\lambda(t)}{\psi_n(t)})=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \fdotlam_n } \inprod{\lambda(t)}{\psi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \fdotlam_n } \fdotlam_n=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \fdoty_n\fdotlam_n = \sum_{n=1}^\xN  a \fdotlam_n^2 }
   \\&= \brp{\frac{1}{\sum_{n=1}^\xN \fdotlam_n^2}}
         \sum_{n=1}^\xN \fdoty_n\fdotlam_n
   \\&= \frac{1}{\norm{\lambda(t)}^2}
         \sum_{n=1}^\xN \fdoty_n\fdotlam_n
\end{align*}

\item Prove that the estimate $\estML[a]$ is \propb{unbiased}:

\begin{align*}
   \pE\estML[a]
     &= \pE\frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} \rvy(t)\sym(t) \dt
     && \text{by previous result}
   \\&= \pE\frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} [ a \sym(t)+\fn(t)]\sym(t) \dt
     && \text{by hypothesis}
   \\&= \frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} \pE[ a \sym(t)+\fn(t)]\sym(t) \dt
     && \text{by linearity of $\int\cdot\dt$ and $\pE$}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \int_{t\in\R} \sym^2(t) \dt
     && \text{by $\pE$ operation}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \norm{\sym(t)}^2
     && \text{by definition of $\norm{\cdot}^2$}
   \\&=   a
\end{align*}

\item Compute the variance of $\estML[a]$:
\begin{align*}
  \pE \estML[a]^2
    &= \pE \left[ \frac{1}{\norm{\lambda(t)}^2} \int_{t\in\R} \rvy(t)\lambda(t) \dt\right]^2
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \rvy(t)\lambda(t) \dt \int_v \rvy(v)\lambda(v) \dv
        \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v [a\lambda(t) + \fn(t)][a\lambda(v) + \fn(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v
        [a^2\lambda(t)\lambda(v) + a\lambda(t)\fn(v) + a\lambda(v)\fn(t) + \fn(t)\fn(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v
        [a^2\lambda(t)\lambda(v) + 0 + 0 + \sigma^2\delta(t-v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v a^2\lambda^2(t)\lambda^2(v) \dv\dt +
        \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v \sigma^2\delta(t-v) \lambda(t) \lambda(v) \dv\dt
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        a^2 \int_{t\in\R} \lambda^2(t) \dt \int_v \lambda^2(v) \dv +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2\int_{t\in\R} \lambda^2(t) \dt
  \\&= a^2 \frac{1}{\norm{\lambda(t)}^4}
        \norm{\lambda(t)}^2 \norm{\lambda(v)}^2 +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2 \norm{\lambda(t)}^2
  \\&= a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}
\\
\\
  \var\estML[a]
    &= \pE \estML[a]^2 - (\pE \estML[a])^2
  \\&= \left.\left.\left(a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}\right) - \right( a^2 \right)
  \\&= \frac{\sigma^2}{\norm{\lambda(t)}^2}
\end{align*}

\item Compute the Cram\'er-Rao Bound:
\begin{align*}
   \pdfpb{\rvy(t)|s(t; a)}
     &=  \pdfpb{\fdoty_1, \fdoty_2,\ldots,\fdoty_{\xN}|s(t; a)}
   \\&=  \prod_{n=1}^\xN \frac{1}{\sqrt{2\pi\sigma^2}}
          \exp{\frac{(\fdoty_n- a\fdotlam_n)^2}{-2\sigma^2}}
   \\&=  \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
\\
\\
   \pderiv{}{a}\ln\pdfpb{\rvy(t)|s(t; a)}
     &=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
       +  \pderiv{}{a}\ln
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}
          \frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2
   \\&=  \frac{1}{-2\sigma^2} \sum_{n=1}^\xN 2(\fdoty_n- a\fdotlam_n)(- \fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdoty_n- a\fdotlam_n)
\\
\\
   \pderiv{^2}{a^2}\ln\pdfpb{\rvy(t)|s(t; a)}
     &=  \pderiv{}{a}\pderiv{}{a}\ln\pdfpb{\rvy(t)|s(t; a)}
   \\&=  \pderiv{}{a}
          \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdoty_n- a\fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(-\fdotlam_n)
   \\&=  \frac{-1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n^2
   \\&=  \frac{-\norm{\lambda(t)}^2}{\sigma^2}
\\
\\
   \var\estML[a]
     &\eqd \pE\brs{\estML[a]-\pE\estML[a]}^2
   \\&=    \pE\brs{\estML[a]- a}^2
   \\&\ge  \frac{-1}{\pE\brp{\pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t)|s(t; a)}}}
   \\&=    \frac{-1}{\pE\brp{\frac{-\norm{\lambda(t)}^2}{\sigma^2}}}
   \\&=    \frac{\sigma^2}{\norm{\lambda(t)}^2}
     \qquad\text{(Cram\'er-Rao lower bound of the variance)}
\end{align*}

\item Prove that $\estML[a]$ is an {\bf \prope{efficient} estimate}:

A estimate is \prope{efficient} if
$\var\estML[a]=\mbox{CR lower bound}$.
We have already proven this, so $\estML[a]$ is an \prope{efficient} estimate.

Also, even without explicitly computing the variance of $\estML[a]$,
the variance equals the \vale{Cram\'er-Rao lower bound}
(and hence $\estML[a]$ is an \prope{efficient} estimate)
if and only if
\\\indentx$\ds\estML[a] -  a =
   \brp{\frac{-1}{\pE\brs{
              \pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t)|s(t; a)}
           }}}
   \brp{\pderiv{}{ a} \ln \pdfpb{\rvy(t)|s(t; a)}}
  $
\begin{align*}
   \brp{\frac{-1}{\pE\brp{\pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t)|s(t; a)}}}}
   \brp{\pderiv{}{ a} \ln \pdfpb{\rvy(t)|s(t; a)}}
     &= \left(\frac{\sigma^2}{\norm{\lambda(t)}^2}\right)
         \left(
           \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam( \fdoty - a \fdotlam)
         \right)
   \\&= \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam \fdoty -
         \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam^2
   \\&= \estML[a] - a
\end{align*}
\end{enumerate}
\end{proof}


%---------------------------------------
\begin{theorem}[\thmd{ML phase estimation}]
\label{thm:estML_phase}
\footnote{
  \citerppg{srv}{159}{160}{013125295X}
  }
\index{maximum likelihood estimation!phase}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
$\begin{array}[t]{rcl}
   \rvy(t)     &=& [\opCawgn s](t) = \rvx(t;\phi) + \fn(t) \\
   \rvx(t;\phi) &=& A\cos(2\pi f_ct +  \phi).
\end{array}$
\\
Then the optimal ML-estimate of parameter $ \phi $ is
\thmbox{
   \estML[\phi]
      =   -\atan\left(
           \frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           \right)
   }
\end{theorem}
\begin{proof}
\begin{align*}
   \estML[\phi]
     &= \argmax_\phi
         \left[ 2\inprod{\rvy(t)}{\rvx(t;u)}-\norm{\rvx(t;\phi)}^2 \right]
     && \text{by \prefpp{thm:estML_general}}
   \\&= \argmax_\phi
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\phi)} \right]
     && \text{because $\norm{\rvx(t;\phi)}$ does not depend on $\phi$}
   \\&= \arg_\phi
         \left[ \pderiv{}{\phi} \inprod{\rvy(t)}{\rvx(t;\phi)} = 0 \right]
   \\&= \arg_\phi
         \left[ \inprod{\rvy(t)}{\pderiv{}{\phi} \rvx(t;\phi)} = 0 \right]
     && \text{because $\inprod{\cdot}{\cdot}$ is a linear operator}
   \\&= \arg_\phi
         \left[ \inprod{\rvy(t)}{\pderiv{}{\phi} A\cos(2\pi f_ct+\phi)} = 0 \right]
   \\&= \arg_\phi
         \left[ \inprod{\rvy(t)}{-A\sin(2\pi f_ct+\phi)} = 0 \right]
   \\&= \arg_\phi
         \left[ -A\inprod{\rvy(t)}{\cos(2\pi f_ct)\sin\phi+\sin(2\pi f_ct)\cos\phi} = 0 \right]
   \\&= \arg_\phi \left[
           \sin\phi\inprod{\rvy(t)}{\cos(2\pi f_ct)} =
          -\cos\phi\inprod{\rvy(t)}{\sin(2\pi f_ct)}
           \right]
   \\&= \arg_\phi \left[
           \frac{\sin\phi}{\cos\phi} =
          -\frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           \right]
   \\&= \arg_\phi \left[
           \tan\phi =
          -\frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           \right]
   \\&=  -\atan\left(
           \frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           \right)
\end{align*}
\end{proof}


%---------------------------------------
\begin{theorem}[\thmd{ML estimation of a function of a parameter}]
\label{thm:estML-CR}
\footnote{
  \citerppg{srv}{142}{143}{013125295X}
  }
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
$\begin{array}[t]{rcl}
   \rvy(t)     &=& [\opCawgn s](t) = \rvx(t;u) + \fn(t) \\
   \rvx(t;u)   &=& \fg( u )
\end{array}$\\
where $\fg$ is one-to-one and onto (invertible).
\\
\thmbox{\begin{array}{M>{\ds}rc>{\ds}l}
  Then the optimal ML-estimate of parameter $ u $ is
   & \estML &=& \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right).
  \\
  If an \fncte{ML estimate} $\estML[u]$ is unbiased ($\pE \estML[u] = \theta$) then
    & \var\estML[u] &\ge&
      \frac{\sigma^2}{\xN}
      \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}.
  \\
  If $\fg(\theta) = \theta$ then $\estML[u]$ is an {\bf \prope{efficient} estimate} such that
   & \var\estML[u] &=& \frac{\sigma^2}{\xN}.
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML[u]
     &= \argmin_u
         \left[ \sum_{n=1}^\xN [\fdoty_n-\fg( u )]^2 \right]
   \\&= \arg_u\left[
            \pderiv{}{ u }\sum_{n=1}^\xN [\fdoty_n-\fg( u )]^2 = 0
         \right]
   \\&= \arg_u\left[
             2\sum_{n=1}^\xN [\fdoty_n-\fg( u )]\pderiv{}{ u }\fg( u ) = 0
         \right]
   \\&= \arg_u\left[
             2\sum_{n=1}^\xN [\fdoty_n-\fg( u )] = 0
         \right]
   \\&= \arg_u\left[
             \sum_{n=1}^\xN \fdoty_n = \xN \fg( u )
         \right]
   \\&= \arg_u\left[
             \fg( u ) = \frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n
         \right]
   \\&= \arg_u\left[
              u  = \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right)
         \right]
   \\&= \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right)
\end{align*}


If $\estML[u]$ is unbiased ($\pE\estML[u]= u $), we can use
the Cram\'er-Rao bound to find a lower bound on the variance:

\begin{align*}
   \var\estML[u]
     &\eqd \pE\brs{\estML[u]-\pE\estML[u]}^2
   \\&= \pE\brs{\estML[u]-\theta}^2
   \\&\ge \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t)|s(t;\theta)}
           \right)}
   \\&=   \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln
              \pdfpb{\fdoty_1, \fdoty_2,\ldots,\fdoty_{\xN}|s(t;\theta)}
           \right)}
   \\&=   \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
              \exp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }\right]
           \right)}
   \\&=   \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN \right]
              +
              \pderiv{^2}{\theta^2} \ln\left[
              \exp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }\right]
           \right)}
  \\&=   \frac{-1}{\pE\left(
             \pderiv{^2}{\theta^2}
             \left(\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 \right)
          \right)}
  \\&=   \frac{2\sigma^2}{\pE\left(
             \pderiv{}{\theta} \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2
          \right)}
  \\&=   \frac{2\sigma^2}{\pE\left(
             -2\pderiv{}{\theta}
             \pderiv{\fg(\theta)}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
          \right)}
  \\&=   \frac{-\sigma^2}{\pE\left(
             \pderiv{\fg^2(\theta)}{\theta^2}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
             +
             \pderiv{\fg(\theta)}{\theta}
             \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
          \right)}
   \\&=   \frac{-\sigma^2}{\pE\left(
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
              -N
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           \right)}
   \\&=   \frac{-\sigma^2}{
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN \pE[\fdoty_n-\fg(\theta)]
              -N
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
   \\&=   \frac{-\sigma^2}{
              -N
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
   \\&=   \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}
\end{align*}

The inequality becomes equality (an \prope{efficient} estimate)
if and only if
\[ \estML[u] - \theta =
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t)|s(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\rvy(t)|s(t;\theta)} \right).
\]

\begin{align*}
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t)|s(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\rvy(t)|s(t;\theta)} \right)
     &= \left(
         \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2} \right)
         \left(\frac{-1}{2\sigma^2}(2)\pderiv{\fg(\theta)}{\theta}
         \sum_{n=1}^\xN [\fdoty_n - \fg(\theta) ]\right)
   \\&= -\frac{1}{\xN}
         \frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\sum_{n=1}^\xN [\fdoty_n - \fg(\theta) ] \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n - \fg(\theta) \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\estML[u] - \fg(\theta) \right)
   \\&= -(\estML[u] - \theta)
\end{align*}
\end{proof}





%======================================
\section{Colored noise}
\index{noise!colored}
%======================================
This chapter presented several theorems whose results depended on the
noise being white.
However if the noise is {\bf colored}, then these results are
invalid.
But there is still hope for colored noise.
Processing colored signals can be accomplished using two techniques:
\begin{enume}
   \item Karhunen-Lo\`{e}ve basis functions
      \footnote{{\bf Karhunen-Lo\`{e}ve}: \prefpp{sec:KL}}
   \item whitening filter
\footnote{
   \begin{tabular}[t]{ll}
      \ope{Continuous data whitening}: & \prefp{sec:whiten}  \\
      \ope{Discrete data whitening}:   & \prefp{sec:d-whiten}
   \end{tabular}
   }
\end{enume}

\paragraph{Karhunen-Lo\`{e}ve.}
If the noise is white, the set $\{\inprod{\rvy(t)}{\psi_n(t)}\}$
is a sufficient statistic regardless of which
set $\{\psi_n(t)\}$ of orthonormal basis functions are used.
If the noise is colored, and if $\{\psi_n(t)\}$ satisfy the
Karhunen-Lo\`{e}ve criterion
   \[ \int_{t_2}\Rxx(t_1,t_2)\psi_n(t_2)\dd{t_2} = \lambda_n \psi_n(t_1) \]
then $\{\inprod{\rvy(t)}{\psi_n(t)}\}$ is still a sufficient statistic.

\paragraph{Whitening filter.}
The whitening filter makes the received signal $\rvy(t)$ statistically white
(uncorrelated in time). In this case,
any orthonormal basis set can be used to generate sufficient statistics.




%======================================
\section{Signal matching}
\index{matched filter}
%======================================
\paragraph{Detection methods.}
There are basically two types of detection methods:
\begin{enume}
   \item signal matching
   \item orthonormal decomposition.
\end{enume}

Let $\setS$ be the set of transmitted waveforms and
$\setY$ be a set of orthonormal basis functions that span $\setS$.
\hie{Signal matching} computes the innerproducts of a
received signal $\rvy(t)$ with each signal from $\setS$.
\hie{Orthonormal decomposition} computes the innerproducts of
$\rvy(t)$ with each signal from the set $\setY$.

In the case where $|S|$ is large, often $|R|<<|S|$
making orthonormal decomposition much easier to implement.
For example, in a QAM-64 modulation system,
signal matching requires $|S|=64$ innerproduct calculations,
while orthonormal decomposition only requires $|R|=2$
innerproduct calculations because all 64 signals in $\setS$ can be spanned
by just 2 orthonormal basis functions.

\paragraph{Maximizing SNR.}
\prefpp{thm:sstat} shows that the innerproducts of $\rvy(t)$ with
basis functions of $\setY$ is sufficient for optimal detection.
\prefpp{thm:mf_maxSNR} (next) shows that a receiver can
maximize the SNR of a received signal when signal matching is used.

%--------------------------------------
\begin{theorem}
\label{thm:mf_maxSNR}
%--------------------------------------
Let $\rvx(t)$ be a transmitted signal, $\fn(t)$ noise, and $\rvy(t)$ the received signal
in an AWGN channel.
Let the \hie{signal to noise ratio} SNR be defined as
\\\indentx$\ds
      \snr[\rvy(t)] \eqd \frac{|\inprod{\rvx(t)}{\rvx(t)}|^2}
                            {\pE\brs{|\inprod{\fn(t)}{\rvx(t)}|^2}}.
          $
\thmboxt{
  $\ds\snr[\rvy(t)] \le \frac{2\norm{\rvx(t)}^2}{\xN_o }$
  \qquad
  and is maximized (equality) when $\rvx(t)=a\rvx(t)$, where $a\in\R$.
  }
\end{theorem}

\begin{proof}
\begin{align*}
   \snr[\rvy(t)]
     &\eqd \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\pE\brs{|\inprod{\fn(t)}{\rvx(t)}|^2}}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{f(t)}}^2}
                {\pE\brs{\left[\int_{t\in\R} \fn(t)\rvx^\ast(t)\;dt\right]
                      \left[\int_u n(u)f^\ast(u)\;du\right]^\ast}
                }
   \\&=    \frac{|\inprod{\rvx(t)}{\rvx(t)}|^2}
                {\pE\brs{\int_{t\in\R} \int_u \fn(t)n^\ast(u)\rvx^\ast(t)\rvx(u)\;dtdu}}
   \\&=    \frac{|\inprod{\rvx(t)}{f(t)}|^2}
                {\int_{t\in\R} \int_u \pE\brs{\fn(t)n^\ast(u)}\rvx^\ast(t)\rvx(u)\;dtdu}
   \\&=    \frac{|\inprod{\rvx(t)}{\rvx(t)}|^2}
                {\int_{t\in\R} \int_u \frac{1}{2}\xN_o\delta(t-u) \rvx^\ast(t)\rvx(u)\;dtdu}
   \\&=    \frac{|\inprod{\rvx(t)}{\rvx(t)}|^2}
                {\frac{1}{2}\xN_o \int_{t\in\R} \rvx^\ast(t)\rvx(t)\dt}
   \\&=    \frac{|\inprod{\rvx(t)}{\rvx(t)}|^2}
                {\frac{1}{2}\xN_o \norm{\rvx(t)}^2}
   \\&\le  \frac{|\norm{\rvx(t)}\;\norm{\rvx(t)}|^2}
                {\frac{1}{2}\xN_o \norm{\rvx(t)}^2}
     &&    \text{by \thme{Cauchy-Schwarz Inequality}}
     &&    \text{\ifsxref{vsinprod}{thm:cs}}
   \\&=    \frac{2\norm{\rvx(t)}^2}
                {\xN_o }
\end{align*}
The Cauchy-Schwarz Inequality becomes an equality
($\snr$ is maximized) when $\rvx(t)=a\rvx(t)$.
\end{proof}

\paragraph{Implementation.}
The innerproduct operations can be implemented using either
  \begin{dingautolist}{"C0}
     \item a correlator or
     \item a matched filter.
  \end{dingautolist}

A correlator is simply an integrator of the form
   $\ds\inprod{\rvy(t)}{f(t)} = \int_0^T \rvy(t)f(t)\dt.$

A matched filter introduces a function $\fh(t)$ such that
$\fh(t) =\rvx(T-t)$ (which implies $\rvx(t)=h(T-t)$) giving
  \[
    \mcom{\inprod{\rvy(t)}{\rvx(t)} = \int_0^T \rvy(t)\rvx(t)\dt }
         {correlator}
    =
    \mcom{\left.\int_0^\infty \rvx(\tau)h(t-\tau)\dtau\right|_{t=T}
            = \left.\rvx(t)\conv \fh(t)\right|_{t=T}
         }{matched filter}.
  \]

This shows that $\fh(t)$ is the impulse response of a filter operation
sampled at time $T$. % (see \prefpp{fig:mf}).
By \prefpp{thm:mf_maxSNR}, the optimal impulse response is
$\fh(T-t)=f(t)=\rvx(t)$.
That is, the optimal $\fh(t)$ is just a ``flipped" and shifted version of $\rvx(t)$.

%\begin{figure}[ht] \color{figcolor}
%\centering%
%
%\setlength{\unitlength}{0.15mm}
%\begin{picture}(350,100)
%  \thicklines
%  %\graphpaper[10](0,0)(700,100)
%  \put(   90,  60 ){\makebox( 0,50)[br]{$\rvy(t)=\rvx(t)+\fn(t)$} }
%  \put(    0,  50 ){\vector(1,0){100} }
%
%  \put( 100 ,  00 ){\framebox( 100,100){$\conv \fh(t)$} }
%  \put( 200 ,  50 ){\vector(1,0){ 50} }
%  \put( 250 ,  50 ){\usebox{\picSampler}}
%  \put( 350 ,  50 ){\vector(1,0){100} }
%\end{picture}
%\caption{
%   Matched Filter
%   \label{fig:mf}
%   }
%\end{figure}



%--------------------------------------
\section{Estimation techniques}
%--------------------------------------
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(1000,400)(-150,-300)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put( 325,  50){\framebox(100, 50)[c]{techniques}          }

  \put(  50,   0){\line    (  1,  0)   {750}                 }
 %\put( 375,   0){\circle*             { 10}                 }

  \put(-100,-225){\line    (  1,  0)   {300}                 }
  \put(  50,-225){\circle*             {  5}                 }
  \put(-100,-250){\line    (  0,  1)   { 25}                 }
  \put(  50,-250){\line    (  0,  1)   {150}                 }
  \put( 200,-250){\line    (  0,  1)   { 25}                 }

  \put(  50,- 50){\line    (  0,  1)   { 50}                 }
  \put( 200,- 50){\line    (  0,  1)   { 50}                 }
  \put( 350,- 50){\line    (  0,  1)   { 25}                 }
  \put( 500,- 50){\line    (  0,  1)   { 25}                 }
  \put( 425,- 25){\line    (  0,  1)   { 25}                 }
 %\put( 425,- 25){\circle*             { 10}                 }
  \put( 350,- 25){\line    (  1,  0)   {150}                 }
  \put( 650,- 50){\line    (  0,  1)   { 50}                 }
  \put( 800,- 50){\line    (  0,  1)   { 50}                 }
  \put( 375,   0){\line    (  0,  1)   { 50}                 }

  \put(  50,-150){\line    (  0,  1)   { 50}                 }
  \put( 200,-150){\line    (  0,  1)   { 50}                 }
  \put( 350,-150){\line    (  0,  1)   { 50}                 }
  \put( 500,-150){\line    (  0,  1)   { 50}                 }

  \put( 500,-250){\line    (  0,  1)   { 50}                 }

  \put(  60,- 45){\makebox (100, 40)[lt]{sequential}         }
  \put(  60,- 45){\makebox (100, 40)[lb]{decoding}           }
  \put( 210,- 45){\makebox (100, 35)[lt]{norm}       }
  \put( 210,- 45){\makebox (100, 35)[lb]{minimization}       }
  \put( 435,- 25){\makebox (100, 25)[l]{gradient search}     }
  \put( 360,- 50){\makebox (100, 25)[l]{use $\grad_\vp$ only}}
  \put( 510,- 50){\makebox (100, 25)[l]{$\grad_\vp,\grad^2_\vp$}}
  \put( 660,- 50){\makebox (100, 50)[l]{inner product}       }
  \put( 810,- 50){\makebox (100, 50)[l]{direct search}       }

  \put(   0,-100){\framebox(100, 50)[c]{}                    }
  \put(   0,- 90){\makebox (100, 30)[t]{code}                }
  \put(   0,- 90){\makebox (100, 30)[b]{tree}                }
  \put(   0,-300){\framebox(100, 50)[c]{}                    }

  \put(-150,-295){\makebox (100, 40)[t]{trellis}             }
  \put(-150,-295){\makebox (100, 40)[b]{(Viterbi)}           }
  \put(-150,-300){\framebox(100, 50)[c]{}                    }
  \put(   0,-295){\makebox (100, 40)[t]{Fano}                }
  \put(   0,-295){\makebox (100, 40)[b]{algorithm}           }
  \put(   0,-300){\framebox(100, 50)[c]{}                    }
  \put( 150,-295){\makebox (100, 40)[t]{Stack}               }
  \put( 150,-295){\makebox (100, 40)[b]{algorithm}           }
  \put( 150,-300){\framebox(100, 50)[c]{}                    }

  \put( 150,-100){\framebox(100, 50)[c]{}                    }
  \put( 150,- 90){\makebox (100, 30)[t]{min. mean}           }
  \put( 150,- 90){\makebox (100, 30)[b]{squares}             }
  \put( 150,-200){\framebox(100, 50)[c]{}                    }
  \put( 150,-190){\makebox (100, 30)[t]{least}               }
  \put( 150,-190){\makebox (100, 30)[b]{square}             }

  \put( 300,-100){\framebox(100, 50)[c]{}                    }
  \put( 300,- 90){\makebox (100, 30)[t]{steepest}            }
  \put( 300,- 90){\makebox (100, 30)[b]{descent}             }
  \put( 300,-200){\framebox(100, 50)[c]{}                    }
  \put( 300,-190){\makebox (100, 30)[t]{least mean}          }
  \put( 300,-190){\makebox (100, 30)[b]{square}             }

  \put( 450,-100){\framebox(100, 50)[c]{}                    }
  \put( 450,- 90){\makebox (100, 30)[t]{Newton}              }
  \put( 450,- 90){\makebox (100, 30)[b]{method}              }
  \put( 450,-200){\framebox(100, 50)[c]{Kalman}              }
  \put( 450,-190){\makebox (100, 30)[t]{}                    }
  \put( 450,-190){\makebox (100, 30)[b]{}                    }
  \put( 450,-295){\makebox (100, 40)[t]{recursive}           }
  \put( 450,-300){\framebox(100, 50)[c]{least}               }
  \put( 450,-295){\makebox (100, 40)[b]{square}             }

  \put( 600,-100){\framebox(100, 50)[c]{wavelets}            }
  \put( 600,- 90){\makebox (100, 30)[t]{}                    }
  \put( 600,- 90){\makebox (100, 30)[b]{}                    }

\end{picture}
\caption{
   Estimation techniques
   \label{fig:est-tech}
   }
\end{figure}


%\begin{figure}[ht]
%\center{\epsfig{file=estimate.eps, width=12cm, clip=}}
%%\center{\includegraphics[0,0][4in,4in]{df1.eps}}
%\caption{
%   Estimation techniques
%   \label{fig:estimate}
%   }
%\end{figure}


%--------------------------------------
\subsection{Sequential decoding}
%--------------------------------------
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(550,320)(-100,0)
  %\graphpaper[10](0,0)(300,300)
  \put( -10 , 300 ){\makebox(0,0)[r]{state $00$}}
  \put( -10 , 200 ){\makebox(0,0)[r]{state $01$}}
  \put( -10 , 100 ){\makebox(0,0)[r]{state $10$}}
  \put( -10 ,   0 ){\makebox(0,0)[r]{state $11$}}

  \thicklines
  \put(   0 ,   0 ){\circle*{10}}
  \put(   0 , 100 ){\circle*{10}}
  \put(   0 , 200 ){\circle*{10}}
  \put(   0 , 300 ){\circle*{10}}

\multiput(0,0)(100,0){5}{
  %\thicklines
  \linethickness{1mm}
  \put        (  0,300){\line( 1,-1){100}} % state0 path1
  \put        (  0,200){\line( 1,-1){100}} % state1 path1
  \put        (  0,100){\line( 1, 1){100}} % state2 path1
  \put        (  0,  0){\line( 1, 1){100}} % state3 path1

  %\thicklines
  \linethickness{0.1mm}
  %\put        (  0,300){\line( 1,-3){100}} % state0 path0
  %\put        (  0,200){\line( 1,-2){100}} % state1 path0
  %\put        (  0,100){\line( 1, 2){100}} % state2 path0
  %\put        (  0,  0){\line( 1, 3){100}} % state3 path0

  \qbezier[50](  0,300)(  0,300)(100,  0)  % state0 path0
  \qbezier[50](  0,200)(  0,200)(100,  0)  % state1 path0
  \qbezier[50](  0,100)(  0,100)(100,300)  % state2 path0
  \qbezier[50](  0,  0)(  0,  0)(100,300)  % state3 path0

  \put( 100 ,   0 ){\circle*{10}}
  \put( 100 , 100 ){\circle*{10}}
  \put( 100 , 200 ){\circle*{10}}
  \put( 100 , 300 ){\circle*{10}}
}
\end{picture}
\hspace{1cm}
\begin{tabular}{cl}
   $\cdots$ & $y_n=0$ \\
  ---     & $y_n=1$
\end{tabular}
\caption{
  Viterbi algorithm trellis
   \label{fig:est_trellis}
   }
\end{figure}

It has been shown that the Viterbi algorithm (trellis) produces
an optimal estimate in the maximal likelihood (ML) sense.
A Verterbi trellis is shown in \prefpp{fig:est_trellis}.

%======================================
\subsection{Norm minimization}
%======================================
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(-100,  10){\makebox ( 50,  0)[b]{$x$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{operator} }
  \put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vp$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn$}           }

  \put( 100,  85){\makebox ( 50,  0)[b]{$y$}               }
  \put( 100,  75){\line    (  1,  0)   { 50}               }
  %\put( 100,- 65){\makebox ( 50,  0)[b]{$U\estn$}            }
  \put( 100,- 75){\line    (  1,  0)   { 50}               }
  \put( 150,  75){\line    (  0, -1)   { 50}               }
  \put( 150,- 75){\line    (  0,  1)   { 50}               }
  \put( 150,  25){\vector  (  1,  0)   { 50}               }
  \put( 150, -25){\vector  (  1,  0)   { 50}               }

  \put( 200,- 50){\framebox(100,100)[c]{cost}    }
  \put( 300,  10){\makebox (100,  0)[b]{$C(\estn,x,y)$}      }
  \put( 300,   0){\vector  (  1,  0)   {100}               }
  \put( 400,- 50){\framebox(100,100)[c]{$\grad_\vp$}        }
  \put( 510,  10){\makebox (100,  0)[lb]{$\grad_\vp C(\vp,x,y)$}      }
  \put( 500,   0){\vector  (  1,  0)   {100}               }

\end{picture}
\caption{
   Estimation using gradient of cost function
   \label{fig:est-grad}
   }
\end{figure}

Norm minimization techniques are very powerful
in that an optimum solution can be computed
in one step without iteration or recursion.
In this section we present two types of norm minimization:
\footnote{
   The Least Squares algorithm is nothing new to mathematics.
   It was first developed in 1795 by Gauss who was also the first
   to discover the FFT.
   }

\begin{enume}
  \item minimum mean square estimation (MMSE): \\
        The MMS estimate is a \hie{stochastic} estimate.
        To compute the MMS estimate, the we do not need to know
        the actual data values, but we must know certain system statistics
        which are the
        input data autocorrelation and input/output crosscorrelation.
        The cost function is the expected value of the norm squared error.
   \item least square estimation (LSE): \\
        The LS estimate is a \hie{deterministic} estimate.
        To compute the LS estimate, we must know the actual data values
        (although these may be ``noisy" measurements).
        The cost function is the norm squared error.
\end{enume}

Solutions to both are given in terms of two matrices:

\begin{tabular}{lll}
   $\setY$: Autocorrelation matrix \\
   $W$: Crosscorrelation matrix.
\end{tabular}

%--------------------------------------
\subsubsection{Minimum mean square estimation}
\label{sec:est_mms}
%--------------------------------------
%--------------------------------------
\begin{definition}
\label{def:est_matrices}
%--------------------------------------
Let the following vectors, matrices, and functions
be defined as follows:

\defbox{\begin{tabular}{lcll}
   $\vx  $&$\in$&$ \vCm        $  & data vector                       \\
   $\vy  $&$\in$&$ \vCn        $  & processed data vector             \\
   $\vye $&$\in$&$ \vCn        $  & processed data estimate vector    \\
   $\ve  $&$\in$&$ \vCn        $  & error vector                     \\
   $\vp  $&$\in$&$ \vRm        $  & parameter vector                 \\
   $U    $&$\in$&$ \mCmn       $  & regression matrix                \\
   $R    $&$\in$&$ \mCmm       $  & autocorrelation matrix           \\
   $W    $&$\in$&$ \vCm        $  & cross-correlation vector         \\
   $C    $&$:  $&$ \vRm\to\Rnn $  & cost function
\end{tabular}}
\end{definition}

%--------------------------------------
\begin{theorem}[\thmd{Minimum mean square estimation}]
\label{thm:est_mms}
%--------------------------------------
Let
\begin{align*}
   \vye(\vp)   &\eqd U^H\vp    \\
   \ve(\vp)    &\eqd \vye-\vy \\
   \fCost(\vp) &\eqd \E\norm{\ve}^2 \eqd \Eb{\ve^H\ve} \\
   \estMS      &\eqd \argmin_\vp \fCost(\vp)  \\
   R           &\eqd \Eb{UU^H}   \\
   W           &\eqd \Eb{ U\vy}.
\end{align*}

Then
\thmbox{\begin{array}{rcl}
   \estMS                          &=& (\Real{\setY})^{-1}(\Real{W})  \\
   \fCost(\vp)                     &=& \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \E{\vy^H\vy} \\
   \grad_\vp \fCost(\vp)           &=& 2\Reb{\setY}\vp - 2\Real{W}  \\
   \fCost(\estMS)                  &=& (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy} \\
   \fCost(\estMS)|_{R\mbox{ real}} &=& \E{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W}).
\end{array}}
\end{theorem}
\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{align*}
   \fCost(\vp)
     &\eqd \E \norm{\ve}^2
   \\&=    \Eb{\ve^H\ve}
   \\&=    \Eb{ \left(\vye-\vy\right)^H\left(\vye-\vy\right) }
   \\&=    \Eb{ \left(U^H\vp-\vy\right)^H\left(U^H\vp-\vy\right) }
   \\&=    \Eb{ \left(\vp^HU-\vy^H\right)\left(U^H\vp-\vy\right) }
   \\&=    \Eb{ \vp^HUU^H\vp - \vp^HU\vy -\vy^HU^H\vp + \vy^H\vy }
   \\&=    \vp^H\Eb{UU^H}\vp - \vp^H\Eb{U\vy} -\Eb{\vy^HU^H}\vp + \E{\vy^H\vy}
   \\&=    \vp^H\Eb{UU^H}\vp - (\Eb{U\vy}^H\vp)^H -\Eb{U\vy}^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - (W^H\vp)^H -W^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - 2\Reb{W^H}\vp + \E{\vy^H\vy}
\\
\\
   \grad_\vp \fCost(\vp)
     &= \grad_\vp \left[ \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \E{\vy^H\vy} \right]
   \\&= R\vp + R^T\vp - [(W^H)^\ast]^T - [W^H]^T + 0
   \\&= R\vp + (R^H)^\ast\vp - W - W^\ast
   \\&= R\vp + R^\ast\vp - W - W^\ast
   \\&= (R + R^\ast)\vp - (W + W^\ast)
   \\&= 2(\Real{\setY})\vp - 2\Real{W}
\\
\\
   \vpo
     &= (\Real{\setY})^{-1}(\Real{W})
\\
\\
   \fCost(\vpo)
     &=    \vpo^H R \vpo - 2\Reb{W^H}\vpo + \E{\vy^H\vy}
   \\&=    [(\Real{\setY})^{-1}(\Real{W})]^H R [(\Real{\setY})^{-1}(\Real{W})] - 2\Reb{W^H}[(\Real{\setY})^{-1}(\Real{W})] + \E{\vy^H\vy}
   \\&=    (\Real{W^H})(\Real{\setY})^{-H} R (\Real{\setY})^{-1}(\Real{W}) - 2\Reb{W^H}(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    (\Real{W^H})(\Real{R^H})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2\Reb{W^H}(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy}
\\
\\
   \fCost(\vpo)|_{R\mbox{ real}}
     &=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    (\Real{W^H})R^{-1} R R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    (\Real{W^H}) R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \E{\vy^H\vy}
   \\&=    \E{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W})
\end{align*}
\end{proof}




\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  \put(-100,  10){\makebox ( 50,  0)[b]{$\vx$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{process} }
  %\put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vp$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn=R^{-1}W$}           }

  \put( 110,  85){\makebox ( 50,  0)[lb]{$y$}               }
  \put( 110, -65){\makebox ( 50,  0)[lb]{$\hat{y}=\vx^T\vp$}               }
  \put( 100,  75){\vector  (  1,  0)   {150}               }
  \put( 100,- 75){\vector  (  1,  0)   {150}               }
  \put( 250,   0){\circle{40} }
  \put( 250,   0){\makebox(0,0)[c]{$+$} }
  \put( 250,  75){\vector  (  0, -1)   { 60}               }
  \put( 250, -75){\vector  (  0,  1)   { 60}               }

  \put( 300,  10){\makebox (100,  0)[b]{$e=\hat{y}-y$}      }
  \put( 270,   0){\vector  (  1,  0)   {80}               }
  \put( 350,   0){\vector  (  0, -1)   {225}               }
  \put( 350,-225){\vector  ( -1,  0)   {250}               }
\end{picture}
\caption{
   Adative filter example
   \label{fig:est_adapt}
   }
\end{figure}

In many adaptive filter and equalization applications,
the autocorrelation matrix $U$ is simply the $m$-element
random data vector $\vx(k)$ at time $k$, as in the \thme{Wiener-Hopf equations} (next).
%--------------------------------------
\begin{corollary}[\thmd{Wiener-Hopf equations}]
\footnote{
  \citerppgc{ifeachor1993}{547}{549}{020154413X}{\textsection ``9.3 Basic Wiener filter theory"},
  \citerppgc{ifeachor2002}{651}{654}{0201596199}{\textsection ``10.3 Basic Wiener filter theory"}   % TODO: check 654
  }
%--------------------------------------
\corbox{
  \brb{
   U \eqd \vx(k) \eqd
   \brs{\begin{array}{l}
      x(k) \\
      x(k-1) \\
      x(k-2) \\
      \vdots \\
      x(k-m+1)
   \end{array}}}
  \implies
  \brb{\begin{array}{rcl}
    \estMS                          &=& R^{-1}W  \\
    \fCost(\estMS)                  &=& W^T R^{-1} R R^{-1}W - 2W^T R^{-1}W + \E{\vy^T\vy} \\
  \end{array}}
  }
\end{corollary}
\begin{proof}
This is a special case of the more general case discussed
in \prefpp{thm:est_mms}.
Here, the dimension of $U$ is $m\times1$ (n=1).
As a result,
$\vy$, $\vye$, and $\ve$ are simply scalar quantities (not vectors).
In this special case, we have the following results
\xref{fig:est_adapt}:

\begin{align*}
   \hat{y}(\vp)   &\eqd \vx^T \vp    \\
   e(\vp)    &\eqd \hat{y}-y \\
   \fCost(\vp) &\eqd \E\norm{\ve}^2 \eqd \Eb{e^2} \\
   \estMS      &\eqd \argmin_\vp \fCost(\vp)  \\
   R           &\eqd \Eb{\vx\vx^T}   \\
   W           &\eqd \Eb{ \vx y }    \\
    \fCost(\vp)                     &= \vp^T R \vp -2W^T\vp  + \Eb{\vy^T\vy} \\
    \grad_\vp \fCost(\vp)           &= 2R\vp - 2W  \\
    \fCost(\estMS)|_{R\mbox{ real}} &=    \E{\vy^T\vy} - W^T R^{-1}W.
\end{align*}
\end{proof}

%======================================
\subsubsection{Least squares}
\label{sec:ls}
%======================================
%--------------------------------------
\begin{theorem}[\thmd{Least squares}]
\label{thm:ls}
\index{least squares}
%--------------------------------------
Let
\begin{align*}
   \vye(\vp)   &\eqd U^H\vp                      \\
   \ve(\vp)    &\eqd \vye-\vy                    \\
   \fCost(\vp) &\eqd \norm{\ve}^2 \eqd \ve^H\ve  \\
   \estLS      &\eqd \argmin_\vp \fCost(\vp)    \\
   R           &\eqd UU^H                        \\
   W           &\eqd U\vy.
\end{align*}

Then
\thmbox{\begin{array}{rcl}
   \estLS                          &=& (\Real{\setY})^{-1}(\Real{W})  \\
   \fCost(\vp)                     &=& \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \E{\vy^H\vy} \\
   \grad_\vp \fCost(\vp)           &=& 2\Reb{\setY}\vp - 2\Real{W}  \\
   \fCost(\estLS)                  &=& (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \E{\vy^H\vy} \\
   \fCost(\estLS)|_{R\mbox{ real}} &=& \E{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W}).
\end{array}}
\end{theorem}

\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{align*}
   \fCost(\vp)
     &\eqd \norm{\ve}^2
   \\&=    e^H\ve
   \\&=    \left(\vye-\vy\right)^H\left(\vye-\vy\right)
   \\&=    \left(U^H\vp-\vy\right)^H\left(U^H\vp-\vy\right)
   \\&=    \left(\vp^HU-\vy^H\right)\left(U^H\vp-\vy\right)
   \\&=    \vp^HUU^H\vp - \vp^HU\vy -\vy^HU^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - (W^H\vp)^H -W^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - 2\Reb{W^H}\vp + \vy^H\vy
\\
\\
   \grad_\vp \fCost(\vp)
     &= \grad_\vp \left[ \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \vy^H\vy \right]
   \\&= R\vp + R^T\vp - [(W^H)^\ast]^T - [W^H]^T + 0
   \\&= R\vp + (R^H)^\ast\vp - W - W^\ast
   \\&= R\vp + R^\ast\vp - W - W^\ast
   \\&= (R + R^\ast)\vp - (W + W^\ast)
   \\&= 2(\Real{\setY})\vp - 2\Real{W}
\\
\\
   \vpo
     &= (\Real{\setY})^{-1}(\Real{W})
\\
\\
   \fCost(\vpo)
     &=    \vpo^H R \vpo - 2\Reb{W^H}\vpo + \vy^H\vy
   \\&=    [(\Real{\setY})^{-1}(\Real{W})]^H R [(\Real{\setY})^{-1}(\Real{W})] - 2\Reb{W^H}[(\Real{\setY})^{-1}(\Real{W})] + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{\setY})^{-H} R (\Real{\setY})^{-1}(\Real{W})     - 2\Reb{W^H}(\Real{\setY})^{-1}(\Real{W})       + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{R^H})^{-1} R (\Real{\setY})^{-1}(\Real{W})   - 2\Reb{W^H}(\Real{\setY})^{-1}(\Real{W})     + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W})     - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W})      + \vy^H\vy
\\
\\
   \fCost(\vpo)|_{R\mbox{ real}}
     &=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \vy^H\vy
   \\&=    (\Real{W^H})R^{-1} R R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \vy^H\vy
   \\&=    (\Real{W^H}) R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \vy^H\vy
   \\&=    \vy^H\vy - (\Real{W^H})R^{-1}(\Real{W})
\end{align*}
\end{proof}


%---------------------------------------
\begin{example}[Polynomial approximation]
\index{polynomial approximation}
\index{least squares}
\index{Vandermonde matrix}
%---------------------------------------
\mbox{}\\
Suppose we {\bf know} the locations
$\set{(x_n,y_n)}{n=1,2,3,4,5}$ of 5 data points.
Let $\vx$ and $\vy$ represent the locations of these points such that
\[
   \vx \eqd
   \left[\begin{array}{l}
      x_1  \\
      x_2  \\
      x_3  \\
      x_4  \\
      x_5
   \end{array}\right]
   \qquad\qquad
   \vy \eqd
   \left[\begin{array}{l}
      y_1  \\
      y_2  \\
      y_3  \\
      y_4  \\
      y_5
   \end{array}\right]
\]
Suppose we want to find a second order polynomial
  \[ c x^2 + bx + a \]
that best approximates
these 5 points in the least squares sense.
We define the matrix $U$ (known) and vector $\estn$ (to be computed)
as follows:
\[
   U^H \eqd
   \mcom{
   \left[\begin{array}{lll}
      1  & x_1 & x_1^2  \\
      1  & x_2 & x_2^2  \\
      1  & x_3 & x_3^2  \\
      1  & x_4 & x_4^2  \\
      1  & x_5 & x_5^2
   \end{array}\right]
   }{Vandermonde matrix \footnotemark}
   \addtocounter{footnote}{-1}\footnote{\citer{horn}{29}}
   \qquad\qquad
   \estn \eqd
   \left[\begin{array}{l}
      a  \\
      b  \\
      c  \\
   \end{array}\right]
\]
Then, using \prefpp{thm:ls}, the best coefficients $\estn$
for the polynomial are
\begin{align*}
  \estn
    &= \left[\begin{array}{l}
          a  \\
          b  \\
          c  \\
       \end{array}\right]
  \\&= R^{-1}W
  \\&= (UU^H)^{-1}\; (U\vy)
  \\&= \left(
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]^H
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]
       \right)^{-1}
       \left(
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]^H
       \left[\begin{array}{l}
          y_1  \\
          y_2  \\
          y_3  \\
          y_4  \\
          y_5
       \end{array}\right]
       \right)
\end{align*}
\end{example}

%======================================
\subsection{Gradient search techniques}
%======================================
One of the biggest advantages of using a gradient search technique is
that they can be implemented \hie{recursively} as shown in the next equation.
The general form of the gradient search parameter estimation techniques is\footnote{\citerp{nelles2001}{90}}
\thmbox{
   \vp_n = \vp_{n-1} - \eta_{n-1} R \;\left[\gradxy{\vp}{\fCost(\vp_n)}\right]
}
where at time $n$

\begin{tabular}{lll}
   $\vp_n$      & is the \hie{state    }   & (vector)  \\
   $\eta_n$     & is the \hie{step size}   & (scalar)  \\
   $\setY$          & is the \hie{direction}   & (matrix)  \\
   $\gradxy{\vp}{\fCost(\vp_n)}$      & is the \hie{gradient } of the cost function $\fCost(\vp_n)$   & (vector)
\end{tabular}

Two major categories of gradient search techniques are
\begin{liste}
   \item steepest descent (includes LMS)
   \item Newton's method (includes RLS and Kalman filters).
\end{liste}

The key difference between the two is that
{\bf \hie{steepest descent} uses only first derivative information},
while
{\bf \hie{Newton's method} uses both first and second derivative information}
making it converge much faster but with significantly higher
complexity.

%======================================
\subsubsection*{First derivative techniques}
\label{sec:1st-deriv}
%======================================
\paragraph{Steepest descent.}
In this algorithm, $R=I$ (identity matrix).
First derivative information is contained in $\grad\fCost$.
Second derivative information, if present, is contained in $\setY$.
Thus, steepest descent algorithms do not use second derivative information.
\thmbox{
  \vp_n = \vp_{n-1} - \eta_{n-1} \;\left[ \gradxy{\vp}{\fCost(\vp_n)} \right]
}
\paragraph{Least Mean Squares (LMS).}\footnote{\citerp{mik}{526}}
This is a special case of \hie{steepest descent}.
In minimum mean square estimation \xref{sec:est_mms},
the cost function $\fCost(\vp)$ is defined as a
\hie{statistical average} of the error vector such that
$\fCost(\vp) = \Eb{\ve^H\ve}$.
In this case the gradient $\grad\fCost$ is difficult to compute.
However, the LMS algorithm greatly simplifies the problem by
instead defining the cost function as a function of the
\hie{instantaneous error} such that
\begin{align*}
   \vy &= y(n)
\\
   \vye &= \hat{y}(n)
\\
   \fCost(\vp)
   &= \norm{e(n)}^2
 \\&= e^2(n)
 \\&= (\hat{y}(n)-y(n))^2
\end{align*}

Computing the gradient of this cost function is then
just a special case of \hie{least squares estimation} \xref{sec:ls}.
Using LS, we let $U=\vx^T$ and hence
\begin{align*}
   \gradxy{\vp}{\fCost(\vp)}
   &= 2U^TU \vp -2U^T\vy                   && \text{ by \prefp{thm:ls}}
\\ &= 2\vx\vx^T \vp -2\vx y               && \text{ by above definitions}
\\ &= 2\vx\hat{y} -2\vx y                    && \text{ }
\\ &= 2\vx(\hat{y} -y)                      && \text{ }
\\ &= 2\vx e(n)                && \text{ }
\end{align*}

The LMS algorithm uses this instantaneous gradient for $\grad\fCost$,
lets $R=I$, and uses a constant step size $\eta$ to give
\thmbox{
  \vp_n = \vp_{n-1} - 2\eta \vx_n e(n)
}
%--------------------------------------
\subsubsection*{Second derivative techniques}
%--------------------------------------
\paragraph{Newton's Method.}
This algorithm uses the \hie{Hessian} matrix $H$,
which is the second derivative of the cost function $\fCost(\vp)$,
and lets $R=H^{-1}$.
\begin{align*}
   H_n &\eqd& \grad_\vp\grad_\vp \fCost(\vp_n)
\\
   \vp_n &= \vp_{n-1} - \eta_{n-1} H_n^{-1} \left[\grad_\vp\fCost(\vp_n)\right]
\end{align*}


\paragraph{Kalman filtering}\footnote{\citerp{nelles2001}{66}}
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}

\if 0
\paragraph{RLS with forgetting}\footnote{\citerp{nelles2001}{64}}
This algorithm introduces a forgetting factor $\lambda$
to help the algorithm track non-stationary channels.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+\lambda}  P(k-1)x(k) \\
   P(k) &= \frac{1}{\lambda}(I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}
\fi

\paragraph{Recursive Least Squares (RLS)}\footnote{\citerp{nelles2001}{66}}
This algorithm is a special case of either the RLS with forgetting
or the Kalman filter.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}  P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1) \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}




%--------------------------------------
\subsection{Direct search}
%--------------------------------------
A direct search algorithm may be used in cases where the cost
function over $\vp$ has several local minimums, making convergence difficult.
Furthermore, direct search algorithms can be very computationally demanding.

