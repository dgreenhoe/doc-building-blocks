%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%=======================================
\chapter[Projection Statistic Algorithms]
        {Projection Statistic Algorithms for Additive Noise Models}
%=======================================
%=======================================
\section{Projection Statistics}
%=======================================
\prefpp{thm:sstat} (next) shows that the finite set
$\setY\eqd\set{\fdoty_n}{n=1,2,\ldots,\xN}$ (a finite number of values) provides just as
good an estimate as having the entire $\rvy(t;\theta)$ waveform
(an uncountably infinite number of values)
with respect to the following cases:
\begin{enume}
   \item the conditional probability of $\rvx(t;\theta)$ given $\rvy(t;\theta)$
   \item the \fncte{MAP estimate} of the sequence
   \item the \fncte{ML estimate}  of the sequence.
\end{enume}
Thus even with a drastic reduction in the number of statistics
from uncountably infinite to finite $\xN$,
no quality is lost with respect to the estimators listed above---that
is, these statistics are \prope{sufficient}, or are ``\prope{sufficient statistics}".
This amazing result is very useful in practical system implementation
and also for proving other theoretical results
(notably estimation and detection theorems).

The concept of sufficient statistics in ``estimation theory" is very similar 
to the concept of \ope{feature extraction}\footnote{
  \citerpc{bishop2006}{2}{9780387310732}{{\scshape``1. Introduction"}}
  }
in ``machine learning".
Thus, estimation theory \emph{is} a form of machine learning.
%But first, some definitions (next) that are used repeatedly in this chapter.

%---------------------------------------
\begin{definition}
\label{def:sstat}
%---------------------------------------
Let $\Psi\eqd\set{\fpsi_n}{n=1,2,\ldots,\xN}$ be an \structe{orthonormal basis}
for a parameterized function $\rvx(t;\theta)$ with parameter $\theta$.
Let $\fy(t;\theta)$ be $\fx(t;\theta)$ plus a \fncte{random process} $\rvv(t)$ such that
\\\indentx$\ds
  \rvy(t;\theta)\eqd\rvx(t;\theta)+\rvv(t)
$\\
Let $\fdoty_n$, $\fdotx_n$, and $\fdotv_n$ be \ope{projections}\ifsxref{operator}{def:opP}
onto the \fncte{basis vector} $\fpsi_n(t)$ such that
\defbox{\begin{array}{r c>{\ds}l c>{\ds}l c>{\ds}l D}
    \fdoty_n(\theta) &\eqd& \opP_n\fy(t;\theta) &\eqd& \inprod{\fy(t;\theta)}{\fpsi_n(t)} &\eqd& \int_{t\in\R}\fy(t;\theta)\fpsi_n(t)\dt
  \\\fdotx_n(\theta) &\eqd& \opP_n\fx(t)        &\eqd& \inprod{\fx(t;\theta)}{\fpsi_n(t)} &\eqd& \int_{t\in\R}\fx(t;\theta)\fpsi_n(t)\dt
  \\\fdotv_n         &\eqd& \opP_n\fv(t)        &\eqd& \inprod{\fv(t)}       {\fpsi_n(t)} &\eqd& \int_{t\in\R}\fv(t)       \fpsi_n(t)\dt
\end{array}}
\\
Let the set $\setY$ be defined as $\setY\eqd\set{\fdoty_n(\theta)}{1,2,\ldots,\xN}$
Let $\estMAP$ be the \fncte{MAP estimate}
and $\estML$  be the \fncte{ML estimate} \xref{def:estML} of $\theta$.
\end{definition}

%---------------------------------------
\begin{lemma}
\label{lem:fdotv_pE}
%---------------------------------------
Let $\Psi$, $\rvv(t)$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\lembox{
  \brb{\begin{array}{rclM}
    \pE\fv(t) &=& 0 & (\prope{zero-mean})
  \end{array}}
  \implies
  \brb{\begin{array}{rclM}
  \pE\fdotv_{n} &=& 0 & (\prope{zero-mean})
  \end{array}}
  }
\end{lemma}
\begin{proof}
\begin{align*}
  \pE\fdotv_{n}
    &= \pE\inprod{\fv(t)}{\fpsi_n(t)}
    && \text{by definition of $\fdotv_n$}
    && \text{\xref{def:sstat}}
  \\&= \inprod{\pE\fv(t)}{\fpsi_n(t)}
    && \text{by \prope{linearity} of $\inprodn$}
  \\&= \inprod{0}{\fpsi_n(t)}
    && \text{by \prope{zero-mean} hypothesis}
  \\&= 0
\end{align*}
\end{proof}

%---------------------------------------
\begin{lemma}
\label{lem:fdotv_N}
%---------------------------------------
Let $\Psi$, $\rvv(t)$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\lembox{
  \brb{\begin{array}{rclD}
    \fv(t) &\sim& \pN{0}{\sigma^2}& (\prope{Gaussian})
  \end{array}}
  \implies
  \brb{\begin{array}{rclD}
    \fdotv_n &\sim& \pN{0}{\sigma^2} & (\prope{Gaussian})
  \end{array}}
}
\end{lemma}
\begin{proof}
  The distribution follows because it is a linear operation on a Gaussian process.
\end{proof}

%---------------------------------------
\begin{lemma}
\label{lem:fdotv_white}
%---------------------------------------
Let $\Psi$, $\rvy(t;\theta)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\lembox{
  \brb{\begin{array}{FrclD}
    (A). & \pE\brs{\fv(t)}           &=        & 0                   & and % (\prope{zero-mean})
  \\(B). & \mc{3}{l}{\cov{\fv(t)}{\fv(u)} = \sigma^2\delta(t-u)} %& (\propb{white})
  \end{array}}
  \implies
  \brb{\begin{array}{FlclD}
       (1). & \pE\fdotv_n              &=& 0                      & (\prope{zero-mean})    %& and
     \\(2). & \cov{\fdotv_n}{\fdotv_m} &=& \sigma^2 \kdelta_{n-m} & (\prope{uncorrelated}) %& and
  \end{array}}
  }
\end{lemma}
\begin{proof}
\begin{enumerate}
  \item %Because the noise is \prope{additive} (hypothesis A)\ldots
    \begin{align*}
         \pE\fdotv_{n}            &= 0                            && \text{by \prope{additive} property and \prefp{thm:an_stats}}
    \end{align*}

  \item
    \begin{align*}
     \cov{\fdotv_m}{\fdotv_n}
        &= \cov{\inprod{\fv(t)}{\fpsi_m(t)}}{\inprod{\fv(t)}{\fpsi_n(t)}}
        && \text{by def. of $\fdotv_n$}
        && \text{\xref{def:sstat}}
      \\&= \cov{\brp{\int_{t\in\R} \fv(t) \fpsi_m(t)\dt}}
               {\brp{\int_{u\in\R} \fv(u) \fpsi_n(u)\du}}
        && \text{by def. of $\inprodn$}
        && \text{\xref{def:sstat}}
      \\&= \pE\brs{\brp{\int_{t\in\R} \fv(t) \fpsi_m(t)\dt}
                   \brp{\int_{u\in\R} \fv(u) \fpsi_n(u)\du}}
        && \text{by def. of $\pCov$}
      \\&= \pE\brs{\int_{t\in\R}\int_{u\in\R} \fv(t)\fv(u) \fpsi_m(t) \fpsi_n(u) \dt\du}
      \\&= \int_{t\in\R}\int_{u\in\R} \pE\brs{\fv(t)\fv(u)} \fpsi_m(t) \fpsi_n(u) \dt\du
      \\&= \int_{t\in\R}\int_{u\in\R} \sigma^2\delta(t-u) \fpsi_m(t) \fpsi_n(u) \dt\du
        && \text{by \prope{white} hyp.}
        && \text{(B)}
      \\&= \sigma^2 \int_{t\in\R}  \fpsi_m(t) \fpsi_n(u) \dt
      \\&= \sigma^2 \inprod{\fpsi_m(t)}{\fpsi_n(u)}
        && \text{by def. of $\inprodn$}
        && \text{\xref{def:sstat}}
      \\&= \brbl{\begin{array}{lM}
               \sigma^2 & for $n=m$ \\
               0        & for $n\ne m$.
            \end{array}}
        && \text{by \prope{orthonormal} prop.}
        && \text{\xref{def:sstat}}
    \end{align*}

\end{enumerate}
\end{proof}


%---------------------------------------
\begin{lemma}
\label{lem:fdotv_cov}
%---------------------------------------
Let $\Psi$, $\rvy(t;\theta)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\lembox{
  \brbr{\begin{array}{FrclD}
    (A). & \cov{\fv(t)}{\fv(u)}      &=        & \sigma^2\delta(t-u) & and %& (\propb{white})                 & and
  \\(B). & \fv(t)                    &\sim     & \pN{0}{\sigma^2}    & and %& (\propb{Gaussian})              & and
  \\(C). & \inprod{\fpsi_n}{\fpsi_m} &=        & \kdelta_{mn}        &     %& (\prope{orthonormal})
  \end{array}}
  \implies
  \brbl{\begin{array}{FrclD}
       (1). & \fdotv_n                            &\sim& \pN{0}{\sigma^2}                     & (\prope{Gaussian})
     \\(2). & \cov{\fdotv_n}{\fdotv_m}            &=   & \sigma^2 \kdelta_{nm}                & (\prope{uncorrelated})
     \\(3). & \psP\setn{\fdotv_n \land \fdotv_m} &=   & \psP\setn{\fdotv_n}\psP\setn{\fdotv_m}   & (\prope{independent})
  \end{array}}
  }
\end{lemma}
\begin{proof}
\begin{enumerate}
  \item Because the operations are \prope{linear} on processes are \prope{Gaussian} (hypothesis C).

  \item
    \begin{align*}
       \pE{\fdotv_{n}}           &= 0                      && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
     \\\cov{\fdotv_m}{\fdotv_n}  &= \sigma^2 \kdelta_{mn}  && \text{by \prope{AWN} properties and \prefp{lem:fdotv_white}}
    \end{align*}

  \item Because the processes are \prope{Gaussian},
        \prope{uncorrelated} implies \prope{independent}.
\end{enumerate}
\end{proof}

%=======================================
\section{Sufficient Statistics}
%=======================================
%---------------------------------------
\begin{theorem}[\thmd{Sufficient Statistic Theorem}]
\footnote{
  \citePpc{fisher1922}{316}{``Criterion of Sufficiency"}
  }
\label{thm:sstat}
\index{optimal receiver}
\index{MAP} \index{maximum a-posteriori probability estimation}
\index{ML}  \index{maximum likelihood estimation}
%---------------------------------------
Let $\Psi$, $\rvy(t;\theta)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
Let $\estMAP$ be the \fncte{MAP estimate}
and $\estML$  be the \fncte{ML estimate} \xref{def:estML} of $\theta$.
\thmbox{
  \brb{\begin{array}{FMD}
     (A). & $\rvv(t)$ is \prope{zero-mean}                       & and
   \\(B). & $\rvv(t)$ is \prope{white}                           & and
   \\(C). & $\rvv(t)$ is \prope{Gaussian}                        &
  \end{array}}
  \implies
  \mcom{\brb{\begin{array}{F rc>{\ds}l D}
     (1). & \mc{3}{l}{\psP\set{ \rvx(t;\theta)}{\rvy(t;\theta)} = \psP\set{\rvx(t;\theta)}{\setY}}        & and
   \\(2). & \estMAP                           &=& \argmax_{\estT} \psP\set{\rvx(t;\theta)}{\setY} & and
   \\(3). & \estML                            &=& \argmax_{\estT} \psP\set{\setY}{\rvx(t;\theta)} &
  \end{array}}}{the $\xN$ element set $\setY$ is a \prope{sufficient statistic} for estimating $\fx(t;\theta)$}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item definition: Let $\ds\rvv'(t) \eqd \rvv(t) - \sum_{n=1}^\xN \fdotv_n \fpsi_n(t)$.
        \label{idef:sstat_vprime}

  \item lemma: The relationship between $\setY$ and $\rvv'(t)$ is given by
        \label{ilem:sstat_Yvprime}
  \begin{align*}
     &\boxed{\rvy(t;\theta)}
     \\&= \sum_{n=1}^\xN \inprod{\rvy(t;\theta)}{\fpsi_n(t)}\fpsi_n(t) +
          \brs{\rvy(t;\theta)- \sum_{n=1}^\xN \inprod{\rvy(t;\theta)}{\fpsi_n(t)}\fpsi_n(t) }
       && \begin{array}{@{}M}%
            by \prope{additive identity} property\\
            of $\fieldC$%
          \end{array}
     \\&\eqd \sum_{n=1}^\xN \inprod{\rvy(t;\theta)}{\fpsi_n(t)}\fpsi_n(t) +
          \brs{\rvy(t;\theta)- \sum_{n=1}^\xN \inprod{\rvx(t)+\rvv(t)}{\fpsi_n(t)}\fpsi_n(t) }
       && \text{by definition of $\fy(t;\theta)$}
     \\&= \sum_{n=1}^\xN \fdoty_n\fpsi_n(t) +
          \mcom{\rvx(t)+\rvv(t)}{$\fy(t;\theta)$}
             - \mcom{\sum_{n=1}^\xN \inprod{\rvx(t)}{\fpsi_n(t)}\fpsi_n(t)}{$\fx(t)$}
             - \mcom{\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)}{$\rvv(t)-\rvv'(t)$}
       && \begin{array}{@{}M}
            by definition of $\fdoty_n$ and\\
            \prope{additive} property of $\inprodn$\\
            \xref{def:inprod}
          \end{array}
     \\&= \sum_{n=1}^\xN \fdoty_n\fpsi_n(t) +
          \rvx(t)+\rvv(t) - \rvx(t) - \brs{\rvv(t) - \rvv'(t)}
     \\&= \boxed{\sum_{n=1}^\xN \fdoty_n\fpsi_n(t) + \rvv'(t)}
  \end{align*}

  \item lemma: $\pE\brs{\fdotv_n\rvv(t)} = N_o\fpsi_n(t)$. Proof: \label{ilem:sstat_vdotv}
    \begin{align*}
      &\pE\brs{\fdotv_n\rvv(t)}
      \\&\eqd \pE\brs{\brp{\int_{t\in\R}\rvv(u)\fpsi_n(u)\du} \rvv(t)}
        && \text{by definition of $\fdotv_n(t)$}
        && \text{\xref{def:sstat}}
      \\&= \pE\brs{\int_{t\in\R}\rvv(u)\rvv(t)\fpsi_n(u)\du}
        && \text{by \prope{linearity} of $\int\du$ operator}
      \\&= \int_{t\in\R}\pE\brs{\rvv(u)\rvv(t)}\fpsi_n(u)\du
        && \text{by \prope{linearity} of $\pE$}
        && \text{\xref{thm:pE_linop}}
      \\&= \int_{t\in\R}N_o\delta(u-t)\fpsi_n(u)\du
        && \text{by \prope{white} hypothesis}
      \\&= N_o\fpsi_n(t)
        && \text{by property of \fncte{Dirac delta} $\delta(t)$}
    \end{align*}

  \item lemma: $\setY$ and $\rvv'(t)$ are \prope{uncorrelated}:\label{ilem:sstat_uncorrelated} Proof:
  \begin{align*}
     &\pE\brs{\fdoty_n \rvv'(t)}
     \\&\eqd \pE\brs{\inprod{\rvy(t;\theta)}{\fpsi_n(t)}\brp{ \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)}}
       && \text{by definitions of $\fdoty_n$ and $\rvv'(t)$}
     \\&\eqd \pE\brs{\inprod{\rvx(t)+\rvv(t)}{\fpsi_n(t)}\left( \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)\right)}
       && \text{by definition of $\fy(t;\theta)$}
     \\&= \pE\brs{\Bigg(\inprod{\rvx(t)}{\fpsi_n(t)}+\inprod{\rvv(t)}{\fpsi_n(t)}\Bigg)
              \brp{ \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)}}
       && \begin{array}{@{}M}%
            by \prope{additive} property of\\
            $\inprodn$ \xref{def:inprod}%
          \end{array}
     \\&= \pE\brs{\Bigg(\fdotx_n+\fdotv_n\Bigg)
              \left( \rvv(t)-\sum_{n=1}^\xN \fdotv_n\fpsi_n(t)\right)}
       && \begin{array}{@{}M}%
            by definitions of $\fdotx_n$ and $\fdotv_n$\\
            \xref{def:sstat}
          \end{array}
     \\&= \pE\brs{\fdotx_n \rvv(t) - \fdotx_n \sum_{n=1}^\xN \fdotv_n\fpsi_n(t)
              +\fdotv_n \rvv(t) - \fdotv_n \sum_{n=1}^\xN \fdotv_n\fpsi_n(t) }
     \\&= \pE\brs{\fdotx_n \rvv(t)}
          - \pE\brs{\fdotx_n \sum_{n=1}^\xN \fdotv_n\fpsi_n(t)}
          + \pE\brs{\fdotv_n\rvv(t)}
          - \pE\brs{\sum_{m=1}^\xN \fdotv_n \fdotv_m\fpsi_m(t)}
       && \begin{array}{@{}M}%
            by \prope{linearity} of $\pE$\\%
            \xref{thm:pE_linop}%
          \end{array}
     \\&= \fdotx_n \cancelto{0}{\pE{\rvv(t)}}
          - \fdotx_n \sum_{n=1}^\xN \cancelto{0}{\pE\brs{\fdotv_n}}\fpsi_n(t)
          + \pE\brs{\fdotv_n\rvv(t)}
          - \sum_{m=1}^\xN \pE\brs{\fdotv_n \fdotv_m}\fpsi_m(t)
       && \begin{array}{@{}M}%
            by \prope{linearity} of $\pE$\\%
            \xref{thm:pE_linop}%
          \end{array}
     \\&= 0 - 0
          + \pE\brs{\fdotv_n\rvv(t)}
          - \sum_{m=1}^\xN N_o\kdelta_{mn} \fpsi_m(t)
       && \text{by \prope{white} hypothesis}
     \\&= N_o\fpsi_n(t) - N_o\fpsi_n(t)
       && \text{by \pref{ilem:sstat_vdotv}}
     \\&= 0
     \\&\implies\text{\propb{uncorrelated}}
  \end{align*}

  \item lemma: $\setY$ and $\rvv'(t)$ are \prope{independent}. Proof:
        By \pref{ilem:sstat_uncorrelated}, $\fdoty_n$ and $\rvv'(t)$ are \prope{uncorrelated}.
        By hypothesis, they are \prope{Gaussian}, and thus are also \propb{independent}.
        \label{ilem:sstat_independent}

  \item Proof that $\psP\set{\rvx(t;\theta)}{\rvy(t;\theta)}=\psP\set{\rvx(t;\theta)}{\fdoty_1,\;\fdoty_2,\ldots,\fdoty_{\xN}}$:
        \label{item:sstat_P}

  \begin{align*}
     \psP\set{\rvx(t;\theta)}{\rvy(t;\theta)}
       &= \psP\set{\rvx(t;\theta)}{\sum_{n=1}^\xN\fdoty_n \fpsi_n(t) + \rvv'(t)}
     \\&= \psP\set{\rvx(t;\theta)}{\setY, \rvv'(t)}
       && \begin{array}{M}
            because $\setY$ and $\rvv'(t)$ can be\\
            extracted by $\inprod{\cdots}{\fpsi_n(t)}$
          \end{array}
     \\&= \frac{\psP\set{\setY, \rvv'(t)}{\rvx(t;\theta)}  P\setn{\rvx(t;\theta)} }
               {\psP\setn{\setY,\rvv'(t)}}
     \\&= \frac{\psP\set{ \setY}{\rvx(t;\theta)}\psP\set{ \rvv'(t)}{\rvx(t;\theta)}\psP\setn{\rvx(t;\theta)}}
               {\psP\setn{\setY}\psP\setn{\rvv'(t)}}
       && \text{by \prope{independence} of $\setY$ and $\rvv'(t)$ \xref{ilem:sstat_independent}}
     \\&= \frac{\psP\set{ \setY}{\rvx(t;\theta)}\psP\setn{ \rvv'(t)}\psP\setn{\rvx(t;\theta)}}
               {\psP\setn{\setY}\psP\setn{\rvv'(t)}}
       && \text{by \prope{independence} of $\fx$ and $\rvv$}
     \\&= \frac{\psP\set{\setY}{\rvx(t;\theta)} \psP\setn{\rvx(t;\theta)}}
               {\psP\setn{\setY}}
     \\&= \frac{\psP\setn{ \setY,\rvx(t;\theta)}}
               {\psP\setn{\setY}}
     \\&= \psP\set{\rvx(t;\theta)}{\setY}
       && \begin{array}{@{}M}%
            by definition of \fncte{conditional probability}\\
            \xref{def:conprob}
          \end{array}
  \end{align*}

  \item Proof that $\setY$ is a \prope{sufficient statistic} for the \vale{MAP estimate}:
  \begin{align*}
     \estMAP
       &\eqd \argmax_{\estT} \psP\set{\rvx(t;\theta)}{\rvy(t;\theta)}
       &&    \text{by definition of \vale{MAP estimate} \xref{def:estMAP}}
     \\&=    \argmax_{\estT} \psP\set{\rvx(t;\theta)}{\setY}
       &&    \text{by \pref{item:sstat_P}}
  \end{align*}

  \item Proof that $\setY$ is a \prope{sufficient statistic} for the \fncte{ML estimate}:
  \begin{align*}
     \estML
       &\eqd \argmax_{\estT} \psP\set{\rvy(t;\theta)}{\rvx(t;\theta)}
       &&    \text{by definition of \fncte{ML estimate} \xref{def:estML}}
     \\&=    \argmax_{\estT} \psP\set{\sum_{n=1}^\xN\fdoty_n\fpsi_n(t)+\rvv'(t)}{\rvx(t;\theta)}
     \\&=    \argmax_{\estT} \psP\set{\setY,\rvv'(t)}{\rvx(t;\theta)}
       &&    \text{because $\setY$ and $\rvv'(t)$ can be extracted by  $\inprod{\cdots}{\fpsi_n(t)}$}
     \\&=    \argmax_{\estT} \psP\set{\setY}{\rvx(t;\theta)}\psP\setn{\rvv'(t)}{\rvx(t;\theta)}
       &&    \text{by \prope{independence} of $\setY$ and $\rvv'(t)$
                   \xref{ilem:sstat_independent}}
     \\&=    \argmax_{\estT} \psP\set{\setY}{\rvx(t;\theta)}\psP\setn{\rvv'(t)}
       &&    \text{by \prope{independence} of $\rvx(t)$ and $\rvv'(t)$}
     \\&=    \argmax_{\estT} \psP\set{\setY}{\rvx(t;\theta)}
       &&    \text{by \prope{independence} of $\rvv'(t)$ and $\theta$}
  \end{align*}
\end{enumerate}
\end{proof}

%======================================
\section{Additive noise}
%======================================
%Depending on the nature of the channel (additive, white, and/or \prope{Gaussian})
%we can know certain characteristics of the noise and received statistics.
%These are described in the next four theorems.

%---------------------------------------
\begin{theorem}[\thmd{Additive noise projection statistics}]
\label{thm:an_stats}
\index{projection statistics!Additive noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t;\theta)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \brb{\begin{array}{FrclDD}
    (A). & \fy(t;\theta)             &\eqd     & \fx(t;\theta) + \fv(t) & (\propb{additive})              & and
  \\(B). & \pE\brs{\fv(t)}           &=        & 0                      & (\prope{zero-mean})             & and
  \\(C). & \fx(t)                    &\subseteq& \linspan\Psi           & ($\Psi$ \prope{spans} $\fx(t)$) & and
  \\(D). & \inprod{\fpsi_n}{\fpsi_m} &=        & \kdelta_{mn}           & (\prope{orthonormal})
  \end{array}}
  \implies
  \brb{\begin{array}{lrcl}
     \pE\brs{\fdoty_n(\theta)} &=& \fdotx_n(\theta)
  \end{array}}
}
\end{theorem}
\begin{proof}
\begin{align*}
   \pE\brs{\fdoty_n(\theta)}
     &\eqd \pE\brs{\inprod{\rvy(t;\theta)}{\fpsi_n(t)}}
     && \text{by definition of $\fdoty_n$}
     && \text{\xref{def:sstat}}
   \\&= \pE{\inprod{\rvx(t;\theta)+\fv(t)}{\fpsi_n(t)}}
     && \text{by \prope{additive} hypothesis}
     && \text{hypothesis (A)}
   \\&= \pE\brs{\inprod{\rvx(t;\theta){\fpsi_n(t)}} + {\inprod{\fv(t)}{\fpsi_n(t)}}}
     && \text{by \prope{additive} property of $\inprodn$}
     && \text{\xref{def:inprod}}
   \\&= \pE\brs{\inprod{\sum_{k=1}^\xN \fdotx_k(\theta) \fpsi_k(t)}{\fpsi_n(t)} + \fdotv_n}
     && \text{by \prope{basis} hypothesis}
     && \text{(C)}
   \\&= \pE\brs{\sum_{k=1}^\xN \fdotx_k(\theta) \inprod{\fpsi_k(t)}{\fpsi_n(t)} + \fdotv_n}
     && \text{by \prope{additive} property of $\inprodn$}
     && \text{\xref{def:inprod}}
   \\&= \pE\brs{\sum_{k=1}^\xN \fdotx_k(\theta) \kdelta_{k-n}(t) + \fdotv_n}
     && \text{by \prope{orthonormal} hypothesis}
     && \text{(D)}
   \\&= \pE\brs{\fdotx_n(\theta) + \fdotv_n}
     && \text{by definition of $\kdelta$}
     && \text{\ifsxref{vsinprod}{def:kdelta}}
   \\&= \pE{\fdotx_n(\theta) } + \cancelto{0}{\pE{\fdotv_n}}
     && \text{by \prope{linearity} of $\pE$}
     && \text{\xref{thm:pE_linop}}
   \\&= \fdotx_n(\theta)
     && \text{by (B) and \prefp{lem:fdotv_pE}}
\end{align*}
\end{proof}

%======================================
%\subsection{Additive gaussian noise channel}
%\label{sec:opCagn}
%======================================
\begin{figure}[ht] \color{figcolor}
\centering%
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotv_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdoty_n(\theta)_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdoty_n(\theta)_2)$} }
\end{picture}
\caption{
  Additive \prope{Gaussian} noise channel Statistics
   %\label{fig:awgn_stats}
   }
\end{figure}

%---------------------------------------
\begin{theorem}[\thmd{Additive Gaussian noise projection statistics}]
\label{thm:agn_stats}
\index{projection statistics!Additive \prope{Gaussian} noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t;\theta)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \mcom{\brb{\begin{array}{FrclDD}
    (A). & \fy(t;\theta)             &\eqd     & \fx(t) + \fv(t)  & (\propb{additive})              & and
  \\(B). & \fv(t)                    &\sim     & \pN{0}{\sigma^2} & (\propb{Gaussian})              & and
  \\(C). & \fx(t)                    &\subseteq& \linspan\Psi     & ($\Psi$ \prope{spans} $\fx(t)$) & and
  \\(D). & \inprod{\fpsi_n}{\fpsi_m} &=        & \kdelta_{mn}     & (\prope{orthonormal})
  \end{array}}}{\prope{additive Gaussian} system}
\implies
\brb{\begin{array}{rclD}
   \fdoty_n(\theta) &\sim& \pN{\fdotx_n(\theta)}{\sigma^2} & (\prope{Gaussian})
\end{array}}
}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Proof for (1): By hypothesis (B) and \prefp{lem:fdotv_pE}.

  \item Proof for (2):
    \begin{align*}
       \pE\brs{\fdoty_n(\theta)}
         &\eqd \pE\brs{\inprod{\rvy(t;\theta)}{\fpsi_n(t)}  |\theta}
         && \text{by definition of $\fdoty_n$}
         && \text{\xref{def:sstat}}
       \\&= \pE\brs{\inprod{\rvx(t;\theta)+\fv(t)}{\fpsi_n(t)}}
         && \text{by \prope{additive} hypothesis}
         && \text{hypothesis (A)}
       \\&= \pE\brs{\inprod{\rvx(t;\theta)}{\fpsi_n(t)}} +   \pE\brs{\inprod{\fv(t)}{\fpsi_n(t)}}
         && \text{by \prope{additive} property of $\inprodn$}
         && \text{\xref{def:inprod}}
       \\&= \pE\inprod{\sum_{k=1}^\xN \fdotx_k(\theta) \fpsi_k(t)}{\fpsi_n(t)} + \pE\fdotv_n
         && \text{by \prope{basis} hypothesis}
         && \text{(C)}
       \\&=    \sum_{k=1}^\xN \pE\brs{\fdotx_k(\theta)}\inprod{\fpsi_k(t)}{\fpsi_n(t)} + \pE\fdotv_n
         && \text{by \prope{additive} property of $\inprodn$}
         && \text{\xref{def:inprod}}
       \\&=    \sum_{k=1}^\xN \pE\brs{\fdotx_k(\theta)}\kdelta_{k-n}(t) + \pE\fdotv_n
         && \text{by \prope{orthonormal} hypothesis}
         && \text{(D)}
       \\&=  \pE\fdotx_n(\theta)  + \pE\fdotv_n
         && \text{by definition of $\kdelta$}
         && \text{\ifsxref{vsinprod}{def:kdelta}}
       \\&= \fdotx_n(\theta) + 0
         && \text{by \prefp{lem:fdotv_pE}}
    \end{align*}

  \item Proof for (3):
        The distribution follows because the process is a linear operations on a Gaussian process.
\end{enumerate}
\end{proof}

%======================================
%\subsection{Additive white noise channel}
%\label{sec:opCawn}
%======================================
%---------------------------------------
\begin{theorem}[\thmd{Additive white noise projection statistics}]
\label{thm:awn_stats}
\index{projection statistics!Additive white noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t;\theta)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \mcom{\brbr{\begin{array}{FrclD}
    (A). & \fy(t;\theta)                    &\eqd     & \fx(t) + \fv(t)     & and %& (\propb{additive})              & and
  \\(B). & \cov{\fv(t)}{\fv(u)}      &=        & \sigma^2\delta(t-u) & and %& (\propb{white})                 & and
  \\(C). & \pE\brs{\fv(t)}           &=        & 0                   & and %  (\prope{zero-mean})             & and
 %\\(B). & \fv(t)                    &\sim     & \pN{0}{\sigma^2}    & and %& (\propb{Gaussian})              & and
  \\(E). & \fx(t)                    &\subseteq& \linspan\Psi        & and %& ($\Psi$ \prope{spans} $\fx(t)$) & and
  \\(E). & \inprod{\fpsi_n}{\fpsi_m} &=        & \kdelta_{mn}        &     %& (\prope{orthonormal})
  \end{array}}}{\prope{additive white} system}
  \implies
  \brbl{\begin{array}{FlclD}
       (1). & \pE\fdotv_n                            &=& 0                     & (\prope{zero-mean})    %& and
     \\(2). & \pE(\fdoty_n(\theta))                   &=& \fdotx_n(\theta)      &                        %& and
     \\(3). & \cov{\fdotv_n}{\fdotv_m}               &=& \sigma^2 \kdelta_{nm} & (\prope{uncorrelated}) %& and
     \\(4). & \cov{\fdoty_n(\theta)}{\fdoty_m|\theta }&=& \sigma^2 \kdelta_{nm} & (\prope{uncorrelated}) %&
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Because the noise is \prope{additive} (hypothesis A)\ldots
    \begin{align*}
         \pE\fdotv_{n}            &= 0                            && \text{by \prope{additive} property and \prefp{thm:an_stats}}
       \\(\fdoty_n(\theta))       &= \fdotx_n(\theta)  + \fdotv_n && \text{by \prope{additive} property and \prefp{thm:an_stats}}
       \\\pE({\fdoty_n} | \theta) &= \fdotx_n(\theta)             && \text{by \prope{additive} property and \prefp{thm:an_stats}}
    \end{align*}

  \item Proof for (4):
    \begin{align*}
      \cov{\fdoty_n(\theta)}{\fdoty_m|\theta }
         &= \pE\brs{\fdoty_n \fdoty_m |\theta} - [\pE\fdoty_n(\theta)][\pE\fdoty_m|\theta ]
       \\&= \pE\brs{(\fdotx_n(\theta) +\fdotv_n)(\fdotx_m(\theta) +\fdotv_m)} - \fdotx_n(\theta) \fdotx_m(\theta)
       \\&= \pE\brs{\fdotx_n(\theta) \fdotx_m(\theta) +\fdotx_n(\theta) \fdotv_m+ \fdotv_n\fdotx_m(\theta) +\fdotv_n\fdotv_m } - \fdotx_n(\theta) \fdotx_m(\theta)
       \\&= \fdotx_n(\theta) \fdotx_m(\theta) + \fdotx_n(\theta) \pE\brs{\fdotv_m}+ \pE\brs{\fdotv_n}\fdotx_m(\theta) +\pE\brs{\fdotv_n\fdotv_m}  - \fdotx_n(\theta) \fdotx_m(\theta)
       \\&= 0 + \fdotx_n(\theta) \cdot0 + 0\cdot\fdotx_m(\theta) + \cov{\fdotv_n}{\fdotv_m}+[\pE\fdotv_n][\pE\fdotv_m]
       \\&= \sigma^2 \kdelta_{nm} + 0\cdot0
         && \text{by \pref{lem:fdotv_white}}
       \\&= \brbl{\begin{array}{lM}
                \sigma^2 & for $n=m$ \\
                0        & for $n\ne m$.
             \end{array}}
    \end{align*}
\end{enumerate}
\end{proof}

%======================================
%\subsection{Additive white gaussian noise channel}
%\label{sec:opCawgn}
%======================================
\begin{figure}[ht] \color{figcolor}
\centering%
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotv_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdoty_n(\theta)_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdoty_n(\theta)_2)$} }
\end{picture}
\caption{
  Additive white \prope{Gaussian} noise channel statistics
   \label{fig:awgn_stats}
   }
\end{figure}

%---------------------------------------
\begin{theorem}[\thmd{AWGN projection statistics}]
\label{thm:awgn_stats}
\label{thm:ms_stats}
\index{projection statistics!Additive white \prope{Gaussian} noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t;\theta)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \mcom{\brbr{\begin{array}{FrclD}
    (A). & \fy(t;\theta)             &\eqd     & \fx(t) + \fv(t)     & and %& (\propb{additive})              & and
  \\(B). & \cov{\fv(t)}{\fv(u)}      &=        & \sigma^2\delta(t-u) & and %& (\propb{white})                 & and
  \\(C). & \fv(t)                    &\sim     & \pN{0}{\sigma^2}    & and %& (\propb{Gaussian})              & and
  \\(D). & \fx(t)                    &\subseteq& \linspan\Psi        & and %& ($\Psi$ \prope{spans} $\fx(t)$) & and
  \\(E). & \inprod{\fpsi_n}{\fpsi_m} &=        & \kdelta_{mn}        &     %& (\prope{orthonormal})
  \end{array}}}{\prope{additive white Gaussian} system}
  \implies
  \brbl{\begin{array}{FlD}
    (1). & \fdoty_n(\theta)           \sim  \pN{\fdotx_n(\theta)}{\sigma^2}            & (\prope{Gaussian})
  \\(2). & \cov{\fdoty_n}{\fdoty_m }  =     \sigma^2 \kdelta_{nm}                      & (\prope{uncorrelated})
  \\(3). & \psP\setn{\fdoty_n \land \fdoty_m} = \psP\setn{\fdoty_n}\psP\setn{\fdoty_m} & (\prope{independent})
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Proof for (1) follow because the operations are \prope{linear} on processes are \prope{Gaussian} (hypothesis C).

  \item
    \begin{align*}
       \pE{\fdotv_{n}}           &= 0                      && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
     \\\fdoty_n                  &= \fdotx_n  + \fdotv_n   && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
     \\\pE{\fdoty_n}             &= \fdotx_n               && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
     \\\cov{\fdoty_n}{\fdoty_m } &= \sigma^2 \kdelta_{mn}  && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
    \end{align*}

  \item Because the processes are \prope{Gaussian},
        \prope{uncorrelated} implies \prope{independent}.
\end{enumerate}
\end{proof}

%======================================
\section{ML estimates}
\label{sec:awgn_est}
\index{maximum likelihood estimation}
%======================================
The AWGN projection statistics provided by
\prefpp{thm:awgn_stats} help generate the optimal
ML-estimates for a number of communication systems.
These ML-estimates can be expressed in either of two standard forms:
\begin{liste}
  \item {\bf Spectral decompostion}:
     The optimal estimate is expressed in terms of \hie{projections}
     of signals onto orthonormal basis functions.
  \item {\bf Matched signal}:
     The optimal estimate is expressed in terms of the (noisy)
     received signal correlated with (``matched" with)
     the (noiseless) transmitted signal.
\end{liste}
\prefpp{thm:estML_general} (next) expresses the general
optimal \fncte{ML estimate} in both of these forms.

Parameter detection is a special case of parameter estimation.
In parameter detection, the estimate is a member of an finite set.
In parameter estimation, the estimate is a member of an infinite set
\xref{sec:awgn_est}.

%---------------------------------------
\begin{theorem}[\thmd{General ML estimation}]
\index{maximum likelihood estimation!general}
\label{thm:estML_general}
\label{thm:ml_est_det}
%\citepp{srv}{157}{158}
%---------------------------------------
Let $\Psi$, $\rvy(t;\theta)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
Let $\estML$  be the \fncte{ML estimate} \xref{def:estML} of $\theta$.
\thmbox{\begin{array}{rc>{\ds}l@{\qquad}D}
   \estML
     &=& \argmin_{\estT} \brs{ \sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 }
       & (spectral decomposition)
   \\&=& \argmax_{\estT}
         \left[ 2\inprod{\rvy(t;\theta)}{\rvx(t;\theta)}-\norm{\rvx(t;\theta)}^2 \right]
       & (matched signal)
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML
     &= \argmax_{\estT} \psP\set{\rvy(t;\theta)}{\rvx(t;\theta)}
   \\&= \argmax_{\estT} \psP\set{\fdoty_1,\fdoty_2,\ldots,\fdoty_n}{\rvx(t;\theta)}
     && \text{by \prefpp{thm:sstat}}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN \psP\set{\fdoty_n}{\rvx(t;\theta)}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN \pdfpb{\fdoty_n|\rvx(t;\theta)}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN
         \frac{1}{\sqrt{2\pi\sigma^2}}
         \exp{\frac{[\fdoty_n-\fdotx_n(\estT)]^2}{-2\sigma^2} }
     && \text{by \prefpp{thm:awgn_stats}}
   \\&= \argmax_{\estT}
         \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
         \exp{\frac{-1}{2\sigma^2} \sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 }
   \\&= \argmax_{\estT}
         \left[ -\sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
\\ \\
   \\&= \argmax_{\estT}
         \left[ -\lim_{N\to\infty}\sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
     && \text{by \prefpp{thm:sstat}}
   \\&= \argmax_{\estT}
         \left[ -\norm{\rvy(t;\theta)-\rvx(t;\theta)}^2 \right]
     && \text{by \thme{Plancheral's formula} 
              \xref{thm:plancherel}}
   \\&= \argmax_{\estT}
         \left[ -\norm{\rvy(t;\theta)}^2 +2\Real\inprod{\rvy(t;\theta)}{\rvx(t;\theta)}-\norm{\rvx(t;\theta)}^2 \right]
   \\&= \argmax_{\estT}
         \left[ 2\inprod{\rvy(t;\theta)}{\rvx(t;\theta)}-\norm{\rvx(t;\theta)}^2 \right]
     && \text{because $\rvy(t;\theta)$ \prope{independent} of $\estT$}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{ML amplitude estimation}]
\footnote{
  \citerppg{srv}{158}{159}{013125295X}
  }
\label{thm:estML_amplitude}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system.
\thmbox{
  \brb{\begin{array}{Frc>{\ds}lD}
     (A). & \mc{3}{M}{$\fv(t)$ is \prope{AWGN}}       & and
   \\(B). & \rvy(t;a) &=&     \rvx(t;a) + \fv(t) & and
   \\(C). & \rvx(t;a)      &\eqd&  a  \sym(t).        &
  \end{array}}
  \implies
  \brb{\begin{array}{Frc>{\ds}lM}
    (1). & \estML[a]    &=&  \frac{1}{\norm{\sym(t)}^2} \sum_{n=1}^\xN \fdoty_n \fdotlam_n &
  \\(2). & \pE\estML[a] &=& a                                    &   (\prope{unbiased})
  \\(3). & \var\estML[a]&=& \frac{\sigma^2}{\norm{\sym(t)}^2}    &
  \\(4). & \var\estML[a]&=& \mbox{CR lower bound}                &   (\prope{efficient})
  \end{array}}
  }
\end{theorem}

\begin{proof}\\
\begin{enumerate}
\item \fncte{ML estimate} in ``matched signal" form:
\begin{align*}
   \estML[a]
     &= \argmax_a
         \left[ 2\inprod{\rvy(t;\theta)}{\rvx(t;\theta)}-\norm{\rvx(t;\phi)}^2 \right]
     && \text{by \prefpp{thm:estML_general}}
   \\&= \argmax_a
         \left[ 2\inprod{\rvy(t;\theta)}{a\lambda(t)}-\norm{a\lambda(t)}^2 \right]
     && \text{by hypothesis}
   \\&= \arg_a
         \left[ \pderiv{}{a}2a\inprod{\rvy(t;\theta)}{\lambda(t)}-\pderiv{}{a}a^2\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ 2\inprod{\rvy(t;\theta)}{\lambda(t)}-2a\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ \inprod{\rvy(t;\theta)}{\lambda(t)}=a\norm{\lambda(t)}^2 \right]
   \\&= \frac{1}{\norm{\lambda(t)}^2} \inprod{\rvy(t;\theta)}{\lambda(t)}
\end{align*}

\item \fncte{ML estimate} in ``spectral decomposition" form:
\begin{align*}
   \estML[a]
     &= \argmin_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}^2 }
     && \text{by \prefpp{thm:estML_general}}
   \\&= \arg_a
         \brp{ \pderiv{}{ a }\sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}^2=0 }
   \\&= \arg_a
         \brp{ 2\sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}\pderiv{}{ a }\fdotx_n( a )=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n - \inprod{ a \lambda(t)}{\fpsi_n(t)}}\pderiv{}{ a }\inprod{ a \lambda(t)}{\fpsi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \inprod{\lambda(t)}{\fpsi_n(t)}}\pderiv{}{ a }( a \inprod{\lambda(t)}{\fpsi_n(t)})=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \fdotlam_n } \inprod{\lambda(t)}{\fpsi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \fdotlam_n } \fdotlam_n=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \fdoty_n\fdotlam_n = \sum_{n=1}^\xN  a \fdotlam_n^2 }
   \\&= \brp{\frac{1}{\sum_{n=1}^\xN \fdotlam_n^2}}
         \sum_{n=1}^\xN \fdoty_n\fdotlam_n
   \\&= \frac{1}{\norm{\lambda(t)}^2}
         \sum_{n=1}^\xN \fdoty_n\fdotlam_n
\end{align*}

\item Prove that the estimate $\estML[a]$ is \propb{unbiased}:

\begin{align*}
   \pE\estML[a]
     &= \pE\frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} \rvy(t;\theta)\sym(t) \dt
     && \text{by previous result}
   \\&= \pE\frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} [ a \sym(t)+\fv(t)]\sym(t) \dt
     && \text{by hypothesis}
   \\&= \frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} \pE[ a \sym(t)+\fv(t)]\sym(t) \dt
     && \text{by linearity of $\int\cdot\dt$ and $\pE$}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \int_{t\in\R} \sym^2(t) \dt
     && \text{by $\pE$ operation}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \norm{\sym(t)}^2
     && \text{by definition of $\norm{\cdot}^2$}
   \\&=   a
\end{align*}

\item Compute the variance of $\estML[a]$:
\begin{align*}
  \pE \estML[a]^2
    &= \pE \left[ \frac{1}{\norm{\lambda(t)}^2} \int_{t\in\R} \rvy(t;\theta)\lambda(t) \dt\right]^2
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \rvy(t;\theta)\lambda(t) \dt \int_v \rvy(v)\lambda(v) \dv
        \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v [a\lambda(t) + \fv(t)][a\lambda(v) + \fv(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v
        [a^2\lambda(t)\lambda(v) + a\lambda(t)\fv(v) + a\lambda(v)\fv(t) + \fv(t)\fv(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v
        [a^2\lambda(t)\lambda(v) + 0 + 0 + \sigma^2\delta(t-v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v a^2\lambda^2(t)\lambda^2(v) \dv\dt +
        \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v \sigma^2\delta(t-v) \lambda(t) \lambda(v) \dv\dt
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        a^2 \int_{t\in\R} \lambda^2(t) \dt \int_v \lambda^2(v) \dv +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2\int_{t\in\R} \lambda^2(t) \dt
  \\&= a^2 \frac{1}{\norm{\lambda(t)}^4}
        \norm{\lambda(t)}^2 \norm{\lambda(v)}^2 +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2 \norm{\lambda(t)}^2
  \\&= a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}
\\
\\
  \var\estML[a]
    &= \pE \estML[a]^2 - (\pE \estML[a])^2
  \\&= \left.\left.\left(a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}\right) - \right( a^2 \right)
  \\&= \frac{\sigma^2}{\norm{\lambda(t)}^2}
\end{align*}

\item Compute the \ineq{Cram/'er-Rao Bound}:
\begin{align*}
   \pdfpb{\rvy(t;\theta)|\fx(t; a)}
     &=  \pdfpb{\fdoty_1, \fdoty_2,\ldots,\fdoty_{\xN}|\fx(t; a)}
   \\&=  \prod_{n=1}^\xN \frac{1}{\sqrt{2\pi\sigma^2}}
          \exp{\frac{(\fdoty_n- a\fdotlam_n)^2}{-2\sigma^2}}
   \\&=  \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
\\
\\
   \pderiv{}{a}\ln\pdfpb{\rvy(t;\theta)|\fx(t; a)}
     &=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
       +  \pderiv{}{a}\ln
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}
          \brs{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \frac{1}{-2\sigma^2} \sum_{n=1}^\xN 2(\fdoty_n- a\fdotlam_n)(- \fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdoty_n- a\fdotlam_n)
\\
\\
   \pderiv{^2}{a^2}\ln\pdfpb{\rvy(t;\theta)|\fx(t; a)}
     &=  \pderiv{}{a}\pderiv{}{a}\ln\pdfpb{\rvy(t;\theta)|\fx(t; a)}
   \\&=  \pderiv{}{a}
          \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdoty_n- a\fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(-\fdotlam_n)
   \\&=  \frac{-1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n^2
   \\&=  \frac{-\norm{\lambda(t)}^2}{\sigma^2}
\\
\\
   \var\estML[a]
     &\eqd \pE\brs{\estML[a]-\pE\estML[a]}^2
   \\&=    \pE\brs{\estML[a]- a}^2
   \\&\ge  \frac{-1}{\pE\brp{\pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t;\theta)|\fx(t; a)}}}
   \\&=    \frac{-1}{\pE\brp{\frac{-\norm{\lambda(t)}^2}{\sigma^2}}}
   \\&=    \frac{\sigma^2}{\norm{\lambda(t)}^2}
     \qquad\text{(Cram/'er-Rao lower bound of the variance)}
\end{align*}

\item Proof that $\estML[a]$ is an \prope{efficient} estimate:

An estimate is \prope{efficient} if
$\var\estML[a]=\mbox{CR lower bound}$.
We have already proven this, so $\estML[a]$ is an \prope{efficient} estimate.

Also, even without explicitly computing the variance of $\estML[a]$,
the variance equals the \vale{Cram/'er-Rao lower bound}
(and hence $\estML[a]$ is an \prope{efficient} estimate)
if and only if
\\\indentx$\ds\estML[a] -  a =
   \brp{\frac{-1}{\pE\brs{
              \pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t;\theta)|\fx(t; a)}
           }}}
   \brp{\pderiv{}{ a} \ln \pdfpb{\rvy(t;\theta)|\fx(t; a)}}
  $
\begin{align*}
   \brp{\frac{-1}{\pE\brp{\pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t;\theta)|\fx(t; a)}}}}
   \brp{\pderiv{}{ a} \ln \pdfpb{\rvy(t;\theta)|\fx(t; a)}}
     &= \left(\frac{\sigma^2}{\norm{\lambda(t)}^2}\right)
         \left(
           \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam( \fdoty - a \fdotlam)
         \right)
   \\&= \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam \fdoty -
         \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam^2
   \\&= \estML[a] - a
\end{align*}
\end{enumerate}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{ML phase estimation}]
\label{thm:estML_phase}
\footnote{
  \citerppg{srv}{159}{160}{013125295X}
  }
\index{maximum likelihood estimation!phase}
%---------------------------------------
\thmbox{
  \brb{\begin{array}{Frc>{\ds}lD}
     (A). & \mc{3}{M}{$\fv(t)$ is \prope{AWGN}}        & and
   \\(B). & \rvy(t;\phi) &=&     \rvx(t;\phi) + \fv(t) & and
   \\(C). & \rvx(t;\phi) &\eqd&  A\cos(2\pi f_ct +  \phi) &  
  \end{array}}
  \implies
  \brb{
   \estML[\phi]
      =   -\atan\brp{
           \frac{\inprod{\rvy(t;\theta)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t;\theta)}{\cos(2\pi f_ct)}}
           }
   }}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML[\phi]
     &= \argmax_\phi
         \brs{ 2\inprod{\rvy(t;\phi)}{\rvx(t;\phi)}-\norm{\rvx(t;\phi)}^2 }
     && \text{by \prefpp{thm:estML_general}}
   \\&= \argmax_\phi
         \brs{ 2\inprod{\rvy(t;\phi)}{\rvx(t;\phi)} }
     && \text{because $\norm{\rvx(t;\phi)}$ does not depend on $\phi$}
   \\&= \arg_\phi
         \brs{ \pderiv{}{\phi} \inprod{\rvy(t;\phi)}{\rvx(t;\phi)} = 0 }
   \\&= \arg_\phi
         \brs{ \inprod{\rvy(t;\phi)}{\pderiv{}{\phi} \rvx(t;\phi)} = 0 }
     && \text{because $\inprodn$ is \prope{linear}}
   \\&= \arg_\phi
         \brs{\inprod{\rvy(t;\phi)}{\pderiv{}{\phi} A\cos(2\pi f_ct+\phi)} = 0 }
     && \text{by definition of $\fx(t;\phi)$}
   \\&= \arg_\phi
         \brs{\inprod{\rvy(t;\phi)}{-A\sin(2\pi f_ct+\phi)} = 0 }
     && \text{because $\pderiv{}{\phi} \cos(x)=-\sin(x)$}
   \\&= \arg_\phi
         \brs{ -A\inprod{\rvy(t;\phi)}{\cos(2\pi f_ct)\sin\phi+\sin(2\pi f_ct)\cos\phi} = 0 }
    %&& \text{because $\sin(x+y)=\sin x \cos y + \cos x \sin y$}
     && \text{by \thme{double angle formulas}}
     && \text{\ifsxref{harTrig}{thm:trig_double}}
   \\&= \arg_\phi \brs{
           \sin\phi\inprod{\rvy(t;\phi)}{\cos(2\pi f_ct)} =
          -\cos\phi\inprod{\rvy(t;\phi)}{\sin(2\pi f_ct)}
           }
   \\&= \arg_\phi \brs{
           \frac{\sin\phi}{\cos\phi} =
          -\frac{\inprod{\rvy(t;\phi)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t;\phi)}{\cos(2\pi f_ct)}}
           }
   \\&= \arg_\phi \brs{
           \tan\phi =
          -\frac{\inprod{\rvy(t;\phi)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t;\phi)}{\cos(2\pi f_ct)}}
           }
   \\&=  -\atan\brp{
           \frac{\inprod{\rvy(t;\phi)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t;\phi)}{\cos(2\pi f_ct)}}
           }
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{ML estimation of a function of a parameter}]
\footnote{
  \citerppg{srv}{142}{143}{013125295X}
  }
\label{thm:estML-CR}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
$\begin{array}[t]{rcl}
     \rvy(t;\theta)       &=& \fx(t;\theta) + \fv(t)
   \\\fx(t;\theta) &=& \fg(\theta)
\end{array}$\\
and $\fg$ is \prope{one-to-one and onto} (\prope{invertible}).
\\
\thmbox{\begin{array}{M>{\ds}rc>{\ds}l}
  Then the optimal ML-estimate of parameter $\theta$ is
   & \estML &=& \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right).
  \\
  If an \fncte{ML estimate} $\estML$ is unbiased ($\pE \estML = \theta$) then
    & \var\estML &\ge&
      \frac{\sigma^2}{\xN}
      \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}.
  \\
  If $\fg(\theta) = \theta$ then $\estML$ is an \propb{efficient} estimate such that
   & \var\estML &=& \frac{\sigma^2}{\xN}.
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML
     &= \argmin_{\theta}
         \brs{\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }
     && \text{by \prefp{thm:estML_general}}
   \\&= \arg_{\theta}\brs{
            \pderiv{}{\theta}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 = 0
         }
     && \text{because form is \prope{quadratic}}
   \\&= \arg_{\theta}\left[
             2\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]\pderiv{}{\theta}\fg(\theta) = 0
         \right]
   \\&= \arg_{\theta}\left[
             2\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)] = 0
         \right]
   \\&= \arg_{\theta}\left[
             \sum_{n=1}^\xN \fdoty_n = \xN \fg(\theta)
         \right]
   \\&= \arg_{\theta}\left[
             \fg(\theta) = \frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n
         \right]
   \\&= \arg_{\theta}\brs{
              \theta  = \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right)
         }
   \\&= \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right)
\end{align*}


If $\estML$ is unbiased ($\pE\estML=\theta$), we can use
the \vale{Cram/'er-Rao bound} to find a lower bound on the variance:

\begin{align*}
  \var\estML
     &\eqd \pE\brs{\estML-\pE\estML}^2
   \\&= \pE\brs{\estML-\theta}^2
   \\&\ge \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t;\theta)|\fx(t;\theta)}
           }}
     && \text{by \ineqe{Cram/'er-Rao Inequality}}
   \\&=   \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln
              \pdfpb{\fdoty_1, \fdoty_2,\ldots,\fdoty_{\xN}|\fx(t;\theta)}
           }}
     && \begin{array}{@{}M}
         by \thme{Sufficient Statistic Theorem}\\
         \xref{thm:sstat}
        \end{array}
   \\&=   \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
              \exp\brp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }\right]
           }}
     && \begin{array}{@{}M}
         by \prope{AWGN} hypothesis\\
         and \prefp{thm:awgn_stats}
        \end{array}
   \\&=   \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN \right]
              +
              \pderiv{^2}{\theta^2} \ln\left[
              \exp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }\right]
           }}
  \\&=   \frac{-1}{\ds\pE\brp{
             \pderiv{^2}{\theta^2}
             \left(\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 \right)
          }}
  \\&=   \frac{2\sigma^2}{\ds\pE\brp{
             \pderiv{}{\theta} \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2
          }}
  \\&=   \frac{2\sigma^2}{\ds\pE\brp{
             -2\pderiv{}{\theta}
             \pderiv{\fg(\theta)}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
          }}
    && \text{by \thme{Chain Rule}}
  \\&=   \frac{-\sigma^2}{\ds\pE\brp{
             \pderiv{\fg^2(\theta)}{\theta^2}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
             +
             \pderiv{\fg(\theta)}{\theta}
             \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
          }}
     && \text{by \thme{Product Rule}}
   \\&=   \frac{-\sigma^2}{\ds\pE\brp{
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
              -N
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }}
   \\&=   \frac{-\sigma^2}{\ds
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN \pE[\fdoty_n-\fg(\theta)]
              -\xN
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
   \\&=   \frac{-\sigma^2}{
              -\xN
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
     && \text{because derivative of constant = 0}
   \\&=   \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}
\end{align*}

The inequality becomes equality (an \prope{efficient} estimate)
if and only if
\[ \estML - \theta =
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t;\theta)|\fx(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\rvy(t;\theta)|\fx(t;\theta)} \right).
\]

\begin{align*}
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t;\theta)|\fx(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\rvy(t;\theta)|\fx(t;\theta)} \right)
     &= \left(
         \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2} \right)
         \left(\frac{-1}{2\sigma^2}(2)\pderiv{\fg(\theta)}{\theta}
         \sum_{n=1}^\xN [\fdoty_n - \fg(\theta) ]\right)
   \\&= -\frac{1}{\xN}
         \frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\sum_{n=1}^\xN [\fdoty_n - \fg(\theta) ] \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n - \fg(\theta) \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\estML - \fg(\theta) \right)
   \\&= -(\estML - \theta)
\end{align*}
\end{proof}

%======================================
\section{Example data}
%======================================
\begin{longtable}{|l|}
\hline
``Pop Goes the World" song by Men Without Hats
\\\includegraphics[width=\tw-10mm]{graphics/MenWithoutHats_PopGoesWorld.jpg}
\\\hline
  Earthquake data
\\\includegraphics[width=\tw-10mm,height=50mm]{graphics/earthquake_kapi_20180928.pdf}\footnote{\url{https://www.iris.edu/wilber3/find_stations/10953070}}
\\\hline
  Sunspot data
\\\includegraphics[width=\tw-10mm,height=50mm]{graphics/sunspots_1749-2004.pdf}\footnote{\url{https://d32ogoqmya1dw8.cloudfront.net/files/introgeo/teachingwdata/examples/GreenwichSSNvstime.txt}}
\\\hline
  Dow Jones Industrial Average
\\\includegraphics[width=\tw-10mm,height=50mm]{graphics/dji30_20171006-20181004.pdf}
\\\hline
  Ebola Sequence
\\\includegraphics[width=\tw-10mm]{graphics/dna_ebola_51_seq.pdf}
\\\hline
\end{longtable}

%\includegraphics[width=\tw-10mm]{graphics/fdie_5eed_51_seq.pdf}

%\footnote{
%\begin{lstlisting}
%#include<stdlib.h>
%...
%srand(0x5EED);
%for(n=0; n<N; n++){x[n] = 'A' + rand()%6;}
%\end{lstlisting}
%  }
%\vfill

%======================================
\input{../common/matched_filter.tex}
%======================================

%======================================
\section{Colored noise}
\index{noise!colored}
%======================================
This chapter presented several theorems whose results depended on the
noise being white.
However if the noise is {\bf colored}, then these results are
invalid.
But there is still hope for colored noise.
Processing colored signals can be accomplished using two techniques:
\begin{enume}
   \item Karhunen-Lo/`eve basis functions \xref{sec:KL}
   \item whitening filter
\footnote{
   \begin{tabular}[t]{ll}
      \ope{Continuous data whitening}: & \prefp{sec:whiten}  \\
      \ope{Discrete data whitening}:   & \prefp{sec:d-whiten}
   \end{tabular}
   }
\end{enume}

\paragraph{Karhunen-Lo/`eve.}
If the noise is \prope{white}, the set $\set{\inprod{\rvy(t;\theta)}{\fpsi_n(t)}}{n=1,2,\ldots,\xN}$
is a \prope{sufficient statistic} regardless of which
set $\setn{\fpsi_n(t)}$ of orthonormal basis functions are used.
If the noise is \prope{colored}, and if $\{\fpsi_n(t)\}$ satisfy the
Karhunen-Lo/`eve criterion
\\\indentx$\ds\int_{t_2}\Rxx(t,u)\fpsi_n(u)\du = \lambda_n \fpsi_n(t)$\\
then the set $\setn{\inprod{\rvy(t;\theta)}{\fpsi_n(t)}}$ is still a \prope{sufficient statistic}.

\paragraph{Whitening filter.}
The whitening filter makes the received signal $\rvy(t;\theta)$ statistically white
(uncorrelated in time). In this case,
any orthonormal basis set can be used to generate sufficient statistics.

\paragraph{Wavelets.}
Wavelets have the property that they tend to whiten data. 
For more information, see 
\citerppgc{walter}{329}{350}{9781584882275}{``Chapter 14 Orthogonal Systems and Stochastic Processes"},
\citerg{mallat}{9780124666061},
\citeP{johnstone1997},
\citeP{wornell1992}, and
\citerppgc{vidakovic}{10}{14}{9780471293651}{``Example 1.2.5 Wavelets whiten data"}
(first four references cited by B. Vidakovic).

