%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================

%======================================
\chapter{Probability Density Functions}
\label{chp:pdfs}
%======================================

\qboxnpq
  {Joseph Leonard Doob (1910--2004), pioneer of and key contributor to mathematical probability\footnotemark}
  {../common/people/doobjl_dartmouthedu.jpg}
  {While writing my book I had an argument with Feller.
   He asserted that everyone said ``random variable" and I asserted that everyone said ``chance variable."
   We obviously had to use the same name in our books, so we decided the issue by a stochastic procedure.
   That is, we tossed for it and he won.}
\citetblt{
  quote: & \citerp{snell1997}{307}, \citerp{snell2005}{251}\\
  %reference: & \citerpg{suhov2008}{238}{0521847672}\\
  image: & \url{http://www.dartmouth.edu/~chance/Doob/conversation.html}
  }
%======================================
\section{Random variables}
%======================================
The concept of the \fncte{random variable} is widely used in probability and
random processes.
Before discussing what a \fncte{random variable} \emph{is}, 
note two things that a \fncte{random variable} is \emph{not} (next remark).
%---------------------------------------
\begin{remark}
\footnote{
  \citerpg{miller2006}{130}{0471458929},
  \citerpgc{feldman2010}{4}{3642051588}{``The name ``random variable" is actually a misnomer, since it is not random and not a variable.\ldots the \fncte{random variable} simply maps each point (outcome) in the sample space to a number on the real line\ldots Technically, the space into which the \fncte{random variable} maps the sample space may be more general than the real line\ldots"},
  \citerpg{curry2011}{4}{3642166180},
  \citerpgc{trivedi2016}{2.1}{1119314208}{``The term ``random variable" is actually a misnomer, since a \fncte{random variable} $\rvX$ is really a function whose domain is the sample space $S$, and whose range is the set of all real numbers, $\R$."}
  }
%---------------------------------------
As pointed out by others, the term ``random variable" is a ``misnomer":
\\\rembox{
  \begin{array}{NM}
    \imark & A \fncte{random variable} is {\bf not random}.\\
    \imark & A \fncte{random variable} is {\bf not a variable}.
  \end{array}}
\end{remark}

What is it then? It is a \structe{function} (next definition).
In particular, it is a function that maps from an underlying stochastic process into $\R$.
Any ``\prope{randomness}" (whatever that means) it may \emph{appear} to have comes from the stochastic process it
is mapping \emph{from}. But the function itself (the \fncte{random variable} itself) is very deterministic and well-defined.
What gives it the appearance of being random is that the outcome $\omega$
of the experiment appears to be random to the observer.
So the \fncte{random variable} $\rvX(\omega)$ is simply a function of an underlying
mechanism that appears to be random.
%---------------------------------------
\begin{definition}
\index{random variable}
\footnote{
  \citerp{papoulis}{63}
  }
\label{def:randvar}
%---------------------------------------
Let $\ps$ be a \structe{probability space} \xref{def:ps}.
\defboxt{
  A \fnctd{random variable} $\rvX$ is any function in $\clFor$.
  }
\end{definition}

%=======================================
\section{Probability distributions}
%=======================================
The probability information about $\sigma$-algebra $\psE$ in a
\structe{probability space} \xref{def:ps} is completely
specified by \fncte{measure} $\psP$.
However, sometimes it is more convenient to express this same \fncte{measure}
information in terms of the \fncte{probability density function} or the
\fncte{cummulative distribution function} of the \structe{probability space}.
%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{linden2014}{93}{1107035902}{Definitions 7.1, 7.2}
  }
\label{def:pdf}
\label{def:cdf}
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} \xref{def:randvar}
on a \structe{probability space} $\ps$.
%Let $\pcx(x)$ be a \fncte{function} in $\psPcl$ and $\ppx(x)$ a function in $\clF{\psE}{\R}$.
\defbox{\begin{array}{MM}
  $\rvX$ has \fnctd{cummulative distribution function} &(\fnctd{cdf}) $\pcx(x)$ if 
    \\\mc{2}{l}{\indentx\ds\pcx(x) \eqd \psP\set{x\in\psE}{\rvX<x}}
    \\
  $\rvX$ \fnctd{probability density function}      &(\fnctd{pdf}) $\ppx(x)$ if 
    \\\mc{2}{l}{\indentx\ds\ppx(x) \eqd \ddx\pcx(x) \eqd \ddx\psP\set{x\in\psE}{\rvX < x }}
\end{array}}
\end{definition}

%---------------------------------------
\begin{remark}
%---------------------------------------
Suppose $\rvX$ be a \structe{random variable} on a \structe{probability space} $\ps$.
Note that 
\\\indentx\begin{tabular}{cl}
    \imark & Both $\rvX$ and $\psE$ are \fncte{function}s.
  \\\imark & But $\rvX$ is a {function} that maps from $\psO$ to $\R$,
  \\\imark & whereas $\psP$ is a {function} that maps from $\psE$ to $\R$.
\end{tabular}
\end{remark}

%---------------------------------------
\begin{definition}
\label{def:jointpdf}
\label{def:jointcdf}
%---------------------------------------
Let $\ps$ be a \structe{probability space} \xref{def:ps} and
$\rvX$ and $\rvY:\pso\to\R$ random variables. Then
a \fnctd{joint probability density function}    $\ppx:\pse\times\pso\to\intcc{0}{1}$ and
a \fnctd{joint cumulative distribution function} $\pcx:\pse\times\pso\to\intcc{0}{1}$
are defined as
\defbox{\begin{array}{rc>{\ds}lD}
  \pcxy(x,y) &\eqd& \psp\set{\rvX\le x}{\rvY\le y}
          & (\fncte{joint cumulative distribution function})
          \\
  \ppxy(x,y) &\eqd& \ddy\ddx\pcxy(x,y)
          & (\fncte{joint probability density function})
\end{array}}
\end{definition}

%---------------------------------------
\begin{definition}
\label{def:conpdf}
\label{def:concdf}
%---------------------------------------
Let $\ps$ be a \structe{probability space} \xref{def:ps} and
$\rvX$ a random variable. Then
a \fnctd{conditional probability density function}    $\ppx:\pse\times\pso\to\intcc{0}{1}$ and
a \fnctd{conditional cumulative distribution function} $\pcx:\pse\times\pso\to\intcc{0}{1}$
are defined as
\defbox{\begin{array}{rc>{\ds}lD}
  \pcx(x|y) &\eqd& \psp\brb{\rvX \le x|\rvY=y}
            & (\fncte{conditional cumulative distribution function}---\fncte{cdf})
            \\
  \ppx(x|y) &\eqd& \ddx\pcx(x|y)
  %\ppx(x|y) &\eqd& \lim_{\varepsilon\to 0} \frac{1}{\varepsilon}\psp\brb{x \le\rvX < x+\varepsilon | \rvY=y}
            & (\fncte{conditional probability density function}---\fncte{pdf})
\end{array}}
\end{definition}

%=======================================
\section{Properties}
%=======================================
\prefpp{def:pdf} defines the pdf and cdf of a \structe{probability space}
$\ps$ in terms of \fncte{measure} $\psp$.
Conversely, the probability \fncte{measure} $\psp\setn{a\le\rvX<b}$
of an event $\{a\le\rvX<b\}$ can be
expressed in terms of either the pdf or cdf.

%---------------------------------------
\begin{proposition}
\label{prop:pdfddx}
%---------------------------------------
Let $\rvX$ a \fncte{random variable} with \fncte{pdf} $\ppx$ and \fncte{cdf} $\pcx$
\xref{def:pdf} on the \structe{probability space} $\ps$ \xref{def:ps}.\\
\propboxt{
  $\brb{\begin{array}{FMD}
    (1).& $\pcx(x)$ and $\pcy(y)$ are \prope{continuous} & OR \\
    (2).& $\ppx(x)$ and $\ppy(y)$ are \prope{continuous}
  \end{array}}$
  \\\quad$\implies\quad
  \brb{\begin{array}{rc>{\ds}l}
      \ppx(x)    &=& \lim_{\varepsilon\to 0} \frac{1}{\varepsilon}\psp\setn{x \le \rvX < x+\varepsilon}
    \\\ppxy(x,y) &=& \lim_{\varepsilon\to 0} \frac{1}{\varepsilon}\psp\setn{x \le \rvX < x+\varepsilon \land y \le\rvY < y+\varepsilon}
  \end{array}}$
  }
\end{proposition}
\begin{proof}
\begin{align*}
  \ppx(x)
    &\eqd \ddx\pcx(x)
    && \text{by definition of $\ppx$}
    && \text{\xref{def:pdf}}
  \\&=    \lim_{\varepsilon\to 0} \frac{1}{\varepsilon}\psp\set{x\in\R}{x \le \rvX < x+\varepsilon}
    && \text{by definition of $\ddx$}
    && \text{\xref{def:ddx}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}
\label{thm:pdfcdf}
%---------------------------------------
Let $\ps$ be a probability space,
$\rvX$ be a random variable, and $(a,b)$ a real interval.
\thmbox{
  \brb{\begin{array}{FMD}
    (1).& $\pcx(x)$ is \prope{continuous} & OR \\
    (2).& $\ppx(x)$ is \prope{continuous}
  \end{array}}
  \implies
  \brb{\begin{array}{rc>{\ds}lc>{\ds}l}
    \psp\setn{a<\rvX\le b}
      &=& \pcx(b) - \pcx(a)
      &=& \int_a^b \ppx(x) \dx
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \psp\setn{a<\rvX\le b}
    &= \psp\setn{\rvX\le b} - \psp\setn{\rvX <  a}
    && \text{by \thme{sum of products}} && \text{\xref{thm:psp_sop}}
  \\&= \psp\setn{\rvX\le b} - \psp\setn{\rvX\le a}
    && \text{by \prope{continuity} hypothesis}
  \\&\eqd \pcx(b) - \pcx(a)
    && \text{by definition of $\pcx$} && \text{\xref{def:cdf}}
  \\
  \\
  \int_a^b \ppx(x) \dx
    &\eqd \int_a^b \brs{\ddx\pcx(x)} \dx
    && \text{by definition of $\ppx$} && \text{\xref{def:pdf}}
  \\&= \brlr{\pcx(x)}_{x=b} - \brlr{\pcx(x)}_{x=a}
    && \text{by \thme{Fundamental theorem of calculus}}
  \\&= \pcx(b) - \pcx(a)
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $\ps$ be a \structe{probability space},
$\rvX$ be a \fncte{random variable}, and $\intoo{a}{b}$ a \structe{real interval}.
\thmbox{\begin{array}{>{\ds}l}
  \psp\setn{a\le\rvX<b}
     =  \int_a^b \ppx(x) \dx
     =  \int_{-\infty}^b \pcx(x) \dx - \int_{-\infty}^a \pcx(x) \dx
\end{array}}
\end{theorem}

The properties of the pdf follow closely the properties of \fncte{measure} $\psp$.
%---------------------------------------
\begin{theorem}
\footnote{
  \citerpgc{papoulis1990}{158}{0137116985}{Auxiliary Variable}
  }
%---------------------------------------
\thmbox{
  \brb{\begin{array}{FMD}
    (A).& $\pcx(x)$ is \prope{continuous} & OR \\
    (B).& $\ppx(x)$ is \prope{continuous}
  \end{array}}
  \quad\implies\quad
  \brb{\begin{array}{Frc>{\ds}lD}
    (1).& \ppxcy(x|y) &=& \frac{\ppxy(x,y)}{\ppy(y)} & and \\
    (2).& \ppx(x)     &=& \int_{y\in\R}\ppxy(x,y) \dy
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \ppxcy(x|y)
    &\eqd \ddx\pcxcy(x|y)
    && \text{by definition of $\pcx$}
    && \text{\xref{def:conprob}}
  \\&\eqd \lim_{\varepsilon\to0} \frac{1}{\varepsilon} \psp\set{x\le\rvX < x+\varepsilon}{\rvY=y}
    && \text{by definition of $\ddx$}
    && \text{\xref{def:ddx}}
  \\&\eqd \lim_{\varepsilon\to0}
          \frac{1}{\varepsilon}
          \frac{\ds\psp\setn{(x\le\rvX < x+\varepsilon) \land (\rvY=y)}}
               {\ds\psp\setn{\rvY=y}}
    && \text{by definition of $\psP\set{\setA}{\setB}$}
    && \text{\xref{def:conprob}}
  \\&= \lim_{\varepsilon\to0}
       \frac{1}{\varepsilon}
       \frac{\ds\psp\setn{(x\le\rvX < x+\varepsilon) \land (y\le\rvY<y+\varepsilon)}}
            {\ds\psp\setn{y\le\rvY<y+\varepsilon}}
    && \text{by \prope{continuity} hypothesis}
  \\&=  \frac{\ds\lim_{\varepsilon\to0} \frac{1}{\varepsilon}\psp\setn{(x\le\rvX < x+\varepsilon) \land (y\le\rvY<y+\varepsilon)}}
             {\ds\lim_{\varepsilon\to0}                      \psp\setn{y\le\rvY<y+\varepsilon}}
    && \text{by property of $\ds\lim_{\varepsilon\to0}$}
  \\&=    \frac{\ppxy(x,y)}{\ppy(y)}
    && \text{by \prefp{prop:pdfddx}}
\\
\\
  \int_{y\in\R}\ppxy(x,y)\dy
    &\eqd \int_{y\in\R} \brs{ \ddy\ddx\pcxy(x,y) } \dy
    && \text{by definition of $\ppx$}
    && \text{\xref{def:pdf}}
  \\&= \ddx\pcxy(x,y)
  \\&\eqd \lim_{\varepsilon\to0} \frac{1}{\varepsilon}
          \int_{y\in\R} \psp\setn{x\le\rvX<x+\varepsilon, y\le\rvY<y+\varepsilon} \dy
    && \text{by definition of $\ddx$}
    && \text{\xref{def:ddx}}
  \\&=    \lim_{\varepsilon\to0} \frac{1}{\varepsilon} \psp\setn{x\le\rvX<x+\varepsilon}
  \\&= \ppx(x)
    && \text{by \prefp{prop:pdfddx}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}
\label{thm:cdf}
%---------------------------------------
\thmbox{\begin{array}{rcl}
  \pcx(\sup\R) &=& 1 \\
  \pcx(\inf\R) &=& 0
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \pcx(\sup\R)
    &\eqd \psp\setn{\rvX\le\sup\R}
    && \text{by definition of $\pcx$}
    &&\text{\xref{def:cdf}}
  \\&=    1
    %&& \text{by definition of $\psp$}
    %&&\text{\xref{def:psp}}
  \\
  \pcx(\inf\R)
    &\eqd \psp\setn{\rvX\le\inf\R}
    && \text{by definition of $\pcx$} &&\text{\xref{def:cdf}}
  \\&=    0
    %&& \text{by definition of $\psp$}
    %&&\text{\xref{def:psp}}
\end{align*}
\end{proof}

The properties of the pdf follow closely the properties of measure $\psp$.
%---------------------------------------
\begin{theorem}
\label{thm:conpdf}
%---------------------------------------
\thmbox{\begin{array}{rc>{\ds}l rc>{\ds}l}
  \pcxcy(x|y) &=& \frac{\ddy\pcxy(x,y)}{\ppy(y)}  &\qquad
  \ppxcy(x|y) &=& \frac{\ppxy(x,y)}{\ppy(y)}  \\
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \pcxcy(x|y)
    &\eqd \pPc{\rvX\le x}{\rvY=y}
    &&    \text{by definition of $\pcxcy$}              &&\text{\xref{def:concdf}}
  \\&\eqd \frac{\psp\set{\rvX\le x}{\rvY=y}}{\psp\setn{\rvY=y}}
    &&    \text{by definition of $\psp\setn{\rvX|\rvY}$}         &&\text{\xref{def:conP}}
  \\&=    \lim_{\varepsilon\to0}\frac{\psp\set{\rvX\le x}{y<\rvY\le y+\varepsilon}}
                                         {\psp\setn{y<\rvY\le y+\varepsilon}}
  \\&=\mathrlap{\lim_{\varepsilon\to0}\frac{\brs{\psp\set{\rvX\le x}{\rvY\le y+\varepsilon}-\psp\set{\rvX\le x}{\rvY\le y}}/\varepsilon}
                                         {\brs{\psp\setn{\rvY\le y+\varepsilon}-\psp\setn{\rvY\le y}}/\varepsilon}}
  \\&\eqd \lim_{\varepsilon\to0}\frac{\brs{\pcxy(x,y+\varepsilon)-\pcxy(x,y)}/\varepsilon}
                                         {\brs{\pcy(y+\varepsilon)-\pcy(y)}/\varepsilon}
    &&    \text{by definition of $\pcxy$}                  &&\text{\xref{def:jointcdf}}
  \\&\eqd \frac{\ddy\pcxy(x,y)}{\ddy\pcy(y)}
    &&    \text{by definition of $\ddy\ff(y)$}
  \\&\eqd \frac{\ddy\pcxy(x,y)}{\ppy(y)}
    &&    \text{by definition of $\ppy$}  &&\text{\xref{def:pdf}}
  \\&=    \frac{\ddy\pcxy(x,y)}{\ppy(y)}
    &&    \text{because $y$ is fixed}
  \\
  \\
  \ppxcy(x|y)
    &\eqd \ddx\pcxcy(x|y)                     &&\text{by definition of $\ppxcy$}                  &&\text{\xref{def:conpdf}}
  \\&=    \ddx\frac{\ddy\pcxy(x,y)}{\ppy(y)}  &&\text{by previous result}
  \\&=    \frac{\ddx\ddy\pcxy(x,y)}{\ppy(y)}  &&\text{because $\ppy(y)$ is not a function of $x$}
  \\&\eqd \frac{\ppxy(x,y)}{\ppy(y)}          &&\text{by definition of $\ppxy(x,y)$}              &&\text{\xref{def:jointpdf}}
\end{align*}

\begin{align*}
  %\ppxcy(x|y)
  %  &\eqd \lim_{h\to0} \frac{1}{h} \psp\setn{x\le\rvX < x+h | Y=y}
  %  &&    \text{by definition of $\ppxcy(x|y)$}
  %  &&    \text{\xref{def:conpdf}}
  %\\&\eqd \lim_{\varepsilon\to0} \frac{1}{\varepsilon} \frac{\psp\setn{(x\le\rvX < x+\varepsilon) \land (Y=y)}}{\psp\setn{Y=y}}
  %  &&    \text{by definition of $\psp\setn{\rvX|\rvY}$}
  %  &&    \text{\xref{def:conP}}
  %\\&=    \lim_{\varepsilon\to0} \frac{1}{\varepsilon} \frac{\psp\setn{(x\le\rvX < x+\varepsilon) \land (y\le Y<y+\varepsilon)}}{\psp\setn{y\le Y<y+\varepsilon}}
  %\\&\eqd \frac{\ppxy(x,y)}{\ppy(y)}
  %  \qquad\mathrlap{\text{by definitions of $\ppxy$ \xref{def:jointpdf} and $\ppy$ \xref{def:pdf}}}
  %\\
  %\int_y\ppxy(x,y)\dy
  %  &\eqd \lim_{\varepsilon\to0} \frac{1}{\varepsilon} \int_y \psp\setn{x\le X<x+\varepsilon \land y\le Y<y+\varepsilon} \dy
  %  &&    \text{by definition of $\ppxy$}
  %  &&    \text{\xref{def:jointpdf}}
  %\\&=    \lim_{\varepsilon\to0} \frac{1}{\varepsilon} \psp\setn{x\le X<x+\varepsilon}
  %  &&    \text{by \thme{sum of products}}
  %  &&    \text{\xref{thm:psp_sop}}
  %\\&\eqd \ppx(x)
  %  &&    \text{by definition of $\ppx$}
  %  &&    \text{\xref{def:psp}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}
\label{thm:pdf1}
%---------------------------------------
Let $\ps$ be a probability space.
\thmbox{\begin{array}{>{\ds}rc>{\ds}l C| >{\ds}rc>{\ds}l}
    \int_{x\in\R} \ppx(x)    \dx &=& 1       &                  & \int_{x\in\R} \ppxcy(x|y) \dx               &=& 1\\
    \int_{y\in\R} \ppxy(x,y) \dy &=& \ppx(x) & \forall x\in\pso & \int_{x\in\R}\int_{y\in\R}\ppxy(x,y) \dy\dx &=& 1
  \end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \int_{\R} \ppx(x) \dx
    &= \pcx(\sup\R) - \pcx(\inf\R)
    && \text{by \prefp{thm:pdfcdf}}
  \\&= 1 - 0
  \\&= 1
    && \mathrlap{\text{because $0$ is the additive identity element in $\fieldR$}}
  \\
  \int_{x\in\R} \ppxcy(x|y) \dx
    &\eqd \int_{x\in\R} \ddx\pcxcy(x|y) \dx
    &&    \text{by definition of $\ppxcy(x|y)$ \xref{def:conpdf}}
  \\&=    \pcxcy(\sup\R|y) - \pcxcy(\inf\R|y)
    &&    \text{by \thme{Fundamental theorem of calculus}}
  \\&=    1-0
  \\&=    1
    && \mathrlap{\text{because $0$ is the additive identity element in $\fieldR$}}
  \\
  \int_{y\in\R} \ppxy(x,y) \dy
    &= \int_{y\in\R} \ppyx(y,x) \dy
  \\&= \int_{y\in\R} \ppycx(y|x)\ppx(x) \dy  && \text{by \prefp{thm:conpdf}}
  \\&= \ppx(x)\int_{y\in\R} \ppycx(y|x) \dy  && \text{because $\ppx(x)$ is not a function of $y$}
  \\&= \ppx(x)\cdot1                         && \text{by previous result}
  \\&= \ppx(x)                               && \mathrlap{\text{because $1$ is the multiplicative identity element in $\fieldR$}}
  \\
  \int_{x\in\R}\int_{y\in\R} \ppxy(x,y) \dy\dx
    &= \int_{x\in\R}\ppx(x)\dx               && \text{by previous result}
  \\&= 1                                     && \text{by previous result}
\end{align*}
\end{proof}

%=======================================
\section{Discrete distributions}
%=======================================
\begin{figure}
\centering
\begin{tabular}{*{11}{c}}
  $\scriptstyle \psp\setn{ 2}=\frac{1}{36}$ &
  $\scriptstyle \psp\setn{ 3}=\frac{2}{36}$ &
  $\scriptstyle \psp\setn{ 4}=\frac{3}{36}$ &
  $\scriptstyle \psp\setn{ 5}=\frac{4}{36}$ &
  $\scriptstyle \psp\setn{ 6}=\frac{5}{36}$ &
  $\scriptstyle \psp\setn{ 7}=\frac{6}{36}$ &
  $\scriptstyle \psp\setn{ 8}=\frac{5}{36}$ &
  $\scriptstyle \psp\setn{ 8}=\frac{4}{36}$ &
  $\scriptstyle \psp\setn{10}=\frac{3}{36}$ &
  $\scriptstyle \psp\setn{11}=\frac{2}{36}$ &
  $\scriptstyle \psp\setn{12}=\frac{1}{36}$
  \\                  &                  &                  &                  &                  & \diceF\diceA &                  &                  &                  &                  &
  \\                  &                  &                  &                  & \diceE\diceA & \diceE\diceB & \diceF\diceB &                  &                  &                  &
  \\                  &                  &                  & \diceD\diceA & \diceD\diceB & \diceD\diceC & \diceE\diceC & \diceF\diceC &                  &                  &
  \\                  &                  & \diceC\diceA & \diceC\diceB & \diceC\diceC & \diceC\diceD & \diceD\diceD & \diceE\diceD & \diceF\diceD &                  &
  \\                  & \diceB\diceA & \diceB\diceB & \diceB\diceC & \diceB\diceD & \diceB\diceE & \diceC\diceE & \diceD\diceE & \diceE\diceE & \diceF\diceE &
  \\ \diceA\diceA & \diceA\diceB & \diceA\diceC & \diceA\diceD & \diceA\diceE & \diceA\diceF & \diceB\diceF & \diceC\diceF & \diceD\diceF & \diceE\diceF & \diceF\diceF
\end{tabular}
  \caption{
    Probability distribution for two dice (see \prefp{ex:two_dice})
    \label{fig:two_dice}
    }
\end{figure}
%---------------------------------------
\begin{example}
\label{ex:two_dice}
%---------------------------------------
Suppose we throw two ``fair" dice and want to know the probabilities of their sum.
Let $\rvX$ represent the sum  of the face values of the two dice.
The resulting probability distribution is illustrated in \prefpp{fig:two_dice}\cittr{osgood}
and probability space is as follows:

\exbox{\begin{array}{ll}
  \pso &= \setn{\text{\diceA\diceA,\; \diceA\diceB,\; \diceA\diceC,\;\ldots,\;\diceF\diceF} }
  \\
  \pse &= \left\{ \pset{\set{X=n}{n=2,3,\ldots,10,11,\text{ or } 12}} \right\}
  \\
  \psp(e) &= \frac{1}{36} \seto{e}
\end{array}}

\end{example}

%=======================================
\section{Continuous distributions}
%=======================================
%=======================================
\subsection{Uniform distribution}
%=======================================
%---------------------------------------
\begin{definition}
\label{def:uniform}
%---------------------------------------
The \fnctd{uniform distribution} $\ppx(x)$ is defined as
\defbox{
  \ppx(x) \eqd \brb{\begin{array}{lM}
                      1  & for $0< x \le 1$\\
                      0  & otherwise
                    \end{array}}
  }
\end{definition}

Note that although ``simple" in form, in light of \thme{Wold's Theorem}, the
value of the \fncte{uniform distribution} should \emph{not} be taken lightly.

%=======================================
\subsection{Gaussian distribution}
\index{Gaussian distribution}
\index{normal distribution}
%=======================================
\qboxnpqt
  {Bernard A. Lippmann as told by Henri Poincar\'e
  \index{Lippmann, Bernard A.} \index{Poincar\'e, Henri}
  \index{quotes!Lippmann, Bernard A.} \index{quotes!Poincar\'e, Henri}
   \footnotemark
  }
  {../common/people/poincare.jpg}
  {Tout le monde y croit cependant, me disait un jour M. Lippmann,
   car les exp\'erimentateurs s'irnaginent q\`ue c'est un th\'eor\`eme
   de math\'ematiques, et les math\'ematiciens que c'est un fait
   exp\'erimental.}
  {Everyone believes in it [(the normal distribution)] however,
   said to me one day Mr. Lippmann, because the experimenters imagine that
   it is a theorem of mathematics,
   and mathematicians that it is an experimental fact.}
  \citetblt{
    quote:       & \citerp{poincare_calc_prob}{171} \\
    translation: & assisted by \href{http://translate.google.com/}{Google Translate} \\
    image:       &
    }



\begin{figure}[ht]
   \begin{center}
   \includegraphics[height=8cm, width=12cm, clip=]{../common/pdf_norm.eps}
   \end{center}
\caption{
  Gaussian pdf with $\mu=0$ and $\sigma\in[0.1,2]$.
  \label{fig:pdf_norm}
  }
\end{figure}

%---------------------------------------
\begin{definition}
%---------------------------------------
The \hid{Gaussian distribution} (or \hid{normal distribution} has pdf
\defbox{
  \ppx(x) \eqd \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}
  }
A random variable $\rvX$ with this distribution is denoted
\defbox{ X \sim \pN{\mu}{\sigma^2} }
The function $Q(x)$ is defined as the area under a
Gaussian PDF with zero mean and variance equal to one
from $x$ to infinity such that
\defbox{Q(x) \eqd \frac{1}{\sqrt{2\pi}} \int_x^\infty e^{\frac{-u^2}{2}} \du }
\end{definition}

%=======================================
\subsection{Gamma distribution}
%=======================================
%---------------------------------------
\begin{definition}
\label{def:Gamma(b)}
\index{gamma function}
\index{$\Gamma(b)$}
\footnote{
  \citerp{papoulis}{79},
  \citerp{ross}{222}
  }
%---------------------------------------
Let $b\in\R$. The \fnctd{gamma function} $\Gamma(b)$ is
\defbox{ \Gamma(b) \eqd \int_0^\infty x^{b-1} e^{-x} \dx  }
\end{definition}

%---------------------------------------
\begin{proposition}
\footnote{
  \citerp{ross}{223}
  }
%---------------------------------------
Let $b\in\R$ and $n\in\Zp$.
\propbox{\begin{array}{rcl}
  \Gamma(b) &=& (b-1)\Gamma(b-1)  \\
  \Gamma(n) &=& (n-1)!
  \end{array}}
\end{proposition}
\begin{proof}
Let
\[\begin{array}{rcl@{\hspace{2cm}}rcl}
  u   &=& x^{b-1}   & \du &=& (b-1)x^{b-1} \dx \\
  \dv &=& e^{-x}\dx & v   &=& -e^{-x}
\end{array}\]

\begin{eqnarray*}
  \Gamma(b)
    &\eqd& \int_0^\infty x^{b-1} e^{-x} \dx
  \\&=&    \int_{x=0}^\infty u \dv
  \\&=&    uv|_{x=0}^\infty - \int_{x=0}^\infty v \du
  \\&=&    -x^{b-1} e^{-x}|_{x=0}^\infty + (b-1)\int_{x=0}^\infty e^{-x} x^{b-1}\dx
  \\&=&    (-0+0) + (b-1)\Gamma(b-1)
\end{eqnarray*}

Note that
  \[ \Gamma(1)
       = \int_0^\infty x^{1-1} e^{-x} \dx
       = \int_0^\infty e^{-x} \dx
       = \left. -e^{-x} \right|_0^\infty
       = -0 + 1
       = 1
  \]
\begin{eqnarray*}
  \Gamma(n)
    &=& (n-1)\Gamma(n-1)
  \\&=& (n-1)(n-2)\Gamma(n-2)
  \\&=& (n-1)(n-2)(n-3)\Gamma(n-3)
  \\&\vdots&
  \\&=& (n-1)(n-2)(n-3)\cdots(1)\Gamma(1)
  \\&=& (n-1)(n-2)(n-3)\cdots(1)
  \\&\eqd& (n-1)!
\end{eqnarray*}
\end{proof}

%---------------------------------------
\begin{definition}
%---------------------------------------
A \fnctd{Gamma distribution} $(b,\lambda)$ has pdf
\defbox{ \ppx(x) \eqd \frac{\lambda}{\Gamma(b)} e^{-\lambda x}(\lambda x)^{b-1} }
\end{definition}

%---------------------------------------
\begin{theorem}
\label{thm:Gamma_X+Y}
\footnote{
  \citerp{ross}{266}
  }
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s on a \structe{probability space} $\ps$.
\thmbox{
  \brb{\begin{array}{FMD}
      (A).& $\rvX$ and $\rvY$ are \prope{independent}                  & and
    \\(B).& $\rvX$ has \prope{Gamma distribution} $\opair{a}{\lambda}$ & and
    \\(C).& $\rvY$ has \prope{Gamma distribution} $\opair{b}{\lambda}$ & and
    \\(D).& $\rvZ\eqd\rvX+\rvY$
  \end{array}}
  \implies
  \brb{\begin{array}{M}
    $\rvZ$ has Gamma distribution\\
    $\opair{a+b}{\lambda}$.
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \ppz(z)
    &= \ppx(z)\conv\ppy(z)
  \\&= \int_{u\in\R} \ppx(u)\ppy(z-u) \du
    && \text{by definition of \ope{convolution}}
    && \text{\xref{def:conv}}
  \\&= \int_0^z
       \frac{1}{\Gamma(a)}\lambda e^{-\lambda u}(\lambda u)^{a-1}
       \frac{1}{\Gamma(b)}\lambda e^{-\lambda(z-u)}(\lambda(z-u))^{b-1}
       \du
  \\&= \frac{1}{\Gamma(a)}\frac{1}{\Gamma(b)}e^{-\lambda z}\lambda^{1+1+a-1+b-1}
       \int_0^z  u^{a-1}(z-u)^{b-1}  \du
  \\&= \frac{1}{\Gamma(a)}\frac{1}{\Gamma(b)}\lambda e^{-\lambda z}\lambda^{a+b-1}
       \int_0^1  (vz)^{a-1}(z-vz)^{b-1}  z\dv
  \\&= \frac{1}{\Gamma(a)}\frac{1}{\Gamma(b)}\lambda e^{-\lambda z}\lambda^{a+b-1}
       z^{a-1+b-1+1}
       \int_0^1  v^{a-1}(1-v)^{b-1}  \dv
  \\&= \brs{
       \frac{1}{\Gamma(a)}\frac{1}{\Gamma(b)}
       \int_0^1  v^{a-1}(1-v)^{b-1}  \dv
       }
       \lambda e^{-\lambda z}(\lambda z)^{a+b-1}
  \\&= C\lambda e^{-\lambda z}(\lambda z)^{a+b-1}
    && \text{where $C$ is some constant}
  \\&= \frac{\lambda}{\Gamma(a+b)} e^{-\lambda z}(\lambda z)^{a+b-1}
    &&\mathrlap{\text{$C$ must be the value that makes $\int_z\ppz(z)=1$}}
\\\implies & \text{$\ppz(z)$ is a $(a+b,\lambda)$ Gamma distribution}
\end{align*}
\end{proof}

%=======================================
\subsection{Chi-squared distributions}
%=======================================
%---------------------------------------
\begin{definition}
\label{def:pdf_chi}
\index{Chi-squared distribution}
\footnote{
  \citerp{proakis}{41},
  \citerpgc{papoulis1990}{219}{0137116985}{7-4 Special Distributions of Statistics, (7-78)}
  }
%---------------------------------------
Let $\pp(x)$ be a \fncte{probability density function} on a \structe{probability space} $\ps$.
\defboxt{
  $\pp(x)$ is a \fnctd{chi-square distribution} if
  \\\indentx$\ds
    \pp(x) \eqd 
    \brb{\begin{array}{>{\ds}lM}
      0                                                            &  if $x<   0$ \\
      \frac{1}{\sqrt{2\pi\sigma^2x}}\exp\brs{-\frac{x}{2\sigma^2}} &  if $x\ge 0$
    \end{array}}$
    \qquad
    for $\sigma>0$
  }
\end{definition}

%A random variable $\rvY$ with \fncte{chi-squared distribution} is equivalent
%to the square of random variable with zero-mean Gaussian distribution.
%Thus if you wanted to generate a random value with chi-square distribution,
%you could simply square the output of a Gaussian noise generator.
%---------------------------------------
\begin{theorem}
\index{chi-squared distribution}
\index{Gamma distribution}
\label{thm:Gamma=X^2}
\footnote{
  \citerp{ross}{267}
  }
%---------------------------------------
\mbox{}\\
\thmboxt{
  The following distributions are equivalent:
  \\\indentx$\begin{array}{FMD}
      (1).& chi-squared distribution                                            & and
    \\(2).& distribution of $\rvX^2$ where $\rvX\sim\pN{0}{\sigma^2}$           & and
    \\(3).& Gamma distribution $\left(\frac{1}{2},\frac{1}{2\sigma^2} \right)$  &
  \end{array}$
  }
\end{theorem}
\begin{proof}
\begin{enumerate}

\item Proof that $\rvX^2$ has chi-squared distribution:
\begin{align*}
  \ppy(y)
    &= \left.\left.\frac{1}{2\sqrt{y}}\right[
       \ppx(-\sqrt{y}) + \ppx( \sqrt{y}) \right]
    && \text{by \prefp{prop:Y=X^2}}
  \\&= \left.\left.\frac{1}{2\sqrt{y}}\right[
       \frac{1}{\sqrt{2\pi\sigma^2}} \exp{-\frac{(-\sqrt{y}-0)^2}{2\sigma^2}}+
       \frac{1}{\sqrt{2\pi\sigma^2}} \exp{-\frac{(+\sqrt{y}-0)^2}{2\sigma^2}}
       \right]
  \\&= \left.\left.\frac{1}{2\sqrt{y}}\right[
       2\frac{1}{\sqrt{2\pi\sigma^2}} \exp{-\frac{y}{2\sigma^2}}+
       \right]
  \\&= \frac{1}{\sqrt{2\pi \sigma^2 y}} \exp{-\frac{y}{2\sigma^2}}
\end{align*}

\item Proof that chi-distribution is a Gamma distribution $(b,\lambda)$:
\begin{align*}
  b &\eqd& \frac{1}{2} \\
  \lambda &\eqd \frac{1}{2\sigma^2} \\
\\
  \frac{1}{\sqrt{2\pi\sigma^2y}}\exp{-\frac{y}{2\sigma^2}}
    &= \frac{1}{\sqrt{\pi}}\lambda^{1/2} \lambda^{1/2} (\lambda y)^{-1/2} e^{-\lambda y}
  \\&= \frac{\lambda}{\sqrt{\pi}} (\lambda y)^{b-1} e^{-\lambda y}
\end{align*}

\end{enumerate}
\end{proof}

%---------------------------------------
\begin{definition}
\index{Chi-squared distribution with $n$ degrees of freedom}
\footnote{
  \citerp{proakis}{41}
  }
%---------------------------------------
The \fnctd{Chi-squared distribution with $n$ degrees of freedom} has pdf
\defbox{
  \ppy(y) \eqd \left\{\begin{array}{ll}
    0 &:y<0 \\ \ds
    \frac{1}{2\sigma^2\Gamma(n/2)}
               \left(\frac{y}{2\sigma^2}\right)^{\frac{n}{2}-1}
               \exp{-\frac{y}{2\sigma^2}}
    &:y\ge 0
    \end{array}\right.
  }
\end{definition}

%---------------------------------------
\begin{theorem}
\index{chi-squared distribution with $n$ degrees of freedom}
\index{Gamma distribution}
\footnote{
  \citerp{ross}{267}
  }
%---------------------------------------
The following distributions are equivalent:
\begin{enumerate}
  \item chi-squared distribution with $n$ degrees of freedom
  \item the distribution of $\ds \sum_{k=1}^n X_k^2$ where
        $\set{X_k}{X_k\sim\pN{0}{\sigma^2}, k=1,2,\ldots,n}$
        are independent random variables.
  \item Gamma distribution $\left(\frac{n}{2}, \frac{1}{2\sigma^2} \right)$.
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
\item Prove chi-squared distribution with $n$ degrees of freedom is the
      Gamma distribution $\left(\frac{n}{2}, \frac{1}{2\sigma^2} \right)$:
  \begin{eqnarray*}
    \lambda &\eqd& \frac{1}{2\sigma^2} \\
    b       &\eqd& \frac{1}{2} \\
    \frac{1}{2\sigma^2\Gamma(n/2)}
                 \left(\frac{y}{2\sigma^2}\right)^{\frac{n}{2}-1}
                 \exp{-\frac{y}{2\sigma^2}}
      &=& \frac{\lambda}{\Gamma(nb)}
          \left(\lambda y \right)^{nb-1}
          \exp{-\lambda y}
  \end{eqnarray*}

\item Prove $\sum_{k=1}^n X^2$ is Gamma $\left(\frac{n}{2}, \frac{1}{2\sigma^2} \right)$:
  \begin{enumerate}
    \item By Theorem~\ref{thm:Gamma=X^2}, $\rvX_k$ has Gamma distribution
          $\left(\frac{1}{2}, \frac{1}{2\sigma^2} \right)$.
    \item By Theorem~\ref{thm:Gamma_X+Y}, $\sum_{k=1}^n X_k^2$ has
          distribution $\left(\frac{n}{2}, \frac{1}{2\sigma^2} \right)$.
  \end{enumerate}
\end{enumerate}
\end{proof}

%---------------------------------------
\begin{definition}
\index{Noncentral chi-square distribution}
\footnote{
  \citerp{proakis}{42}
  }
%---------------------------------------
A \fnctd{noncentral chi-square distribution} $(\mu,\sigma^2)$
has pdf
\defbox{
  \ppy(y)
  = \frac{1}{\sqrt{2\pi\sigma^2y}} \exp{\frac{y+\mu^2 }{-2\sigma^2}}
          \cosh{\frac{ \mu\sqrt{y}}{\sigma^2}}
  }
\end{definition}

%---------------------------------------
\begin{theorem}
%---------------------------------------
\thmboxt{
  The following distributions are equivalent:
  \\\qquad$\begin{array}{FM}
    (1).& \fncte{non-central chi-squared distribution} $(\mu,\sigma^2)$\\
    (2).& distribution of $\rvX^2$ where $\rvX\sim\pN{\mu}{\sigma^2}$
  \end{array}$
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
\item Prove $\rvY=X^2$ has a non-central chi-squared distribution:
\begin{eqnarray*}
  \ppy(y)
    &=& \left.\left.\frac{1}{2\sqrt{y}}\right[
        \ppx(-\sqrt{y}) + \ppx( \sqrt{y}) \right]
        \hspace{1cm}\mbox{by Proposition~\ref{prop:Y=X^2} page~\pageref{prop:Y=X^2}}
  \\&=& \left.\left.\frac{1}{2\sqrt{y}}\right[
        \frac{1}{\sqrt{2\pi\sigma^2}} \exp{-\frac{(-\sqrt{y}-\mu)^2}{2\sigma^2}}+
        \frac{1}{\sqrt{2\pi\sigma^2}} \exp{-\frac{(+\sqrt{y}-\mu)^2}{2\sigma^2}}
        \right]
  \\&=& \left.\left.\frac{1}{2\sqrt{y}}\right[
        \frac{1}{\sqrt{2\pi\sigma^2}} \exp{-\frac{y+\mu^2 }{2\sigma^2}} \exp{\frac{-2\mu\sqrt{y}}{2\sigma^2}}+
        \frac{1}{\sqrt{2\pi\sigma^2}} \exp{-\frac{y+\mu^2 }{2\sigma^2}} \exp{\frac{ 2\mu\sqrt{y}}{2\sigma^2}}
        \right]
  \\&=& \frac{1}{\sqrt{2\pi\sigma^2y}} \exp{-\frac{y+\mu^2 }{2\sigma^2}}
        \frac{1}{2}
        \left[
          \exp{\frac{ 2\mu\sqrt{y}}{2\sigma^2}} +
          \exp{\frac{-2\mu\sqrt{y}}{2\sigma^2}}
        \right]
  \\&=& \frac{1}{\sqrt{2\pi\sigma^2y}} \exp{\frac{y+\mu^2 }{-2\sigma^2}}
          \cosh{\frac{ \mu\sqrt{y}}{\sigma^2}}
\end{eqnarray*}
\end{enumerate}
\end{proof}

%---------------------------------------
\begin{definition}
\footnote{
  \citerp{proakis}{43}
  }
%---------------------------------------
The \fnctd{$\alpha$th-order modified Bessel function of the first kind}
$I_\alpha(x)$ is
\defbox{
  I_\alpha(x) = \sum_{k=0}^\infty
  \frac{1}{k!\Gamma(\alpha+k+1)} \left(\frac{x}{2}\right)^{\alpha+2k}
  }
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citerp{proakis}{43}
  }
%---------------------------------------
The \fnctd{noncentral chi-square with $n$-degrees of freedom} distribution has
pdf
\defbox{
  \ppy(y) =
  \frac{1}{2\sigma^2}
  \left(\frac{y}{s^2}\right)^\frac{n-2}{4}
  \exp{\frac{y+s^2}{-2\sigma^2}}
  I_{n/2-1}\left(\sqrt{y}\frac{s}{\sigma^2}\right)
  \hspace{1cm}\mbox{where }
  s^2 \eqd \sum_{k=1}^n \mu_k^2
  }
\end{definition}

%=======================================
\subsection{Radial distributions}
%=======================================
%---------------------------------------
\begin{definition}
\index{Rayleigh distribution}
\footnote{
  \citerp{proakis}{44}
  }
%---------------------------------------
The \fnctd{Rayleigh distribution} is the pdf
\defbox{
  \ppr(r) = \left\{\begin{array}{l@{\hspace{4ex}\mbox{for}\hspace{4ex}}l}
  0 & r<0 \\
  \frac{r}{\sigma^2}\exp{-\frac{r^2}{2\sigma^2}}
  & r\ge 0
  \end{array}\right.
  }
\end{definition}

Note that by Proposition~\ref{prop:XY->RT_n},
this distribution is equivalent to the distribution of
$R=\sqrt{X^2+Y^2}$ where $\rvX$ and $\rvY$ are independent random variables
each with distribution $\pN{0}{\sigma^2}$.

%---------------------------------------
\begin{definition}
\index{Rice distribution}
\footnote{
  \citerp{proakis}{46}
  }
%---------------------------------------
The \fnctd{Rice distribution} is the pdf
\defbox{
  \ppr(r) = \left\{\begin{array}{l@{\hspace{4ex}\mbox{for}\hspace{4ex}}l}
  0 & r<0 \\
  \frac{r}{\sigma^2}\exp{\frac{r^2+s^2}{-2\sigma^2}}
  I_o\left(\frac{rs}{\sigma^2}\right)
  & r\ge 0
  \end{array}\right.
  }
\end{definition}





%
%
%
%%=======================================
%\subsection{Nakagami $m$-distribution}
%\index{Nakagami $m$-distribution}
%\citep{proakis}{47}
%%=======================================

%=======================================
\section{Joint Gaussian distributions}
%=======================================
\begin{figure}
\begin{tabular}{*{4}{c}}
  \includegraphics[width=\tw/4-5mm,clip=true]{../common/normxy_00.eps} &
  \includegraphics[width=\tw/4-5mm,clip=true]{../common/normxy_50.eps} &
  \includegraphics[width=\tw/4-5mm,clip=true]{../common/normxy_80.eps} &
  \includegraphics[width=\tw/4-5mm,clip=true]{../common/normxy_95.eps}
\end{tabular}
\caption{
  \fncte{Joint Gaussian distribution}s $\ppxy(x,y)$ with varying correlations
  \label{fig:psub_joint_gaussian}
  }
\end{figure}

%---------------------------------------
\begin{definition}[Joint Gaussian pdf]
\footnote{
  \citerp{proakis}{49},
  \citerp{moon2000}{34}
  }
%---------------------------------------
\defbox{\begin{array}{rclD}
  \pp(x_1,x_2,\ldots,x_n)
    &\eqd& \ds\frac{1}{\sqrt{(2\pi)^n |\vM|}}
           \exp{-\frac{1}{2}(\vx-\pE\vx)^T\vM^{-1}(\vx-\pE\vx)}
    &      (Gaussian joint pdf)
    \\ \\
  \vx
    &\eqd& \left[\begin{array}{c}
             x_1    \\
             x_2    \\
             \vdots \\
             x_n
           \end{array}\right]
    \\
  \rvZ_k
    &\eqd& X_k - \pE X_k
    & (zero mean random variables)
    \\ \\
  \vM
    &\eqd& \brs{\begin{array}{cccc}
             \pE[\rvZ_1 \rvZ_1] & \pE[\rvZ_1 \rvZ_2] & \cdots & \pE[\rvZ_1 \rvZ_n]   \\
             \pE[\rvZ_2 \rvZ_1] & \pE[\rvZ_2 \rvZ_2] &        & \pE[\rvZ_2 \rvZ_n]   \\
             \vdots       & \vdots       & \ddots & \vdots         \\
             \pE[\rvZ_n \rvZ_1] & \pE[\rvZ_n \rvZ_2] & \cdots & \pE[\rvZ_n \rvZ_n]
           \end{array}}
    & (correlation matrix)
\end{array}}
\end{definition}


\begin{figure}[ht]
   \begin{center}
   \includegraphics[height=8cm, width=12cm, clip=]{../common/pdf_norm.eps}
   \end{center}
\caption{
  Gaussian pdf with $\mu=0$ and $\sigma\in[0.1,2]$.
  \label{fig:net_norm}
}
\end{figure}

%---------------------------------------
\begin{example}[1 variable joint Gaussian pdf]
%---------------------------------------
The \hid{Gaussian distribution} (or \hid{normal distribution} has pdf
\exbox{
  \ppx(x) \eqd \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}
  }

\begin{align*}
  t
    &= \arg_t \min_t
       \brs{
       \frac{1}{2}\int_t^\infty    \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}} +
       \frac{1}{2}\int_{-\infty}^t \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\eta)^2}{2\sigma^2}}
       }
  \\&= \arg_t \brb{\pderiv{}{t}
       \brs{
       \frac{1}{2}\int_t^\infty    \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}} +
       \frac{1}{2}\int_{-\infty}^t \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\eta)^2}{2\sigma^2}}
       }=0}
  \\&= \arg_t \brb{\frac{1}{2\sqrt{2\pi\sigma^2}}
       \brs{
       \pderiv{}{t}\int_t^\infty    e^{\frac{-(x-\mu)^2}{2\sigma^2}} +
       \pderiv{}{t}\int_{-\infty}^t e^{\frac{-(x-\eta)^2}{2\sigma^2}}
       }=0}
  \\&= \arg_t \brb{
       \brs{
       \brp{e^{\frac{-(\infty-\mu)^2}{2\sigma^2}}0 - e^{\frac{-(t-\mu)^2}{2\sigma^2}}1} +
       \brp{e^{\frac{-(t-\mu)^2}{2\sigma^2}}1 - e^{\frac{-(\infty-\mu)^2}{2\sigma^2}}0}
       }=0}
  \\&= \arg_t \brb{
       \brs{e^{\frac{-(t-\eta)^2}{2\sigma^2}}- e^{\frac{-(t-\mu)^2}{2\sigma^2}}}=0}
  \\&= \arg_t \brb{(t-\eta)^2=(t-\mu)^2}
  \\&= \frac{\mu+\eta}{2}
\end{align*}
\end{example}

%---------------------------------------
\begin{example}[2 variable joint Gaussian pdf]
\label{prop:prob_gaussian_xy}
%---------------------------------------
\exbox{\begin{array}{rc>{\ds}l}
  z_1 &\eqd& x_1 - \pE x_1 \\
  z_2 &\eqd& x_2 - \pE x_2 \\
  \abs{M} &\eqd& \abs{\pE[z_1 z_1]\pE[z_2 z_2]-\pE[z_1 z_2]\pE[z_1 z_2]}
  \\
  \pp(x_1,x_2) &\eqd&
    \frac{1}{2\pi \sqrt{\abs{M}}}
    \exp\left(
      \frac{z_1^2\pE[z_2z_2] - 2z_1z_2\pE[z_1z_2] + z_2^2\pE[z_1z_1]}
            {-2\abs{M}}
        \right)
\end{array}}
\end{example}


