%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================

%======================================
\chapter{Continuous Random Processes}
\label{app:random_processes}
%======================================
\qboxnps
  {Aristotle (384 BC -- 322 BC)
    \index{Aristotle}
    \index{quotes!Aristotle}
    \footnotemark
  }
  {../common/people/aristot.jpg}
  {A likely impossibility is always preferable to an
  unconvincing possibility.}
  \footnotetext{\begin{tabular}[t]{ll}
    quote: & \url{http://en.wikiquote.org/wiki/Aristotle} \\
    image: & \url{http://en.wikipedia.org/wiki/Aristotle}
  \end{tabular}}



%=======================================
%\section{Continuous-time random processes}
%=======================================
%=======================================
\section{Definitions}
%=======================================
%---------------------------------------
\begin{definition}
\index{random variable}
\index{random process}
\citetbl{
  \citerp{papoulis}{63},
  \citerp{papoulis}{285}
  }
%---------------------------------------
Let $\ps$ be a \structe{probability space}.\\
\defboxt{
  The function $\rvx:\pso\to\R$ is a \fnctd{random variable}.\\
  The function $\rvy:\R\times\pso\to\R$ is a \fnctd{random process}.
  }
\end{definition}

The random process $\rvx(t,\omega)$, where $t$ commonly represents time
and $\omega\in\pso$ is an outcome of an experiment,
can take on more specialized forms depending on whether
$t$ and $\omega$ are fixed or allowed to vary.
These forms are illustrated in \prefp{fig:X(t,w)}\footnote{\citerpp{papoulis}{285}{286}}
and \prefp{fig:X(t,w)graph}.

\begin{figure}[ht]\color{figcolor}
\begin{center}
   \begin{tabular}{|c||c|c|}
      \hline
         $\rvx(t,\omega)$ &  fixed $t$      & variable $t$   \\
      \hline
      \hline
         fixed    $\omega$ & number          & time function  \\
      \hline
         variable $\omega$ & random variable & random process \\
      \hline
   \end{tabular}
\caption{
   Specialized forms of a random process $\rvx(t,\omega)$
   \label{fig:X(t,w)}
   }
\end{center}
\end{figure}

\begin{figure}[ht]\color{figcolor}
\begin{center}
\includegraphics[height=8cm,width=12cm]{../common/x_tw.eps}
\end{center}
\caption{
  Example of a random process $\rvx(t,\omega)$
  \label{fig:X(t,w)graph}
}
\end{figure}


%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{papoulis1984}{216}{0070484686}{$R_{xy}(t_1,t_2)=E\brb{\rvx(t_1)\rvy^\ast(t_2)}$ (9-35)},
  }
\label{def:Rxx}
\label{def:opR}
\label{def:Rxy}
%---------------------------------------
Let $\rvx(t)$ and $\rvy(t)$ be random processes.\\
\defbox{
  \begin{array}{MlMlc>{\ds}l}
       The \fnctd{mean}                     & \pmeanx(t) & of $\rvx(t)$              is & \pmeanx(t)&\eqd& \pE\brs{\rvx(t)}                    %& \text{(\prope{mean functional})} \\
     \\The \fnctd{cross-correlation}        & \Rxy(t)    & of $\rvx(t)$ and $\rvy(t)$is & \Rxy(t,u) &\eqd& \pE\brs{\rvx(t)\rvy^\ast(u)}        %& \text{(\prope{cross-correlation bilinear functional})} \\
     \\The \fnctd{autocorrelation function} & \Rxx(t)    & of $\rvx(t)$              is & \Rxx(t,u) &\eqd& \pE\brs{\rvx(t)\rvx^\ast(u)}        %& \text{(\prope{auto-correlation bilinear functional})} \\
     \\The \fnctd{autocorrelation operator} & \opR\ff    & of $\ff(t)$               is & \opR f    &\eqd& \int_{u\in\R}\Rxx(t,u)\ff(u) \du     %& \text{(\prope{auto-correlation operator})}
  \end{array}
  }
\end{definition}

%---------------------------------------
\begin{remark}
\footnote{
  \citer{fredholm1900},
  \citerp{fredholm1903}{365},
  \citerp{michel1993}{97},
  \citerp{keener}{101}
  }
%---------------------------------------
The equation $\int_{u\in\R}\Rxx(t,u)\ff(u) \du$ is a
   \ope{Fredholm integral equation of the first kind} and
   $\Rxx(t,u)$ is the \ope{kernel} of the equation.
\end{remark}

%---------------------------------------
\section{Properties}
%---------------------------------------

%---------------------------------------
\begin{theorem}
\label{thm:Rxx_prop}
\index{cross-correlation}
\index{symmetric!conjugate}
\index{conjuage symmetric}
%---------------------------------------
Let $\fx(t)$ and $\fy(t)$ be random processes with
cross-correlation $\Rxy(t,u)$ and
let $\Rxx(t,u)$ be the auto-correlation of $\fx(t)$.
\thmbox{
\begin{array}{rcll}
   \Rxx(t,u) &=& \Rxx^\ast(u,t) & \text{(\prope{conjugate symmetric})}\\
   \Rxy(t,u) &=& \Ryx^\ast(u,t) & 
\end{array}
}
\end{theorem}
\begin{proof}
\begin{align*}
   \Rxx(t,u)
      &\eqd \pE\brs{\rvx(t) \rvx^\ast(u)}
      &=    \pE\brs{\rvx^\ast(u) \rvx(t)}
      &=    \brp{ \pE\brs{\rvx(u) \rvx^\ast(t)}}^\ast
      &\eqd \Rxx^\ast(u,t)
\\
   \Rxy(t,u)
      &\eqd \pE\brs{\rvx(t) \rvy^\ast(u)}
      &=    \pE\brs{\rvy^\ast(u) \rvx(t) }
      &=    \brp{\pE\brs{\rvy(u) \rvx^\ast(t)}}^\ast
      &\eqd \Ryx^\ast(u,t)
\end{align*}
\end{proof}


%---------------------------------------
\begin{theorem}
\index{non-negative}
\index{positive definite}
%---------------------------------------
Let $\opR:\spX\to\spX$ be an auto-correlation operator.
\thmbox{\begin{array}{lll}
  \inprod{\opR \fx}{\fx} \ge 0
    & \forall \fx\in\spX
    & \text{(\prope{non-negative})}  \\
  \inprod{\opR \fx}{\fy} = \inprod{\fx}{\opR \fy}
    & \forall \fx,\fy\in\spX
    & \text{(\prope{self-adjoint})}
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
\intertext{1. Proof that $\opR$ is non-negative:}
   \inprod{\opR \fy}{\fy}
     &= \inprod{\int_{u\in\R}\Rxx(t,u) \fy(u) \du}{\fy(t)}
     && \text{by definition of $\opR$}
     && \text{\xref{def:opR}}
   \\&= \inprod{\int_{u\in\R}\pE\brs{\fx(t)\fx^\ast(u)} \fy(u) \du}{\fy(t)}
     && \text{by definition of $\Rxx(t,u)$}
     && \text{\xref{def:Rxx}}
   \\&= \pE\brs{\inprod{\int_{u\in\R}\fx(t)\fx^\ast(u) \fy(u) \du}{\fy(t)}}
     && \text{by \prope{linearity} of $\inprodn$ and $\int$}
     && \text{\ifsxref{vsinprod}{def:inprod}}
   \\&= \pE\brs{\int_{u\in\R}\fx^\ast(u) \fy(u) \du \inprod{\fx(t)}{\fy(t)}}
     && \text{by \prope{additivity} property of $\inprodn$}
     && \text{\ifsxref{vsinprod}{def:inprod}}
   \\&= \pE\brs{\inprod{\fy(u)}{\fx(u)} \inprod{\fx(t) }{\fy(t)}}
     && \text{by local definition of $\inprodn$}
     && \text{\ifsxref{vsinprod}{def:inprod}}
   \\&= \pE\brs{\inprod{\fx(u)}{\fy(u)}^\ast \inprod{\fx(t) }{\fy(t)}}
     && \text{by \prope{conjugate symmetry} prop.}
     && \text{\ifsxref{vsinprod}{def:inprod}}
   \\&= \pE{\abs{\inprod{\fx(t) }{\fy(t)}}^2}
     && \text{by definition of $\absn$} 
     && \text{\xref{def:abs}}
   \\&\ge 0
\intertext{2. Proof that $\opR$ is self-adjoint:}
   \inprod{\brs{\opR \fx}(t)}{\fy}
     &= \inprod{\int_{u\in\R}\Rxx(t,u) \fx(u) \du}{\fy(t)}
     && \text{by definition of $\opR$}
     && \text{\xref{def:opR}}
   \\&= \int_{u\in\R}\fx(u) \inprod{\Rxx(t,u)  }{\fy(t)} \du
     && \text{by \prope{additive} property of $\inprodn$}
     && \text{\ifsxref{vsinprod}{def:inprod}}
   \\&= \int_{u\in\R}\fx(u) \inprod{\fy(t)}{\Rxx(t,u)}^\ast \du
     && \text{by \prope{conjugate symmetry} prop.}
     && \text{\ifsxref{vsinprod}{def:inprod}}
   \\&= \inprod{ \fx(u) }{\inprod{\fy(t)}{\Rxx(t,u)} }
     && \text{by local definition of $\inprodn$}
     && \text{\ifsxref{vsinprod}{def:inprod}}
   \\&= \inprod{ \fx(u) }{\int_{t\in\R}\fy(t) \Rxx^\ast(t,u)\dt }
     && \text{by local definition of $\inprodn$}
     && \text{\ifsxref{vsinprod}{def:inprod}}
   \\&= \inprod{ \fx(u) }{\int_{t\in\R}\fy(t) \Rxx(u,t)\dt }
     && \text{by property of $\Rxx$}
     && \text{\xref{thm:Rxx_prop}}
   \\&= \inprod{ \fx(u) }{\mcom{\opR}{$\opRa$} \fy }
     && \text{by definition of $\opR$}
     && \text{\xref{def:opR}}
   \\\implies&\qquad \opR=\opRa \qquad\implies \text{$\opR$ is \prope{self adjoint}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}
\footnote{
  \citerpp{keener}{114}{119}
  }
%---------------------------------------
Let $\seq{\lambda_n}{n\in\Z}$ be the eigenvalues and
    $\seq{\fpsi_n}{n\in\Z}$ be the eigenfunctions of
    operator $\opR$ such that
    $\opR \psi_n = \lambda_n \psi_n$.
\thmbox{\begin{array}{rlp{7cm}}
  1. & \lambda_n \in \R
     & (eigenvalues of $\opR$ are \prope{real})
     \\
  2. & \lambda_n\ne \lambda_m \implies \inprod{\psi_n}{\psi_m}=0
     & (eigenfunctions associated with distinct eigenvalues are \prope{orthogonal})
     \\
  3. & \norm{\psi_n(t)}^2>0 \implies \lambda_n\ge0
     & (eigenvalues are \prope{non-negative})
     \\
  4. & \norm{\psi_n(t)}^2>0, \inprod{\opR f}{f} > 0 \implies \lambda_n>0
     & (if $\opR$ is \prope{positive definite}, then eigenvalues are \prope{positive})
\end{array}}
\end{theorem}
\begin{proof}
\begin{enumerate}
\item Proof that eigenvalues are \prope{real-valued}:
Because $\opR$ is self-adjoint, its eigenvalues are real\ifsxref{operator}{thm:self_adjoint}.

\item eigenfunctions associated with distinct eigenvalues are orthogonal:
Because $\opR$ is self-adjoint, this property follows\ifsxref{operator}{thm:self_adjoint}.

\item Proof that eigenvalues are \prope{non-negative}:
\begin{align*}
   0 &\ge \inprod{\opR \psi_n}{\psi_n}
     &&   \text{by definition of non-negative definite}
   \\&=   \inprod{\lambda_n \psi_n}{\psi_n}
     &&   \text{by hypothesis}
   \\&=   \lambda_n \inprod{\psi_n}{\psi_n}
     &&   \text{by definition of inner-products}
   \\&=   \lambda_n \norm{\psi_n}^2
     &&   \text{by definition of norm induced by inner-product}
\end{align*}

\item Eigenvalues are \prope{positive} if $\opR$ is \prope{positive definite}:
\begin{align*}
   0 &> \inprod{\opR \psi_n}{\psi_n}
     && \text{by definition of \prope{positive definite}}
   \\&= \inprod{\lambda_n \psi_n}{\psi_n}
     && \text{by hypothesis}
   \\&= \lambda_n \inprod{\psi_n}{\psi_n}
     && \text{by definition of inner-products}
   \\&= \lambda_n \norm{\psi_n}^2
     && \text{by definition of norm induced by inner-product}
\end{align*}

\end{enumerate}
\end{proof}


%=======================================
\section{Basis for random processes}
\label{sec:KL}
%=======================================
If a random process $\rvx(t)$ is white
\footnote{{\em white noise process}: random process $\rvx(t)$ with autocorrelation $\Rxx(\tau)=\delta(\tau)$}
and $\Psi=\{\psi_1(t),\psi_2(t),\ldots,\psi_N(t)\}$ is \textbf{any} set of orthonormal basis functions,
then the innerproducts
$\inprod{n(t)}{\psi_n(t)}$ and $\inprod{n(t)}{\psi_m(t)}$ are \prope{uncorrelated}
for $m\ne  n$.
However, if $\rvx(t)$ is colored (not white), then the innerproducts are not
in general uncorrelated.
But if the elements of $\Psi$ are chosen to be the eigenfunctions of $\opR$ such
that
\[ \opR \psi_n = \lambda_n \psi_n,\]
then by \prefp{thm:Rxx_prop}, $\{\psi_n(t)\}$ are orthogonal and
the innerproducts \textbf{are} uncorrelated eventhough $\rvx(t)$ is
not white.
This criterion is called the  Karhunen-Lo\`{e}ve criterion for $\rvx(t)$.

%---------------------------------------
\begin{theorem}[\thmd{Karhunen-Lo/`eve Expansion}]
\footnote{
  \citerpp{keener}{114}{119}
  }
%---------------------------------------
Let $\opR$ be the \ope{autocorrelation operator} of a \fncte{random process} $\rvx(t)$.
\\
\thmboxt{
  $\brb{\begin{array}{FMD}
      (A).&$\seq{\lambda_n}{n\in\Z}$ are the eigenvalues of $\opR$ & and
    \\(B).&$\seq{\fpsi_n}{n\in\Z}$ are the eigenfunctions of $\opR$ & such that
    \\(C).&$\opR \psi_n = \lambda_n \psi_n$
  \end{array}}$
  \\\indentx$\implies\quad
  \brb{\begin{array}{M}
       $\ds\pE\brb{\abs{ x(t)-\sum_{n\in\Z}\inprod{x(t)}{\psi_n(t)} \psi_n(t) }^2} = 0$
     \\($\setn{\psi_n(t)}$ is a \structe{basis} for $\rvx(t)$)
  \end{array}}$
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
\item $\{\psi_n(t)\}$ is a basis for $\rvx(t)$
      \[ \pE\brb{\left| x(t)-\sum_{n\in\Z}\dot{x}_n \psi_n(t) \right|^2} = 0
         \hspace{1cm}\mbox{where } \dot{x}_n\eqd \inprod{x(t)}{\psi_n(t)}
      \]

\begin{align*}
   \pE\brs{x(t) \brp{\sum_{n\in\Z}\dot{x}_n \psi_n(t)}^\ast}
     &= \pE\brs{x(t) \brp{\sum_{n\in\Z}\int_{u\in\R}x(u)\psi_n^\ast(u)\du \psi_n(t)}^\ast}
   \\&= \sum_{n\in\Z}\brp{\int_{u\in\R}\pE\brs{x(t)x^\ast(u)}\psi_n(u)\du} \psi_n^\ast(t)
   \\&= \sum_{n\in\Z}\brp{\int_{u\in\R}\Rxx(t,u)\psi_n(u)\du} \psi_n^\ast(t)
   \\&= \sum_{n\in\Z}\lambda_n\psi_n(t) \psi_n^\ast(t)
   \\&= \sum_{n\in\Z}\lambda_n \left|\psi_n(t) \right|^2
\\ \\
   \pE\brs{\sum_{n\in\Z}\dot{x}_n \psi_n(t)\brp{\sum_{m\in\Z}\dot{x}_m \psi_m(t)}^\ast}
     &= \pE\brs{\sum_{n\in\Z}\int_{u\in\R}x(u)\psi_n^\ast(u)\du   \psi_n(t)\brp{\sum_{m\in\Z}\int_v x(v)\psi_m^\ast(v)\dv \psi_m(t)}^\ast}
   \\&= \sum_{n\in\Z}\sum_{m\in\Z}\int_u\brp{\int_v \pE\brs{x(u)x^\ast(v)}\psi_m(v)\dv} \psi_n^\ast(u)\du   \psi_n(t)   \psi_m^\ast(t)
   \\&= \sum_{n\in\Z}\sum_{m\in\Z}\int_u\brp{\int_v \Rxx(u,v)\psi_m(v)\dv} \psi_n^\ast(u)\du   \psi_n(t)   \psi_m^\ast(t)
   \\&= \sum_{n\in\Z}\sum_{m\in\Z}\int_u\brp{\lambda_m\psi_m(u)} \psi_n^\ast(u)\du   \psi_n(t)   \psi_m^\ast(t)
   \\&= \sum_{n\in\Z}\sum_{m\in\Z}\lambda_m \brp{\int_{u\in\R}\psi_m(u) \psi_n^\ast(u)\du }   \psi_n(t)   \psi_m^\ast(t)
   \\&= \sum_{n\in\Z}\sum_{m\in\Z}\lambda_m \kdelta_{mn}   \psi_n(t)   \psi_m^\ast(t)
   \\&= \sum_{n\in\Z}\lambda_n   \psi_n(t)   \psi_n^\ast(t)
   \\&= \sum_{n\in\Z}\lambda_n  \left| \psi_n(t) \right|^2
\end{align*}


\item Using the previous two results, we can prove the following:

\begin{align*}
  &\pE\brb{\left| x(t)-\sum_{n\in\Z}\dot{x}_n \psi_n(t) \right|^2}
  \\&= \pE\brs{\brs{ x(t)-\sum_{n\in\Z}\dot{x}_n \psi_n(t) }\brs{ x(t)-\sum_{m\in\Z}\dot{x}_m \psi_m(t) }^\ast}
  \\&= \pE\brs{x(t)x^\ast(t) -x(t)\brp{\sum_{n\in\Z}\dot{x}_n \psi_n(t)}^\ast -x^\ast(t)\sum_{n\in\Z}\dot{x}_n \psi_n(t) + \sum_{n\in\Z}\dot{x}_n \psi_n(t) \brp{\sum_{m\in\Z}\dot{x}_m \psi_m(t) }^\ast }
  \\&= \pE\brs{x(t)x^\ast(t)} -\pE\brs{x(t)\brp{\sum_{n\in\Z}\dot{x}_n \psi_n(t)}^\ast} -\pE\brs{x^\ast(t)\sum_{n\in\Z}\dot{x}_n \psi_n(t)} + \pE\brs{\sum_{n\in\Z}\dot{x}_n \psi_n(t) \brp{\sum_{m\in\Z}\dot{x}_m \psi_m(t) }^\ast }
  \\&\text{by \thme{Mercer's Theorem}\xref{thm:mercer}}
  \\&= \sum_{n\in\Z}\lambda_n |\psi_n(t)|^2 -\sum_{n\in\Z}\lambda_n \left|\psi_n(t) \right|^2  -\brp{\sum_{n\in\Z}\lambda_n \left|\psi_n(t) \right|^2}^\ast + \sum_{n\in\Z}\lambda_n \left|\psi_n(t) \right|^2
  \\&= \sum_{n\in\Z}\lambda_n |\psi_n(t)|^2 -\sum_{n\in\Z}\lambda_n \left|\psi_n(t) \right|^2  -\sum_{n\in\Z}\lambda_n \left|\psi_n(t) \right|^2 + \sum_{n\in\Z}\lambda_n \left|\psi_n(t) \right|^2
  \\&= 0
\end{align*}
\end{enumerate}
\end{proof}

%=======================================
\section{LTI Operations on non-stationary random processes}
\index{LTI!operations on non-stationary random processes}
%=======================================
\begin{figure}[ht]\color{figcolor}
\begin{fsK}
\begin{center}
  \setlength{\unitlength}{0.2mm}
  \begin{picture}(300,130)(-100,-80)
  \thicklines
  %\graphpaper[10](0,0)(160,80)
  \put(-100,  10 ){\makebox (100, 40)[b]{$\rvx(t)$}  }
  \put(-100, -50 ){\makebox (100, 40)[t]{$\Rxx(t,u)$}  }
  \put(-100,   0 ){\vector  (  1,  0){100}             }
  \put(   0, -50 ){\framebox(100,100){$\conv \fh(t)$}  }
  \put( 100,   0 ){\vector  (  1,  0){100}             }
  \put( 110,  10 ){\makebox (100, 40)[lb]{$\ds\rvy(t)=\rvx(t)\conv\fh(t)=\int_u\fh(u)\rvx(t-u)\du$}  }
  \put( 100, -50 ){\makebox (100, 40)[t]{$\Ryy(t,u)$}  }
  \put(  50, -60 ){\makebox (  0,  0)[t]{$\Rxy(t,u)$}  }
  \end{picture}
\caption{
   Linear system with random process input and output
   \label{fig:linear-sys}
   }
\end{center}
\end{fsK}
\end{figure}

%---------------------------------------
\begin{theorem}
\index{linear time invariant}
\footnote{\citerp{papoulis}{312}}
%---------------------------------------
Let $\fh:\R\to\C$ be the impulse response of a linear time-invariant system and
Let $\rvy(t)=h(t)\conv \rvx(t) \eqd \int_{u\in\R}h(u)\rvx(t-u) \du$ as
illustrated in \prefp{fig:linear-sys}.
Then
\thmbox{\begin{array}{rclcl}
\mc{5}{l}{\mbox{\bf Correlation functions}} \\
   \Rxy(t,u)
     &=&    \Rxx(t,u) \conv h^\ast(u)
     &\eqd& \int_v \fh^\ast(v)  \Rxx(t,u-v)  \dv
\\
   \Ryy(t,u)
     &=&    \Rxy(t,u) \conv \fh(t)
     &\eqd& \int_v \fh(v) \Rxy(t-v,u) \dv
\\
   \Ryy(t,u)
     &=&    \Rxx(t,u) \conv \fh(t) \conv \fh^\ast(u)
     &\eqd& \int_w \fh^\ast(w) \int_v \fh(v) \Rxx(t-v,u-w) \dv\dw
\\ \\
\mc{5}{l}{\mbox{\bf Laplace power spectral density functions}} \\
   \LSxy(s,r) &=& \LSxx(s,r) \Lh^\ast(r^\ast)         \\
   \LSyy(s,r) &=& \LSxy(s,r) \Lh(s)              \\
   \LSyy(s,r) &=& \LSxx(s,r) \Lh(s) \Lh^\ast(r^\ast)  \\
\\
\mc{5}{l}{\mbox{\bf Power spectral density functions}} \\
   \Sxy(f,g)  &=& \Sxx(f,g) \Fh^\ast(-g)          \\
   \Syy(f,g)  &=& \Sxy(f,g) \Fh(\omega)               \\
   \Syy(f,g)  &=& \Sxx(f,g) \Fh(\omega) \Fh^\ast(-g)
\end{array}}
\end{theorem}
\begin{proof}
\begin{eqnarray*}
   \Rxy(t,u)
     &\eqd& \E\brs{\rvx(t) \rvy^\ast(u) }
   \\&=&    \E\brs{\rvx(t) \brp{ \int_v \fh(v) \rvx(u-v)  \dv }^\ast }
   \\&=&    \E\brs{\rvx(t) \int_v \fh^\ast(v) \rvx^\ast(u-v)  \dv }
   \\&=&    \int_v \fh^\ast(v)  \E\brs{\rvx(t)\rvx^\ast(u-v)} \dv
   \\&=&    \int_v \fh^\ast(v)  \Rxx(t,u-v)  \dv
   \\&\eqd& \Rxx(t,u) \conv h^\ast(u)
\\ \\
   \Ryy(t,u)
     &\eqd& \E\brs{\rvy(t) \rvy^\ast(u) }
   \\&=&    \E\brs{\brp{\int_v \fh(v) \fx(t-v)\dv}  \rvy^\ast(u) }
   \\&=&    \int_v \fh(v) \E\brs{\fx(t-v)\rvy^\ast(u)} \dv
   \\&=&    \int_v \fh(v) \Rxy(t-v,u) \dv
   \\&\eqd& \Rxy(t,u) \conv \fh(t)
\\ \\
   \Ryy(t,u)
     &\eqd& \E\brs{\rvy(t) \rvy^\ast(u) }
   \\&=&    \E\brs{\brp{\int_v \fh(v) \fx(t-v)\dv}
                    \brp{\int_w \fh(w) \fx(u-w)\dw}^\ast
              }
   \\&=&    \int_w \fh^\ast(w) \int_v \fh(v)
                   \E\brs{\fx(t-v)\fx^\ast(u-w)} \dv\dw
   \\&=&    \int_w \fh^\ast(w) \int_v \fh(v)
                   \Rxx(t-v,u-w) \dv\dw
   \\&=&    \int_w \fh^\ast(w) \brs{\Rxx(t,u-w)\conv\fh(t)}\dw
   \\&\eqd& \Rxx(t,u) \conv \fh(t) \conv \fh^\ast(u)
\\ \\
   \LSxy(s,r)
     &\eqd& \opL\Rxy(t,u)
   \\&=&    \opL[\Rxx(t,u) \conv \fh^\ast(u)]
   \\&=&    \opL[\Rxx(t,u)]\opL[\fh^\ast(u)]
   \\&=&    \LSxx(s,r)  \int_{u\in\R}\fh^\ast(u) e^{-ru}\du
   \\&=&    \LSxx(s,r)  \brs{\int_{u\in\R}\fh(u) e^{-r^\ast u}\du }^\ast
   \\&=&    \LSxx(s,r) \Lh^\ast(r^\ast)
\\ \\
   \LSyy(s,r)
     &\eqd& \opL\Ryy(t,u)
   \\&=&    \opL[\Rxy(t,u) \conv \fh(t)]
   \\&=&    \opL[\Rxy(t,u)]\opL[\fh(t)]
   \\&=&    \LSxy(s,r) \Lh(s)
\\
   \\&=&    \LSxy(s,r) \Lh(s)
   \\&=&    \LSxx(s,r) \Lh^\ast(r^\ast)\Lh(s)
   \\&=&    \LSxx(s,r) \Lh(s) \Lh^\ast(r^\ast)
\\ \\
   \Sxy(f,g)
     &\eqd& \opFT\Rxy(t,u)
   \\&=&    \opFT[\Rxx(t,u) \conv \fh^\ast(u)]
   \\&=&    \opFT[\Rxx(t,u)]\opFT[\fh^\ast(u)]
   \\&=&    \Sxx(f,g) \int_{u\in\R}\fh^\ast(u) e^{-i2\pi g u} \du
   \\&=&    \Sxx(f,g) \brs{\int_{u\in\R}\fh(u) e^{i2\pi g u} \du }^\ast
   \\&=&    \Sxx(f,g) \brs{\int_{u\in\R}\fh(u) e^{-i2\pi(-g)u} \du }^\ast
   \\&=&    \Sxx(f,g) \Fh^\ast(-g)
\\ \\
   \Syy(f,g)
     &\eqd& \opFT\Ryy(t,u)
   \\&=&    \opFT[\Rxy(t,u) \conv \fh(t)]
   \\&=&    \opFT[\Rxy(t,u)]\opFT[\fh(t)]
   \\&=&    \Sxy(f,g) \Fh(\omega)
\\
   \\&=&    \Sxy(f,g) \Fh(\omega)
   \\&=&    \Sxx(f,g) \Fh^\ast(-g) \Fh(\omega)
\end{eqnarray*}
\end{proof}


%=======================================
\section{LTI Operations on WSS random processes}
\index{LTI!operations on WSS random processes}
\index{WSS}
\index{wide sense stationary}
%=======================================
%---------------------------------------
\begin{definition}
\label{def:wss}
%---------------------------------------
\defboxt{
  A random process $\rvx(t)$ is \propd{wide sense stationary} (\propd{WSS}) if
  \\\indentx$\begin{array}{FlMMD}
   (1).& \pmeanx(t)     & is \prope{constant} with respect to $t$ & (\prope{stationary in the mean})    & and \\
   (2).& \Rxx(t+\tau,t) & is \prope{constant} with respect to $t$ & (\prope{stationary in correlation})
  \end{array}$
  }
\end{definition}

If a process $\rvx(t)$ is \prope{wide sense stationary}, mean and correlation are often written
$\pmeanx$ and $\Rxx(\tau)$, respectively.
If a pair of processes $\rvx(t)$ and $\rvy(t)$ are \prope{WSS},
then their cross-correlation is commonly written $\Rxy(\tau)$.

%---------------------------------------
\begin{definition}
\label{def:LSxx}
\label{def:LSyy}
\label{def:LSxy}
\label{def:LSyx}
%---------------------------------------
Let $\rvx(t)$ and $\rvy(t)$ be WSS random processes.
%Then the \textbf{power spectral density} of $\rvx(t)$ and $\rvy(t)$ are defined as
\defbox{\begin{array}{rclcl}
     \LSxx(s) &\eqd& \opL{\Rxx(\tau)} &\eqd& \ds \int_{\tau\in\R}\Rxx(\tau) e^{-s\tau} \dtau
   \\\LSyy(s) &\eqd& \opL{\Ryy(\tau)} &\eqd& \ds \int_{\tau\in\R}\Ryy(\tau) e^{-s\tau} \dtau
   \\\LSxy(s) &\eqd& \opL{\Rxy(\tau)} &\eqd& \ds \int_{\tau\in\R}\Rxy(\tau) e^{-s\tau} \dtau
   \\\LSyx(s) &\eqd& \opL{\Ryx(\tau)} &\eqd& \ds \int_{\tau\in\R}\Ryx(\tau) e^{-s\tau} \dtau
\end{array}}
\end{definition}

%---------------------------------------
\begin{definition}
\label{def:Swxx}
\label{def:Swyy}
\label{def:Swxy}
\label{def:Swyx}
%---------------------------------------
Let $\rvx(t)$ and $\rvy(t)$ be WSS random processes.
%Then the \textbf{power spectral density} of $\rvx(t)$ and $\rvy(t)$ are defined as
\defbox{\begin{array}{rc>{\ds}lc>{\ds}l}
     \Swxx(\omega) &\eqd& [\opFT{\Rxx(\tau)}](\omega) &\eqd& \ds \int_{\tau\in\R} \Rxx(\tau) e^{-i\omega\tau} \dtau
   \\\Swyy(\omega) &\eqd& [\opFT{\Ryy(\tau)}](\omega) &\eqd& \ds \int_{\tau\in\R} \Ryy(\tau) e^{-i\omega\tau} \dtau
   \\\Swxy(\omega) &\eqd& [\opFT{\Rxy(\tau)}](\omega) &\eqd& \ds \int_{\tau\in\R} \Rxy(\tau) e^{-i\omega\tau} \dtau
   \\\Swyx(\omega) &\eqd& [\opFT{\Ryx(\tau)}](\omega) &\eqd& \ds \int_{\tau\in\R} \Ryx(\tau) e^{-i\omega\tau} \dtau
\end{array}}
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{papoulis1984}{246}{0070484686}{Mean-Ergodic processes},
  \citerpgc{papoulis2002}{523}{0071226613}{12-1 \scshape Ergodicity},
  \citerpgc{kay1988}{58}{8131733564}{3.6 \scshape Ergodicity of the autocorrelation function},
  \citerpgc{manolakis2005}{106}{1580536107}{Ergodic random processes},
  \citerppg{koopmans1995}{53}{61}{0124192513},
  \citerpgc{cadzow}{378}{0023180102}{11.13 \scshape Ergodic time series},
  \citerpg{helstrom1991}{336}{0023535717}
  }
\label{def:ergomean}
%---------------------------------------
Let $\rvx(t)$ be a random variable that is \prope{stationary in the mean} such that
\\\indentx$\ds\pE\brs{\rvx(t)}$ is constant with respect to $t$.
\defboxt{
  $\rvx(t)$ is \propd{ergodic in the mean} if
  \\\indentx$\ds\mcomr{\pE\brs{\rvx(t)}}{\ope{ensemble average}} 
   = \lim_{\tau\to\infty}
   \mcom{\frac{1}{2\tau}\int_{-\tau}^{+\tau}\rvx(t)\dt}{\ope{time average}}
  $
  }
\end{definition}

%---------------------------------------
\begin{proposition}
%---------------------------------------
\propbox{
  \brb{\begin{array}{M}
    $\rvx(t)$ is \prope{non-stationary}
  \end{array}}
  \quad\implies\quad
  \brb{\begin{array}{M}
    $\rvx(t)$ is \prope{not ergodic in the mean}
  \end{array}}
  }
\end{proposition}
\begin{proof}
  If $\rvx(t)$ is \prope{non-stationary}, then $\pE\brs{\rvx(t)}$ is not constant with time.
  But $\lim_{\tau\to\infty}\frac{1}{2\tau}\int_{-\tau}^{+\tau}\rvx(t)\dt$ must be a constant 
  (if it is \prope{convergent}).
\end{proof}

%---------------------------------------
\begin{definition}
\footnote{
  \citerpg{bendat2011}{177}{1118210824}
  }
\label{def:Pavg}
%---------------------------------------
Let $\rvx(t)$ be a \prope{wide sense stationary} random process.
\defbox{\begin{array}{FMrc>{\ds}l}
    (1).&The \fnctd{average power} $\Pavg$ of $\rvx(t)$ is 
        & \Pavg\rvx(t) &\eqd& \lim_{\tau\to\infty}\frac{1}{2\tau}\int_{t\in\R} \abs{\rvx(t)}^2\dt
  \\(2).&The \fnctd{energy spectral density} $\abs{\Fx(\omega)}^2$  of $\rvx(t)$ is 
        & \abs{\Fx(\omega)}^2 &\eqd& \abs{\opFT\rvx(t)}^2
\end{array}}
\end{definition}

Why does $\Swxx(\omega)$ deserve the name \fncte{power spectral density}?
This is answered by \pref{thm:psd_ergodic} (next).
But to elaborate further, note that $\Swxx$ is the spectral representation of 
the statistical relationship (the \ope{variance}) between samples of $\rvx(t)$. 
For example, if there is no relationship, then $\Swxx(\omega)=1$.
But in the case that $\rvx(t)$ is \prope{ergodic in the mean}, then $\Swxx$ takes on 
an additional meaning---it describes the ``spectral power" present in $\rvx(t)$.
This is demonstrated by the next theorem.

%---------------------------------------
\begin{theorem}
\label{thm:psd_ergodic}
%---------------------------------------
Let $\rvx(t)$ be a \fncte{random process}.
\thmbox{
  \brb{\begin{array}{FMD}
      %(A).&$\rvx(t)$ is \prope{wide sense stationary} & and
      (A).&$\rvx(t)$ is \prope{ergodic in the mean} & and
    \\(B).&$\ft{x}(\omega)$ \prope{exists}
  \end{array}}
  \implies
  \brb{\begin{array}{Frc>{\ds}lD}
      (1).& \Swxx(\omega)      &=& \lim_{\tau\to\infty}\frac{1}{2\tau} \mcom{\abs{\ft{x}(\omega)}^2}{\fncte{(ESD)}} & and
    \\(2).& \Pavg\brs{\rvx(t)} &=& \int_{\omega\in\R} \Swxx(\omega) \dw                               & 
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{align*}
   \Swxx(\omega)
     &\eqd \int_{\tau\in\R}\Rxx(\tau) e^{-i\omega\tau} \dtau
     &&    \text{by definition of $\Swxx(\omega)$}
     &&    \text{\xref{def:Swxx}}
   \\&=    \int_{\tau\in\R}\pE\brs{\rvx(t+\tau)\rvx^\ast(t)} e^{-i\omega\tau} \dtau
     &&    \text{by definition of $\Rxx(t)$}
     &&    \text{\xref{def:Rxx}}
   \\&=    \pE\brs{\rvx^\ast(t) \int_{\tau\in\R}\rvx(t+\tau) e^{-i\omega\tau} \dtau }
     &&    \text{by \prope{linearity} of $\pE$ operator}
   \\&=    \pE\brs{\rvx^\ast(t) \int_{u\in\R}\rvx(u) e^{-i\omega(u-t)} \du }
     &&    \text{where $u\eqd t+\tau$ $\implies$ $\tau=u-t$}
   \\&=    \pE\brs{\rvx^\ast(t)e^{i\omega t} \int_{u\in\R}\rvx(u) e^{-i\omega u} \du }
   \\&=    \pE\brs{\rvx^\ast(t)e^{i\omega t} \ft{x}(\omega) }
     &&    \text{by definition of \ope{Fourier Transform}}
     &&    \text{\xref{def:opFT}}
   \\&=    \pE\brs{\rvx^\ast(t)e^{i\omega t}} \ft{x}(\omega)
     &&    \text{by hypothesis (B)}
   \\&=    \brs{\lim_{\tau\to\infty}\frac{1}{2\tau}\int_{-\tau}^{+\tau} \rvx^\ast(t)e^{i\omega t} \dt} \ft{x}(\omega)
     &&    \text{by \prope{ergodic in the mean} hypothesis}
     &&    \text{\xref{def:ergomean}}
   \\&=    \lim_{\tau\to\infty}\frac{1}{2\tau}\brs{\int_{t\in\R}\rvx(t)e^{-i\omega t} \dt}^\ast \ft{x}(\omega)
   \\&=    \lim_{\tau\to\infty}\frac{1}{2\tau} \ft{x}^\ast(\omega) \ft{x}(\omega)
     &&    \text{by hypothesis (B)}
   \\&=    \lim_{\tau\to\infty}\frac{1}{2\tau} \abs{\ft{x}(\omega)}^2
\end{align*}
\begin{align*}
   \int_{\omega\in\R} \Swxx(\omega) \dw
     &= \int_{\omega\in\R} \lim_{\tau\to\infty}\frac{1}{2\tau} \abs{\ft{x}(\omega)}^2 \dw
   \\&= \lim_{\tau\to\infty}\frac{1}{2\tau} \int_{\omega\in\R} \abs{\ft{x}(\omega)}^2 \dw
   \\&= \lim_{\tau\to\infty}\frac{1}{2\tau} \int_{t\in\R} \abs{\rvx(t)}^2 \dt
     && \text{by \thme{Plancheral's formula}}
     && \text{\xxref{thm:planform}{thm:plancherel}}
   \\&= \Pavg
     && \text{by definition of $\Pavg$}
     && \text{\xref{def:Pavg}}
\end{align*}

Thus, $\Swxx(\omega)$ is the power density of $\rvx(t)$ in the frequency domain.
\end{proof}


\begin{figure}[ht]\color{figcolor}
\begin{fsK}
\begin{center}
  \setlength{\unitlength}{0.2mm}
  \begin{picture}(300,130)(-100,-80)
  \thicklines
  %\graphpaper[10](0,0)(160,80)
  \put(-100,  10 ){\makebox (100, 40)[b]{$\rvx(t)$}  }
  \put(-100, -50 ){\makebox (100, 40)[t]{$\Rxx(\tau)$}  }
  \put(-100, -50 ){\makebox (100, 40)[b]{$\Sxx(s)$}  }
  \put(-100,   0 ){\vector  (  1,  0){100}             }
  \put(   0, -50 ){\framebox(100,100){$\conv h(t)$}  }
  \put( 100,   0 ){\vector  (  1,  0){100}             }
  \put( 100,  10 ){\makebox (100, 40)[b]{$\rvy(t)$}  }
  \put( 100, -50 ){\makebox (100, 40)[t]{$\Ryy(\tau)$}  }
  \put( 100, -50 ){\makebox (100, 40)[b]{$\Syy(s)$}  }
  \put(  50, -60 ){\makebox (  0,  0)[t]{$\Sxy(s)$}  }
  \end{picture}
\caption{
   Linear system with WSS random process input and output
   \label{fig:linear-sys-WSS}
   }
\end{center}
\end{fsK}
\end{figure}

%---------------------------------------
\begin{theorem}
\index{convolution}
\index{linear time invariant systems}
\index{LTI}
\footnote{\citerpp{papoulis}{323}{324}}
%---------------------------------------
Let $\fh:\R\to\C$ be the impulse response of a linear time-invariant system and
let $\rvy(t)=h(t)\conv \rvx(t) \eqd \int_{u\in\R}h(u)\rvx(t-u) \du$ as
illustrated in \prefp{fig:linear-sys}.
Then
\thmbox{\begin{array}{rclcl}
   \Rxy(\tau) &=&    \Rxx(\tau)\conv h^\ast(-\tau)
              &\eqd& \int_{u\in\R}h^\ast(-u) \Rxx(\tau-u)\du \\
   \Ryy(\tau) &=&    \Rxy(\tau)\conv h(\tau)
              &\eqd& \int_{u\in\R}h(u) \Rxy(\tau-u)\du \\
   \Ryy(\tau) &=&    \Rxx(\tau)\conv h(\tau)\conv h^\ast(-\tau)
              &\eqd& \int_v \int_{u\in\R}h(u-v)h^\ast(-v) \Rxx(\tau-u-v)\du\dv  \\
\\
   \Sxy(s)    &=&    \Sxx(s) \hat{h}^\ast(-s^\ast)             \\
   \Syy(s)    &=&    \Sxy(s) \hat{h}(s)                        \\
   \Syy(s)    &=&    \Sxx(s) \hat{h}(s) \hat{h}^\ast(-s^\ast)  \\
\\
   \Swxy(\omega)    &=&    \Swxx(\omega) \Fh^\ast(\omega)  \\
   \Swyy(\omega)    &=&    \Swxy(\omega) \Fh(\omega)       \\
   \Swyy(\omega)    &=&    \Swxx(\omega) |\Fh(\omega)|^2   \\
\end{array}}
\end{theorem}

\begin{proof}
\begin{align*}
   \Rxx(\tau) \conv h^\ast(-\tau)
     &\eqd \int_{u\in\R}h^\ast(-u) \Rxx(\tau-u)\du
   \\&=    \int_{u\in\R}h^\ast(-u) \E\brs{\rvx(t) \rvx^\ast(t-\tau+u) } \du
   \\&=    \E\brs{\rvx(t) \int_{u\in\R}h^\ast(-u)  \rvx^\ast(t-\tau+u) \du   }
   \\&=    \E\brs{\rvx(t) \int_{u\in\R}h^\ast(u^\prime)  \rvx^\ast(t-\tau-u^\prime) \du^\prime   }
   \\&=    \E\brs{\rvx(t) \rvy^\ast(t-\tau)  }
   \\&\eqd \Rxy(\tau)
\\
\\
   \Rxy(\tau) \conv h(\tau)
     &\eqd \int_{u\in\R}h(u) \Rxy(\tau-u)\du
   \\&=    \int_{u\in\R}h(u) \E\brs{\rvx(t+\tau-u) \rvy^\ast(t) } \du
   \\&=    \E\brs{\rvy^\ast(t) \int_{u\in\R}h(u) \rvx(t+\tau-u)  \du }
   \\&=    \E\brs{\rvy^\ast(t) \rvy(t+\tau) }
   \\&=    \E\brs{ \rvy(t+\tau) \rvy^\ast(t)}
   \\&\eqd \Ryy(\tau)
\\
\\
   \Ryy(\tau)
     &= \Rxy(\tau) \conv h(\tau)
   \\&= \Rxx(\tau) \conv h^\ast(-\tau) \conv h(\tau)
   \\&= \Rxx(\tau) \conv h(\tau)  \conv h^\ast(-\tau)
\\
\\
  \Sxy(s)
     &\eqd \opL \Rxy(\tau)
   \\&\eqd \int_{\tau\in\R}\Rxy(\tau) e^{-s\tau} \; d\tau
   \\&=    \int_{\tau\in\R}\brs{ \Rxx(\tau) \conv h^\ast(-\tau) } e^{-s\tau} \; d\tau
   \\&=    \int_{\tau\in\R}\brs{ \int_{u\in\R}h^\ast(-u) \Rxx(\tau-u)\du } e^{-s\tau} \; d\tau
   \\&=    \int_{u\in\R}h^\ast(-u) \int_{\tau\in\R}\Rxx(\tau-u)e^{-s\tau} \; d\tau\du
   \\&=    \int_{u\in\R}h^\ast(-u) \int_v \Rxx(v)e^{-s(v+u)} \dv \du
     && \text{where $v=\tau-u\iff \tau=v+u$}
   \\&=    \int_{u\in\R}h^\ast(-u) e^{-su} \du \int_v \Rxx(v)e^{-sv} \dv
   \\&=    \int_{u\in\R}h^\ast(u) e^{-s(-u)} \du \int_v \Rxx(v)e^{-sv} \dv
   \\&=    \brp{\int_{u\in\R}h(u) e^{-(-s^\ast)u} \du }^\ast
           \int_v \Rxx(v)e^{-sv} \dv
   \\&\eqd \hat{h}^\ast(-s^\ast) \Sxx(s)
\\
\\
   \Syy(s)
     &\eqd \opL \Ryy(\tau)
   \\&\eqd \int_{\tau\in\R}\Ryy(\tau) e^{-s\tau} \; d\tau
   \\&=    \int_{\tau\in\R}\brs{ \Rxy(\tau) \conv h(\tau) } e^{-s\tau} \; d\tau
   \\&=    \int_{\tau\in\R}\brs{ \int_{u\in\R}h(u) \Rxy(\tau-u)\du } e^{-s\tau} \; d\tau
   \\&=    \int_{u\in\R}h(u) \int_{\tau\in\R}\Rxy(\tau-u) e^{-s\tau} \; d\tau\du
   \\&=    \int_{u\in\R}h(u) \int_{\tau\in\R}\Rxy(v) e^{-s(v+u)} \; d\tau\du
     && \text{where $v=\tau-u\iff \tau=v+u$}
   \\&=    \int_{u\in\R}h(u)e^{-su} \;du \int_{\tau\in\R}\Rxy(v) e^{-sv} \; d\tau
   \\&\eqd \hat{h}(s) \Sxy(s)
\\
\\
   \Syy(s)
     &= \hat{h}(s) \Sxy(s)
   \\&= \hat{h}(s) \hat{h}^\ast(-s^\ast) \Sxx(s)
\\
\\
   \Swxy(\omega)
     &=    \left.\Sxy(s)\right|_{s=j\omega}
   \\&=    \left. \hat{h}^\ast(-s^\ast) \Sxx(s)\right|_{s=j\omega}
   \\&=    \left.
           \brp{\int_{u\in\R}h(u) e^{-(-s^\ast)u} \du }^\ast
           \int_v \Rxx(v)e^{-sv} \dv
           \right|_{s=j\omega}
   \\&=    \brp{\int_{u\in\R}h(u) e^{-(-j\omega)^\ast u} \du }^\ast
           \int_v \Rxx(v)e^{-j\omega v} \dv
   \\&=    \brp{\int_{u\in\R}h(u) e^{-j\omega u} \du }^\ast
           \int_v \Rxx(v)e^{-j\omega v} \dv
   \\&\eqd \Fh^\ast(\omega) \Swxx(\omega)
\\
\\
   \Swyy(\omega)
     &=    \left.\Syy(s)\right|_{s=j\omega}
   \\&=    \left. \hat{h}(s) \Sxy(s) \right|_{s=j\omega}
   \\&=    \left. \int_{u\in\R}h(u)e^{-su} \;du \int_{\tau\in\R}\Rxy(v) e^{-sv} \; d\tau\right|_{s=j\omega}
   \\&=    \int_{u\in\R}h(u)e^{-j\omega u} \;du \int_{\tau\in\R}\Rxy(v) e^{-j\omega v} \; d\tau
   \\&=    \Fh(\omega) \Swxy(\omega)
\\
\\
  \Swyy(\omega)
     &=    \Fh(\omega)\Swxy(\omega)
   \\&=    \Fh(\omega) \Fh^\ast(\omega) \Swxx(\omega)
   \\&=    |\Fh(\omega)|^2 \Swxx(\omega)
\end{align*}


\end{proof}

\begin{figure}[ht]\color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.08mm}
\begin{tabular}{c@{\hspace{1cm}}c@{\hspace{1cm}}c@{\hspace{1cm}}c}
$\Reb{\Rxx(\tau)}$ & $\Imb{\Rxx(\tau)}$ &
$|\Rxx(\tau)|$     & $\angle\Rxx(\tau)$
\\
\begin{picture}(340,300)(-150,-150)
  %\graphpaper[10](0,0)(600,200)
  \thicklines
  \put(-150,   0){\line(1,0){300} }
  \put(   0,-150){\line(0,1){300} }
  \put( 160,   0){\makebox(0,0)[l]{$f$} }
  \put(-100,   0){\line( 1,1){100} }
  \put( 100,   0){\line(-1,1){100} }
  %\put(  60,  60 ){\makebox(0,0)[bl]{$\Reb{\Rxx(\tau)}$}}
\end{picture}
&
\begin{picture}(340,300)(-150,-150)
  %\graphpaper[10](0,0)(600,200)
  \thicklines
  \put(-150,   0){\line(1,0){300} }
  \put(   0,-150){\line(0,1){300} }
  \put( 160,   0){\makebox(0,0)[l]{$f$} }
  \qbezier(0,0)( 20, 80)( 100, 100)
  \qbezier(0,0)(-20,-80)(-100,-100)
  \put( 100,   0){\line(0, 1){100} }
  \put(-100,   0){\line(0,-1){100} }
  %\put(- 10,  60 ){\makebox(0,0)[br]{$\Imb{\Rxx(\tau)}$}}
\end{picture}
&
\begin{picture}(340,300)(-150,-150)
  %\graphpaper[10](0,0)(600,200)
  \thicklines
  \put(-150,   0){\line(1,0){300} }
  \put(   0,-150){\line(0,1){300} }
  \put( 160,   0){\makebox(0,0)[l]{$f$} }
  \qbezier(0,100)( 20,20)( 100, 0)
  \qbezier(0,100)(-20,20)(-100, 0)
  %\put( 60,  60 ){\makebox(0,0)[bl]{$|\Rxx(\tau)|$}}
\end{picture}
&
\begin{picture}(340,300)(-150,-150)
  %\graphpaper[10](0,0)(600,200)
  \thicklines
  \put(-150,   0){\line(1,0){300} }
  \put(   0,-150){\line(0,1){300} }
  \put( 160,   0){\makebox(0,0)[l]{$f$} }
  \put( 100,   0){\line(0, 1){100} }
  \put(-100,   0){\line(0,-1){100} }
  \put(-100,-100){\line(1, 1){200} }
  %\put(- 10,  60 ){\makebox(0,0)[br]{$\angle\Rxx(\tau)$}}
\end{picture}
\\
(symmetric) & (anti-symmetric) & (symmetric) & (anti-symmetric)
\end{tabular}
\end{fsL}
\end{center}
\caption{
   Autocorrelation $\Rxx(\tau)$
   \label{fig:Rxx}
   }
\end{figure}

%---------------------------------------
\begin{theorem}
\index{conjugate symmetric}
%---------------------------------------
Let $\rvx:\R\to\C$ be a WSS random process with
auto-correlation $\Rxx(\tau)$.
Then $\Rxx(\tau)$ is \textbf{conjugate symmetric} such that
(see \prefp{fig:Rxx})
\thmbox{\begin{array}{rclD}
  \Rxx(\tau)       &=& \Rxx^\ast(-\tau)    & (\prope{conjugate symmetric}) \\
  \Reb{\Rxx(\tau)} &=& \Reb{\Rxx(-\tau)}   & (\prope{symmetric          }) \\
  \Imb{\Rxx(\tau)} &=& -\Imb{\Rxx(-\tau)}  & (\prope{anti-symmetric     }) \\
  |\Rxx(\tau)|     &=& |\Rxx(-\tau)|       & (\prope{symmetric          }) \\
  \angle\Rxx(\tau) &=& \angle\Rxx(-\tau)   & (\prope{anti-symmetric     }).
\end{array}}
\end{theorem}
\begin{proof}
\begin{eqnarray*}
   \Rxx^\ast(\tau)
     &\eqd& \brp{ \pE\brs{\rvx(t-\tau)\rvx^\ast(t)}}^\ast
   \\&=&           \pE\brs{\rvx^\ast(t-\tau)\rvx(t)}
   \\&=&           \pE\brs{\rvx(t)\rvx^\ast(t-\tau)}
   \\&=&           \pE\brs{\rvx(u+\tau)\rvx^\ast(u)}
       \hspace{4ex}u=t-\tau \iff t=u+\tau
   \\&\eqd&        \Rxx(\tau)
\end{eqnarray*}

\[\begin{array}{*{10}{l}}
   \Reb{\Rxx(\tau)}
     &=& \Reb{\Rxx^\ast(-\tau)}
     &=& \Reb{\Rxx(-\tau)}
\\
   \Imb{\Rxx(\tau)}
     &=& \Imb{\Rxx^\ast(-\tau)}
     &=& -\Imb{\Rxx(-\tau)}
\\
   |\Rxx(\tau)|
     &=& |\Rxx^\ast(-\tau)|
     &=& |\Rxx(-\tau)|
\\
   \angle\Rxx(\tau)
     &=& \angle\Rxx^\ast(-\tau)
     &=& -\angle\Rxx(-\tau)
\end{array}\]

\end{proof}


%---------------------------------------
\section{Whitening}
\index{whitening filter}
\label{sec:whiten}
%---------------------------------------
Simple algebraic operations on white noise processes
(processes with autocorrelation $\Rxx(\tau)=\delta(\tau)$)
often produce {\em colored} noise
(processes with autocorrelation $\Rxx(\tau)\ne\delta(\tau)$).
Sometimes we would like to process a colored noise process
to produce a white noise process.
This operation is known as {\em whitening}.
Reasons for why we may want to whiten a noise process include
\begin{enume}
   \item Samples from a white noise process are uncorrelated.
         If the noise process is Gaussian, then these samples
         are also independent which often greatly simplifies analysis.
   \item Any orthonormal basis can be used to decompose a white noise process.
         This is not true of a colored noise process.
         Karhunen--Lo\`eve expansion can be used to decompose colored noise.
         \footnote{{\em Karhunen--Lo\`eve expansion}: \prefp{sec:KL}}
\end{enume}

%---------------------------------------
\begin{definition}
\index{rational expression}
\index{poles}
\index{zeros}
%---------------------------------------
A \textbf{rational expression} $\fp(s)$ is a polynomial divided by a polynomial
such that
\defbox{
   \fp(s) = \frac{\ds\sum_{n=0}^N b_n s^n}{\ds\sum_{n=0}^M a_n s^n}.
}
The \textbf{zeros} of a rational expression are the roots of its numerator polynomial.
The \textbf{poles} of a rational expression are the roots of its denominator polynomial.
\end{definition}

%---------------------------------------
\begin{definition}
\index{minimum phase}
\index{rational expression}
%---------------------------------------
Let $\Lh(s)$ be the Laplace transform of the impulse response of a filter.
If $\Lh(s)$ can be expressed as a rational expression with poles and zeros at
$a_n + ib_n$,
then the filter is \textbf{minimum phase} if each $a_n<0$
(all roots lie in the left hand side of the complex $s$-plane).
\end{definition}

Note that if $L(s)$ has a root at $s=-a+ib$, then
$L^\ast(-s^\ast)$ has a root at
  \[  -s^\ast = -(-a+ib)^\ast = -(-a-ib) = a+ib.   \]
That is, if $L(s)$ has a root in the left hand plane,
then $L^\ast(-s^\ast)$ has a root directly opposite across the imaginary
axis in the right hand plane (see \prefp{fig:s-roots}).
A causal stable filter $\hat{h}(s)$ must have all of its poles in the
left hand plane.
A minimum phase filter is a filter with both its poles and zeros in the
left hand plane.
One advantage of a minimum phase filter is that its recipricol
(zeros become poles and poles become zeros)
is also causal and stable.

\begin{figure}[ht]\color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.2mm}
\begin{picture}(200,230)(-100,-100)
  %\graphpaper[10](0,0)(200,200)
  \thicklines
  \put(-100 ,   0 ){\line(1,0){200} }
  \put(   0 ,-100 ){\line(0,1){200} }
  \thicklines
  \qbezier[20](-70, 40)(  0, 40)( 70, 40)
  \qbezier[ 8]( 70,  0)( 70, 20)( 70, 40)
  \qbezier[ 8](-70,  0)(-70, 20)(-70, 40)

  \qbezier[ 8](-30,-60)(  0,-60)( 30,-60)
  \qbezier[10]( 30,  0)( 30,-30)( 30,-60)
  \qbezier[10](-30,  0)(-30,-30)(-30,-60)

  \put(  70,  -10 ){\makebox(  0,0)[t]{$+a_z$} }
  \put( -70,  -10 ){\makebox(  0,0)[t]{$-a_z$} }
  \put(  30,   10 ){\makebox(  0,0)[b]{$+a_p$} }
  \put( -30,   10 ){\makebox(  0,0)[b]{$-a_p$} }
  \put( 105 ,   0 ){\makebox(  0,0)[l]{$\Re$}  }
  \put(   0 , 105 ){\makebox(  0,0)[b]{$\Im$}  }

  \put(  70 ,  40 ){\circle{10}}
  \put( -70 ,  40 ){\circle{10}}
  \put( -30 , -60 ){\makebox(0,0){$\times$}}
  \put(  30 , -60 ){\makebox(0,0){$\times$}}

  \put(  80 ,  40 ){\makebox(0,0)[l]{zero of $L^\ast(-s^\ast)$}}
  \put( -80 ,  40 ){\makebox(0,0)[r]{zero of $L(s)$}}
  \put(  40 , -60 ){\makebox(0,0)[l]{pole of $L^\ast(-s^\ast)$}}
  \put( -40 , -60 ){\makebox(0,0)[r]{pole of $L(s)$}}
\end{picture}
\end{fsL}
\end{center}
\caption{
   Mirrored roots in complex-s plane
   \label{fig:s-roots}
   }
\end{figure}



\begin{figure}[ht]\color{figcolor}
\begin{fsK}
\begin{center}
  \setlength{\unitlength}{0.2mm}
  \begin{picture}(700,100)(-100,-50)
  \thicklines
  %\graphpaper[10](0,0)(160,80)
  \put(-100,  10 ){\makebox (100, 40)[b]{$\rvx(t)$}                  }
  \put(-100, -50 ){\makebox (100, 40)[t]{$\Rxx(\tau)$}               }
  \put(-100, -50 ){\makebox (100, 40)[b]{$\Sxx(s)$}                  }
  \put(-100,   0 ){\vector  (  1,  0){100}                           }

  \put(   0, -50 ){\framebox(100,100)   {$\conv\gamma(t)$}           }
  \put(   0, -40 ){\makebox (100, 80)[t]{whitening}                  }
  \put(   0, -40 ){\makebox (100, 80)[b]{$\Gamma(s)$}                }
  \put( 100,   0 ){\vector  (  1,  0)   {200}                        }
  \put( 100,  10 ){\makebox (200, 40)[t]{white noise process}        }
  \put( 100,  10 ){\makebox (200, 40)[b]{$\vw(t)$}                 }
  \put( 100, -50 ){\makebox (200, 40)[t]{$\Rww(\tau)=\delta(\tau)$}  }
  \put( 100, -50 ){\makebox (200, 40)[b]{$\Sww(s)=1$}                }

  \put( 300, -50 ){\framebox(100,100)   {$\conv l(t)$}               }
  \put( 300, -40 ){\makebox (100, 80)[t]{innovations}                }
  \put( 300, -40 ){\makebox (100, 80)[b]{$L(s)$}                     }
  \put( 400,   0 ){\vector  (  1,  0)   {100}                        }
  \put( 400,  10 ){\makebox (100, 40)[b]{$\rvx(t)$}                  }
  \put( 400, -50 ){\makebox (200, 40)[t]{$\Rxx(\tau)=l(\tau)\conv l^\ast(-\tau)$}  }
  \put( 400, -50 ){\makebox (200, 40)[b]{$\Sxx(s)=L(s)L^\ast(-s^\ast)$}  }
  \end{picture}
\end{center}
\end{fsK}
\caption{
   Innovations and whitening filters
   \label{fig:innovations}
   }
\end{figure}

The next theorem demonstrates a method for ``whitening"
a random process $\fx(t)$ with a filter constructed from a decomposition
of $\Rxx(\tau)$.
The technique is stated precisely in \prefp{thm:innovations}
and illustrated in \prefp{fig:innovations}.
Both imply two filters with impulse responses $l(t)$ and $\gamma(t)$.
Filter $l(t)$ is referred to as the \textbf{innovations filter}
(because it generates or ``innovates" $\fx(t)$ from a white noise
process $\fw(t)$)
and $\gamma(t)$ is referred to as the \textbf{whitening filter}
because it produces a white noise sequence when the input sequence
is $\fx(t)$.
\footnote{\citerpp{papoulis}{401}{402}}


%---------------------------------------
\begin{theorem}
\label{thm:innovations}
%---------------------------------------
Let $\fx(t)$ be a WSS random process with autocorrelation $\Rxx(\tau)$
and spectral density $\Sxx(s)$.
\textbf{If} $\Sxx(s)$ has a \textbf{rational expression},
then the following are true:
\begin{enume}
   \item There exists a rational expression $L(s)$ with minimum phase
         such that
         \[ \Sxx(s) = L(s)L^\ast(-s^\ast). \]
   \item An LTI filter for which the Laplace transform of
         the impulse response $\gamma(t)$ is
         \[ \Gamma(s) = \frac{1}{L(s)} \]
         is both causal and stable.
   \item If $\fx(t)$ is the input to the filter $\gamma(t)$,
         the output $\fy(t)$ is a \textbf{white noise sequence} such that
         \[ \Syy(s)=1 \hspace{2cm} \Ryy(\tau)=\delta(\tau).\]
\end{enume}
\end{theorem}

\begin{proof}
\begin{eqnarray*}
   \Sww(s)
     &=& \Gamma(s)\Gamma^\ast(-s^\ast) \Sxx(s)
   \\&=& \frac{1}{L(s)} \frac{1}{L^\ast(-s^\ast)} \Sxx(s)
   \\&=& \frac{1}{L(s)} \frac{1}{L^\ast(-s^\ast)} L(s) L^\ast(-s^\ast)
   \\&=& 1
\end{eqnarray*}
\end{proof}



