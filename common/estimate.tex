%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================

%======================================
\chapter{Estimation Overview}
\label{app:est}
\label{chp:est}
%======================================
%======================================
\section{Model-based estimation}
\label{sec:model-based}
%======================================
Starting in the 1980s and continuing into 2019, James V. Candy has proposed 
the \structd{model-based approach}{\footnotemark} 
to estimation. This approach partitions the task of estimation into three parts:
\footnotetext{
  \citerg{candy1985}{9780070097254},
  \citerg{candy1988}{9780070097513},
  \citeP{candy1992},
  \citerpg{candy2005}{5}{9780471732662},
  \citerpg{candy2019}{7}{9781119457763}.
  ``Dr. Candy received the 
  IEEE Distinguished Technical Achievement Award for the
  \emph{development of model-based signal processing in ocean acoustics}":
  \citerpg{rossing2015}{1234}{9781493907557}
  }
\begin{enumerate}
  \item The \structd{model}:\footnote{
          \citerppc{box1976}{173}{207}{Chapter 6 ``Model Identification"}
          }
        The presumed system architecture,
        ``the form of which is usually suggested by prior knowledge, 
        physical understanding or guesswork."\footnote{\citerp{scargle1979}{5}}
        A very common example is the \structe{additive noise model}.
  \item The \opd{algorithm}: 
        The operation used to calculate the estimate.
        An \ope{algorithm} may also be called a \opd{processor},
        \opd{filter}, or \opd{method}.
  \item The \fnctd{criterion function}: 
        A \fncte{function} which measures the performance
        of the algorithm.
        This function may also be called a \fnctd{cost function}. 
\end{enumerate}

%=======================================
\subsection{Estimation models}
%=======================================
\begin{figure}
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
     \tboxc{\includegraphics{graphics/sys_xvy.pdf}}
    &\tboxc{\includegraphics{graphics/sysH_cnoise.pdf}}
    &\tboxc{\includegraphics{graphics/sysH_mnoise.pdf}}
    \\
      \structe{additive noise model}
    & \structe{communication system model}
    & \structe{measurement noise model}
    \\\hline
  \end{tabular}
  \caption{Some estimation models \label{fig:estmodels}}
\end{figure}
Estimation modeling includes several sub-models:
\begin{enumerate}
  \item The signal model (parametric, nonparametric)---more on this below.
  \item The noise model---this can be assumed (Guassian is a common choice) or estimated \xref{chp:pdfest}.
  \item The model architecture \xref{fig:estmodels}---additive noise model is a common choice, with justification from \thme{Wold's Theorem}.
\end{enumerate}

As for the the signal model,
let $\rvx(t;\theta)$ be a signal with parameter $\theta$.
There are three basic types of signal models, leading to three fundamental types of estimation:

\begin{enume}
   \item \ope{detection}:
      \begin{liste}
         \item The waveform $\rvx(t;\theta_n)$ is known except for the value of parameter $\theta_n$.
         \item The parameter $\theta_n$ is one of a finite set of values.
         \item Estimate $\theta_n$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{parametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t;\theta)$ is known except for the value of parameter $\theta$.
         \item The parameter $\theta$ is one of an infinite set of values.
         \item Estimate $\theta$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{nonparametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t)$ is unknown and assumed without any parameter $\theta$.
         \item Estimate $\rvx(t)$.
      \end{liste}
\end{enume}


%=======================================
\subsection{Estimation algorithms}
\label{ref:sec:parameter-est}
%=======================================
\begin{figure}
\centering%
\includegraphics{graphics/estimation_techniques.pdf}
\caption{Estimation Algorithms\label{fig:est-tech}}
\end{figure}
Estimation algorithms include the following \xref{fig:est-tech}:\footnote{%
  \citerpgc{nelles2001}{26}{3540673695}{``Fig 2.2 Overview of linear and nonlinear optimization techniques"},
  \citerpgc{nelles2001}{33}{3540673695}{``Fig 2.5 The Bayes method is the most general approach but\ldots"},
  \citerpgc{nelles2001}{63}{3540673695}{``Table 3.3 Relationship between linear recursive and nonlinear optimization techniques"},
  \citerpg{nelles2001}{66}{3540673695},
  \citerpgc{clarkson1993}{276}{0849386098}{``Figure 6.1.1 Options for the adaptive algorithm design."},
  \citerppgc{vaseghi2000}{4}{5}{9780471626923}{\textsection ``1.2 Signal Processing Methods"}
  }
\begin{enume}
   \item Bayesian Networks \xref{chp:bayesnets}
   \item Gradient Search
   \item Bayesian signal processing
   \item Neural Networks
   \item Direct search
\end{enume}

Bayesian signal processing involves estimating the joint-pdf of a process \xref{chp:pdfest},
and then integrating over a portion of the this (possibly high dimensional) pdf to find an MAP estimate.
The integration itself may be estimated using \ope{Monte Carlo Integration}.\footnote{
  \citerppgc{candy2009}{4}{7}{9780470430576}{\textsection {\scshape``1.3 Simulation-based approach to Bayesian Processing"}},
  \citerpgc{liu2013}{1}{9780387763712}{\textsection 1.1 The Need of Monte Carlo Techniques}
  }

%20191005%Sequential decoding is a non-linear estimation family.
%20191005%Perhaps the most famous of these is the Veterbi algorithm which
%20191005%uses a trellis to calculate the estimate.
%20191005%The Verterbi algorithm has been shown to yield an optimal estimate
%20191005%in the maximal likelihood (ML) sense.
%20191005%Norm minimization and gradient search algorithms are all linear algorithms.
%20191005%While this restriction to linear operations often simplifies calculations,
%20191005%it often yields an estimate that is not optimal in the ML sense.

%=======================================
\subsection{Estimation criterion function}
\label{sec:est_criterion}
%=======================================
\begin{figure}
\centering%
\includegraphics{graphics/latestimation.pdf}
\caption{
   Estimation criterion
   \label{fig:est-criterion}
   }
\end{figure}

Optimization requires a criterion against which the quality of an
estimate is measured.\footnote{\citergc{srv}{chapters 3, 5}{013125295X}.}
The most demanding and general criterion is the \prope{Bayesian} criterion.
The Bayesian criterion requires knowledge of the probability
distribution functions and the definition of a \fncte{cost function}.
Other criterion are special cases of the Bayesian criterion
such that the cost function is defined in a special way,
no cost function is defined, and/or the distribution is not known
\xref{fig:est-tech}.

%--------------------------------------
\begin{definition}
\index{MAP}
\index{ML}
\index{maximum a-posteriori}
\index{maximum likelihood}
\label{def:MAP}
\label{def:ML}
\label{def:estB}
\label{def:estMS}
\label{def:estMM}
\label{def:estMAP}
\label{def:estML}
%--------------------------------------
Let\\
$\begin{array}{FlM}
    (A).& \rvx(t;\theta)            & be a random process with unknown parameter $\theta$
  \\(B).& \rvy(t)                   & an observed random process which is statistically dependent on $\rvx(t;\theta)$
  \\(C).& \fCost(\theta,\pdfp(x,y)) & be a cost function.
\end{array}$
\\
Then the following \fnctd{estimate}s are defined as follows:
\defbox{\begin{array}{FMMlc>{\ds}l}
     (1).&\fnctd{Bayesian estimate}                         &                          & \estB   &\eqd& \argmin_{\theta} C(\theta,\pdfp(x,y))
   \\(2).&\fnctd{Mean square estimate}                      &(``\fnctd{MS  estimate}") & \estMS  &\eqd& \argmin_{\theta} \E\norm{C(\theta,\pdfp(x,y))}^2
   \\(3).&\fnctd{mini-max estimate}                         &(``\fnctd{MM  estimate}") & \estMM  &\eqd& \argmin_{\theta}\max_{\pdfp} C(\theta,\pdfp(x,y))
   \\(4).&\mc{2}{M}{\begin{tabular}[t]{@{}l}\fnctd{maximum a-posteriori probability estimate}\\
                                         (``\fnctd{MAP estimate}")
                    \end{tabular}}
         & \estMAP &\eqd& \argmax_{\theta} \psP\setn{\rvx(t;\theta)|\rvy(t)}
   \\(5).&\fnctd{maximum likelihood estimate}               &(``\fnctd{ML  estimate}") & \estML  &\eqd& \argmax_{\theta} \psP\setn{\rvy(t)|\rvx(t;\theta)}
\end{array}}
\end{definition}

%--------------------------------------
\begin{theorem}
\label{thm:map=ml}
%--------------------------------------
Let $\rvx(t;\theta)$ be a random process with unknown parameter $\theta$.
\thmbox{
  \brb{\text{$\psP\setn{\theta}=$\prope{constant}}}
  \quad\implies\quad
  \brb{\estMAP = \estML}
  }
\end{theorem}
\begin{proof}
\begin{align*}
   \estMAP
     &\eqd \argmax_{\theta} \psP\setn{\rvx(t;\theta)|\rvy(t)}
     &&    \text{by definition of $\estMAP$}
     &&    \text{\xref{def:estMAP}}
   \\&\eqd \argmax_{\theta} \frac{\psP\setn{\rvx(t;\theta) \land \rvy(t)}}
                               {\psP\setn{r(t)}}
     && \text{by definition of \fncte{conditional probability}}
     && \text{\xref{def:conprob}}
   \\&\eqd \argmax_{\theta} \frac{\psP\setn{r(t) | \rvx(t;\theta) }\psP\setn{\rvx(t;\theta) }}
                               {\psP\setn{\rvy(t)}}
     && \text{by definition of \fncte{conditional probability}}
     && \text{\xref{def:conprob}}
   \\&=    \argmax_{\theta} \psP\setn{\rvy(t) | \rvx(t;\theta) }\psP\setn{\rvx(t;\theta)}
     &&\mathrlap{\text{because $\rvy(t)$ is independent of $\theta$}}
   \\&=    \argmax_{\theta} \psP\setn{\rvy(t) | \rvx(t;\theta) }
   \\&\eqd \estML
     &&  \text{by definition of $\estML$}
     &&  \text{\xref{def:estML}}
\end{align*}
\end{proof}

%=======================================
\section{Estimation applications}
%=======================================
Applications of estimation include the following:
\begin{enumerate}
  \item \opb{prediction}---for speech processing, financial markets, etc.
  \item \opb{deconvolution}---estimate a system transfer function $\estH(z)$ and calculate it's inverse $\estH^{-1}(z)$.
  \item \opb{de-noising}---estimate a signal $\fx(t)$ that is buried in noise $\fv(t)$.
\end{enumerate}

%=======================================
\section{Measures of estimator quality}
\label{sec:quality}
%=======================================
%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{silverman1986}{35}{9780412246203}{\textsection ``1.3.2 Measures of discrepancy\ldots"},
  \citerpgc{clarkson1993}{50}{0849386098}{``c) Mean-Squared Error"},
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"},
  \citerp{bendat1966}{183}{\textsection ``5.3 Statistical Errors for Parameter Estimates"}
  }
\label{def:mse}
%---------------------------------------
\defbox{
  \begin{array}{M}
    The \fnctd{mean square error} $\mse(\estT)$ of an estimate $\estT$\\ 
    of a parameter $\theta$ is defined as
  \end{array}
  \qquad
  \begin{array}{rc>{\ds}l}
    \mse(\estT) &\eqd& \pE\brs{\brp{\estT-\theta}^2}
  \end{array}
  }
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"}
  }
\label{def:nre}
%---------------------------------------
\defbox{
  \begin{array}{M}
    The \fnctd{normalized rms error} $\nre(\estT)$\\ 
    of an estimate $\estT$\\ 
    of a parameter $\theta$ is defined as
  \end{array}
  \qquad
  \begin{array}{rc>{\ds}l}
    \nre(\estT) &\eqd& \frac{\sqrt{\mse(\estT)}}{\theta} 
                 \eqd \frac{\sqrt{\pE\brs{\brp{\estT-\theta}^2}}}{\theta}
  \end{array}
  }
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{silverman1986}{35}{9780412246203}{\textsection ``1.3.2 Measures of discrepancy\ldots"},
  \citePpc{rosenblatt1956}{835}{``integrated mean square error"}
  }
\label{def:mise}
%---------------------------------------
\defbox{
  \begin{array}{M}
    The \fnctd{mean integrated square error} $\mise(\estT)$\\
    of an estimate $\estT$ of a parameter $\theta$ is defined as
  \end{array}
  \qquad
  \begin{array}{rc>{\ds}l}
    \mise(\estT) &\eqd& \pE\int_{\theta\in\R}\brs{\brp{\estT-\theta}^2}
  \end{array}
  }
\end{definition}

The \fncte{mean square error} of $\estT$ can be expressed as the sum of two components:
the variance of $\estT$ and the bias of $\estT$ squared (next Theorem).
For an example of \pref{thm:mse} in action, see the proof for the $\mse(\meanest)$ of the 
\fncte{arithmetic mean estimate} as provided in \prefpp{thm:mse_mean}.
%---------------------------------------
\begin{theorem}
\footnote{
  \citerpg{choi1978}{76}{9780135016190},
  \citerpgc{kay1988}{45}{8131733564}{\textsection\scshape``3.3 Estimation Theory"},
  \citerpgc{stuart1991}{629}{9780340560235}{``Minium mean-square-error estimation"},
  \citerpgc{clarkson1993}{51}{0849386098}{\textsection ``2.6 Estimation of Moments"},
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"},
  \citerp{bendat1966}{183}{\textsection ``5.3 Statistical Errors for Parameter Estimates"},
  \citerpgc{bendat1980}{39}{0471058874}{\textsection ``2.4.1 Bias versus Random Errors"}
  }
\label{thm:mse}
%---------------------------------------
Let $\mse(\estT)$ be the \fncte{mean square error} \xref{def:mse} 
and $\nre(\estT)$    the \fncte{normalized rms error} \xref{def:nre} of an estimator $\estT$.
\thmbox{\begin{array}{rc>{\ds}l | rc>{\ds}l}
  \mse(\estT) &=&
      \mcom{\pE\brs{\brp{\estT-\pE\estT}^2}}{variance of $\estT$}
    + \mcom{\brs{\pE\estT - \theta}^2}{bias of $\estT$ squared}
  &
  \nre(\estT) &=&
    \frac{\sqrt{
    \pE\brs{\brp{\estT-\pE\estT}^2} + \brs{\pE\estT - \theta}^2
    }}{\theta}
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \mse(\estT)
    &\eqd \pE\brs{\brp{\estT-\theta}^2}
    && \text{by definition of $\mse$}
    && \text{\xref{def:mse}}
  \\&= \mathrlap{\pE\brs{\brp{\estT\mcom{-\pE\estT+\pE\estT}{$0$}-\theta}^2}
     \qquad\text{by \prope{additive identity} property of $\fieldC$}}
  \\&= \pE\brs{
         \brp{\estT-\pE\estT}^2
        +\mcom{\brp{\pE\estT-\theta}^2}{constant}
        -2\brp{\estT-\pE\estT}\brp{\pE\estT-\theta}
       }
    && \text{by \thme{Binomial Theorem}}
    && \text{\ifxref{polynom}{thm:binomial}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2	
        -2\pE\brs{
         \estT\pE\estT
        -\estT\theta
        -\pE\estT\estT
        +\pE\estT\theta
        }
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
        -2\mcom{\brs{
         \pE\estT\pE\estT
        -\pE\estT\pE\theta
        -\pE\estT\pE\estT
        +\pE\estT\pE\theta
        }}{$0$}
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
\end{align*}
\end{proof}

%--------------------------------------
\begin{definition}
\footnote{
  \citerpg{choi1978}{76}{9780135016190},
  \citerpgc{shao2003}{161}{9780387953823}{\textsection ``The UMVUE"},
  \citerpgc{bolstad2007}{164}{9780470141151}{\textsection ``Minimum Variance Unbiased Estimator"}
  }
%--------------------------------------
\defboxt{
  An estimate $\estT$ of a parameter $\theta$ is a
  \propd{minimum variance unbiased estimator} (\propd{MVUE}) if 
  \\\indentx$\begin{array}{FMD}
    (1). & $\pE\estT=0$ (\prope{unbiased}) & and
  \\(2). & no other unbiased estimator $\estPhi$ has smaller variance $\var(\estPhi)$
  \end{array}$
  }
\end{definition}
