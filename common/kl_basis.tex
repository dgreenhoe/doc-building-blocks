%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%=======================================
\chapter{Sufficient Statistics}
%=======================================
%=======================================
\section{Main result}
%=======================================
\prefpp{thm:sstat} (next) shows that the finite set
$\setY\eqd\set{\fdoty_n}{n=1,2,\ldots,\xN}$ (a finite number of values) provides just as
good an estimate as having the entire $\rvy(t)$ waveform
(an uncountably infinite number of values)
with respect to the following cases:
\begin{enume}
   \item the conditional probability of $\rvx(t;\estT)$ given $\rvy(t)$
   \item the \fncte{MAP estimate} of the sequence
   \item the \fncte{ML estimate}  of the sequence.
\end{enume}
That is, even with a drastic reduction in the number of statistics
from uncountably infinite to finite $\xN$,
no quality is lost with respect to the estimators listed above.
This amazing result is very useful in practical system implementation
and also for proving other theoretical results
(notably estimation and detection theorems).

But first, some definitions (next) that are used repeatedly in this chapter.
%---------------------------------------
\begin{definition}
\label{def:sstat}
%---------------------------------------
Let $\Psi\eqd\set{\fpsi_n}{n=1,2,\ldots,\xN}$ be an \structe{orthonormal basis} 
for a parameterized function $\rvx(t;\theta)$ with parameter $\theta$.
Let $\fy(t)$ be $\fx(t;\theta)$ plus a \fncte{random process} $\rvv(t)$ such that 
\\\indentx$\ds
  \rvy(t)\eqd\rvx(t;\theta)+\rvv(t)
$\\
Let $\fdoty_n$, $\fdotx_n$, and $\fdotv_n$ be \ope{projections}\ifsxref{operator}{def:opP}
onto the \fncte{basis vector} $\fpsi_n(t)$ such that 
\defbox{\begin{array}{r c>{\ds}l c>{\ds}l c>{\ds}l D}
    \fdotx_n &\eqd& \opP_n\fx(t) &\eqd& \inprod{\fx(t)}{\fpsi_n(t)} &\eqd& \int_{t\in\R}\fx(t)\fpsi_n(t)\dt
  \\\fdotv_n &\eqd& \opP_n\fv(t) &\eqd& \inprod{\fv(t)}{\fpsi_n(t)} &\eqd& \int_{t\in\R}\fv(t)\fpsi_n(t)\dt
  \\\fdoty_n &\eqd& \opP_n\fy(t) &\eqd& \inprod{\fy(t)}{\fpsi_n(t)} &\eqd& \int_{t\in\R}\fy(t)\fpsi_n(t)\dt
\end{array}}
\\
Let the set $\setY$ be defined as $\setY\eqd\set{\fdoty_n}{1,2,\ldots,\xN}$ 
Let $\estMAP$ be the \fncte{MAP estimate} 
and $\estML$  be the \fncte{ML estimate} \xref{def:estML} of $\theta$.
\end{definition}

%---------------------------------------
\begin{theorem}[\thmd{Sufficient Statistic Theorem}]
\footnote{
  \citePpc{fisher1922}{316}{``Criterion of Sufficiency"}
  }
\label{thm:sstat}
\index{optimal receiver}
\index{MAP} \index{maximum a-posteriori probability estimation}
\index{ML}  \index{maximum likelihood estimation}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
Let $\estMAP$ be the \fncte{MAP estimate} 
and $\estML$  be the \fncte{ML estimate} \xref{def:estML} of $\theta$.
\thmbox{
  \brb{\begin{array}{FMD}
     (A). & $\rvv(t)$ is \prope{zero-mean}                       & and 
   \\(B). & $\rvv(t)$ is \prope{uncorrelated}                    &     
        \\&                                   (\prope{white})    & and 
   \\(C). & $\rvv(t)$ is \prope{Gaussian}                        & 
  \end{array}}
  \implies
  \mcom{\brb{\begin{array}{F rc>{\ds}l D}
     (1). & \mc{3}{l}{\psP\set{ \rvx(t;\estT)}{\rvy(t)} = \psP\set{\rvx(t;\estT)}{\setY}}        & and
   \\(2). & \estMAP                           &=& \argmax_{\estT} \psP\set{\rvx(t;\estT)}{\setY} & and
   \\(3). & \estML                            &=& \argmax_{\estT} \psP\set{\setY}{\rvx(t;\estT)} &
  \end{array}}}{the $\xN$ element set $\setY$ is a \prope{sufficient statistic} for estimating $\fx(t)$}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Definition: Let $\ds\rvv'(t) \eqd \rvv(t) - \sum_{n=1}^\xN \fdotv_n \fpsi_n(t)$.
        \label{idef:sstat_vprime}

  \item lemma: The relationship between $\setY$ and $\rvv'(t)$ is given by
        \label{ilem:sstat_Yvprime}
  \begin{align*}
     &\boxed{\rvy(t)}
     \\&= \sum_{n=1}^\xN \inprod{\rvy(t)}{\fpsi_n(t)}\fpsi_n(t) +
          \brs{\rvy(t)- \sum_{n=1}^\xN \inprod{\rvy(t)}{\fpsi_n(t)}\fpsi_n(t) }
       && \begin{array}{@{}M}%
            by \prope{additive identity} property\\
            of $\fieldC$%
          \end{array}
     \\&\eqd \sum_{n=1}^\xN \inprod{\rvy(t)}{\fpsi_n(t)}\fpsi_n(t) +
          \brs{\rvy(t)- \sum_{n=1}^\xN \inprod{\rvx(t)+\rvv(t)}{\fpsi_n(t)}\fpsi_n(t) }
       && \text{by definition of $\fy(t)$}
     \\&= \sum_{n=1}^\xN \fdoty_n\fpsi_n(t) +
          \mcom{\rvx(t)+\rvv(t)}{$\fy(t)$}
             - \mcom{\sum_{n=1}^\xN \inprod{\rvx(t)}{\fpsi_n(t)}\fpsi_n(t)}{$\fx(t)$}
             - \mcom{\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)}{$\rvv(t)-\rvv'(t)$}
       && \begin{array}{@{}M}
            by definition of $\fdoty_n$ and\\
            \prope{additive} property of $\inprodn$\\ 
            \xref{def:inprod}
          \end{array}
     \\&= \sum_{n=1}^\xN \fdoty_n\fpsi_n(t) +
          \rvx(t)+\rvv(t) - \rvx(t) - \brs{\rvv(t) - \rvv'(t)}
     \\&= \boxed{\sum_{n=1}^\xN \fdoty_n\fpsi_n(t) + \rvv'(t)}
  \end{align*}

  \item Definitions. Let \label{idef:sstat_dotxv}
        $\begin{array}[t]{@{}r c>{\ds}l c>{\ds}l D}
            \fdotx_n &\eqd& \inprod{\fx(t)}{\fpsi_n(t)} &\eqd& \int_{t\in\R}\fx(t)\fpsi_n(t)\dt & and 
          \\\fdotv_n &\eqd& \inprod{\rvv(t)}{\fpsi_n(t)} &\eqd& \int_{t\in\R}\rvv(t)\fpsi_n(t)\dt &.
        \end{array}$

  \item lemma: $\pE\brs{\fdotv_n\rvv(t)} = N_o\fpsi_n(t)$. Proof: \label{ilem:sstat_vdotv}
    \begin{align*}
      &\pE\brs{\fdotv_n\rvv(t)} 
      \\&\eqd \pE\brs{\brp{\int_{t\in\R}\rvv(u)\fpsi_n(u)\du} \rvv(t)}
        && \text{by definition of $\fdotv_n(t)$}
        && \text{\xref{idef:sstat_dotxv}}
      \\&= \pE\brs{\int_{t\in\R}\rvv(u)\rvv(t)\fpsi_n(u)\du}
        && \text{by \prope{linearity} of $\int\du$ operator}
      \\&= \int_{t\in\R}\pE\brs{\rvv(u)\rvv(t)}\fpsi_n(u)\du
        && \text{by \prope{linearity} of $\pE$}
        && \text{\xref{thm:pE_linop}}
      \\&= \int_{t\in\R}N_o\delta(u-t)\fpsi_n(u)\du
        && \text{by \prope{white} hypothesis}
      \\&= N_o\fpsi_n(t)
        && \text{by property of \fncte{Dirac delta} $\delta(t)$}
    \end{align*}

  \item lemma: $\setY$ and $\rvv'(t)$ are \prope{uncorrelated}:\label{ilem:sstat_uncorrelated} Proof:
  \begin{align*}
     &\pE\brs{\fdoty_n \rvv'(t)}
     \\&\eqd \pE\brs{\inprod{\rvy(t)}{\fpsi_n(t)}\brp{ \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)}}
       && \text{by definitions of $\fdoty_n$ and $\rvv'(t)$}
     \\&\eqd \pE\brs{\inprod{\rvx(t)+\rvv(t)}{\fpsi_n(t)}\left( \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)\right)}
       && \text{by definition of $\fy(t)$}
     \\&= \pE\brs{\Bigg(\inprod{\rvx(t)}{\fpsi_n(t)}+\inprod{\rvv(t)}{\fpsi_n(t)}\Bigg)
              \brp{ \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)}}
       && \begin{array}{@{}M}%
            by \prope{additive} property of\\ 
            $\inprodn$ \xref{def:inprod}%
          \end{array}
     \\&= \pE\brs{\Bigg(\fdotx_n+\fdotv_n\Bigg)
              \left( \rvv(t)-\sum_{n=1}^\xN \fdotv_n\fpsi_n(t)\right)}
       && \begin{array}{@{}M}%
            by definitions of $\fdotx_n$ and $\fdotv_n$\\
            \xref{idef:sstat_dotxv}
          \end{array}
     \\&= \pE\brs{\fdotx_n \rvv(t) - \fdotx_n \sum_{n=1}^\xN \fdotv_n\fpsi_n(t)
              +\fdotv_n \rvv(t) - \fdotv_n \sum_{n=1}^\xN \fdotv_n\fpsi_n(t) }
     \\&= \pE\brs{\fdotx_n \rvv(t)}
          - \pE\brs{\fdotx_n \sum_{n=1}^\xN \fdotv_n\fpsi_n(t)}
          + \pE\brs{\fdotv_n\rvv(t)}
          - \pE\brs{\sum_{m=1}^\xN \fdotv_n \fdotv_m\fpsi_m(t)}
       && \begin{array}{@{}M}%
            by \prope{linearity} of $\pE$\\%
            \xref{thm:pE_linop}%
          \end{array}
     \\&= \fdotx_n \cancelto{0}{\pE{\rvv(t)}}
          - \fdotx_n \sum_{n=1}^\xN \cancelto{0}{\pE\brs{\fdotv_n}}\fpsi_n(t)
          + \pE\brs{\fdotv_n\rvv(t)}
          - \sum_{m=1}^\xN \pE\brs{\fdotv_n \fdotv_m}\fpsi_m(t)
       && \begin{array}{@{}M}%
            by \prope{linearity} of $\pE$\\%
            \xref{thm:pE_linop}%
          \end{array}
     \\&= 0 - 0
          + \pE\brs{\fdotv_n\rvv(t)} 
          - \sum_{m=1}^\xN N_o\kdelta_{mn} \fpsi_m(t)
       && \text{by \prope{white} hypothesis}
     \\&= N_o\fpsi_n(t) - N_o\fpsi_n(t)
       && \text{by \pref{ilem:sstat_vdotv}}
     \\&= 0
     \\&\implies\text{\propb{uncorrelated}}
  \end{align*}
  
  \item lemma: $\setY$ and $\rvv'(t)$ are \prope{independent}. Proof:\\
        By \pref{ilem:sstat_uncorrelated}, $\fdoty_n$ and $\rvv'(t)$ are \prope{uncorrelated}.
        By hypothesis, they are \prope{Gaussian}, and thus are also \propb{independent}.
        \label{ilem:sstat_independent}
  
  \item Proof that $\psP\set{\rvx(t;\estT)}{\rvy(t)}=\psP\set{\rvx(t;\estT)}{\fdoty_1,\;\fdoty_2,\ldots,\fdoty_{\xN}}$:
        \label{item:sstat_P}
  
  \begin{align*}
     \psP\set{\rvx(t;\estT)}{\rvy(t)}
       &= \psP\set{\rvx(t;\estT)}{\sum_{n=1}^\xN\fdoty_n \fpsi_n(t) + \rvv'(t)}
     \\&= \psP\set{\rvx(t;\estT)}{\setY, \rvv'(t)}
       && \begin{array}{M}
            because $\setY$ and $\rvv'(t)$ can be\\ 
            extracted by $\inprod{\cdots}{\fpsi_n(t)}$
          \end{array}
     \\&= \frac{\psP\set{\setY, \rvv'(t)}{\rvx(t;\estT)}  P\setn{\rvx(t;\estT)} }
               {\psP\setn{\setY,\rvv'(t)}}
     \\&= \frac{\psP\set{ \setY}{\rvx(t;\estT)}\psP\set{ \rvv'(t)}{\rvx(t;\estT)}\psP\setn{\rvx(t;\estT)}}
               {\psP\setn{\setY}\psP\setn{\rvv'(t)}}
       && \text{by \prope{independence} of $\setY$ and $\rvv'(t)$ \xref{ilem:sstat_independent}}
     \\&= \frac{\psP\set{ \setY}{\rvx(t;\estT)}\psP\setn{ \rvv'(t)}\psP\setn{\rvx(t;\estT)}}
               {\psP\setn{\setY}\psP\setn{\rvv'(t)}}
       && \text{by \prope{independence} of $\fx$ and $\rvv$}
     \\&= \frac{\psP\set{\setY}{\rvx(t;\estT)} \psP\setn{\rvx(t;\estT)}}
               {\psP\setn{\setY}}
     \\&= \frac{\psP\setn{ \setY,\rvx(t;\estT)}}
               {\psP\setn{\setY}}
     \\&= \psP\set{\rvx(t;\estT)}{\setY}
       && \begin{array}{@{}M}%
            by definition of \fncte{conditional probability}\\ 
            \xref{def:conprob}
          \end{array}
  \end{align*}
  
  \item Proof that $\setY$ is a \prope{sufficient statistic} for the \vale{MAP estimate}:
  \begin{align*}
     \estMAP
       &\eqd \argmax_{\estT} \psP\set{\rvx(t;\estT)}{\rvy(t)}
       &&    \text{by definition of \vale{MAP estimate} \xref{def:estMAP}}
     \\&=    \argmax_{\estT} \psP\set{\rvx(t;\estT)}{\setY}
       &&    \text{by \pref{item:sstat_P}}
  \end{align*}
  
  \item Proof that $\setY$ is a \prope{sufficient statistic} for the \fncte{ML estimate}:
  \begin{align*}
     \estML
       &\eqd \argmax_{\estT} \psP\set{\rvy(t)}{\rvx(t;\estT)}
       &&    \text{by definition of \fncte{ML estimate} \xref{def:estML}}
     \\&=    \argmax_{\estT} \psP\set{\sum_{n=1}^\xN\fdoty_n\fpsi_n(t)+\rvv'(t)}{\rvx(t;\estT)}
     \\&=    \argmax_{\estT} \psP\set{\setY,\rvv'(t)}{\rvx(t;\estT)}
       &&    \text{because $\setY$ and $\rvv'(t)$ can be extracted by  $\inprod{\cdots}{\fpsi_n(t)}$}
     \\&=    \argmax_{\estT} \psP\set{\setY}{\rvx(t;\estT)}\psP\setn{\rvv'(t)}{\rvx(t;\estT)}
       &&    \text{by \prope{independence} of $\setY$ and $\rvv'(t)$}
     \\&=    \argmax_{\estT} \psP\set{\setY}{\rvx(t;\estT)}\psP\setn{\rvv'(t)}
       &&    \text{by \prope{independence} of $\rvx(t)$ and $\rvv'(t)$}
     \\&=    \argmax_{\estT} \psP\set{\setY}{\rvx(t;\estT)}
       &&    \text{by \prope{independence} of $\rvv'(t)$ and $\estT$}
  \end{align*}
\end{enumerate}
\end{proof}

%======================================
\section{Additive noise}
%======================================
%Depending on the nature of the channel (additive, white, and/or \prope{Gaussian})
%we can know certain characteristics of the noise and received statistics.
%These are described in the next four theorems.

%---------------------------------------
\begin{theorem}[\thme{Sufficient statistics for additive noise system}]
\label{thm:an_stats}
\index{projection statistics!Additive noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \brb{\begin{array}{FMD}
    (A). & $\Psi$ is an \structe{orthonormal basis} for $\fx(t)$ & and
  \\(B). & $\fy(t) \eqd \fx(t) + \fv(t)$                         & (\prope{additive noise})
  \end{array}}
  \implies
\implies
\brb{\begin{array}{lrclD}
   \pE(\fdoty_n|\theta)       &= \fdotx_n(\theta) + \pE \fdotv_n & (\prope{additive noise})
\end{array}}
}
\end{theorem}
\begin{proof}
\begin{align*}
   \pE(\fdoty_n |\theta)
     &\eqd \pE\brp{\inprod{\rvy(t)}{\fpsi_n(t)}  |\theta}
     && \text{by definition of $\fdoty_n$}
   \\&= \pE{\inprod{\rvx(t;\theta)+\fv(t)}{\fpsi_n(t)}}
     && \text{by definition of $\fy(t)$}
   \\&= \pE{\inprod{\rvx(t;\theta)}{\fpsi_n(t)}} +   {\inprod{\fv(t)}{\fpsi_n(t)}}
     && \text{by \prope{additive} property of $\inprodn$}
     && \text{\xref{def:inprod}}
   \\&= \pE\brp{\inprod{\sum_{k=1}^\xN \fdotx_k(\theta) \fpsi_k(t)}{\fpsi_n(t)} + \fdotv_n}
     && \text{by definition of $\fdotv_n$}
   \\&= \pE\brp{\sum_{k=1}^\xN \fdotx_k(\theta) \inprod{\fpsi_k(t)}{\fpsi_n(t)} + \fdotv_n}
     && \text{by \prope{additive} property of $\inprodn$}
     && \text{\xref{def:inprod}}
   \\&= \pE\brp{\sum_{k=1}^\xN \fdotx_k(\theta) \kdelta_{k-n}(t) + \fdotv_n}
     && \text{by \prope{orthonormal} hypothesis of $\Psi$}
   \\&= \pE\brp{\fdotx_n(\theta) + \fdotv_n}
     && \text{by definition of $\kdelta$}
   \\&= \pE{\fdotx_n(\theta) } +   \pE{\fdotv_n}
     && \text{by \prope{linearity} of $\pE$}
     && \text{\xref{thm:pE_linop}}
   \\&= \fdotx_n(\theta)
\end{align*}
\end{proof}


%======================================
%\subsection{Additive gaussian noise channel}
%\label{sec:opCagn}
%======================================
\begin{figure}[ht] \color{figcolor}
\centering%
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotv_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdoty_n|\theta_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdoty_n|\theta_2)$} }
\end{picture}
\caption{
  Additive \prope{Gaussian} noise channel Statistics
   %\label{fig:awgn_stats}
   }
\end{figure}

%---------------------------------------
\begin{theorem}%[Additive \prope{Gaussian} noise projection statistics]
\label{thm:agn_stats}
\index{projection statistics!Additive \prope{Gaussian} noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \mcom{\brb{\begin{array}{FMD}
    (A). & $\Psi$ is an \structe{orthonormal basis} for $\fx(t)$ & and
  \\(B). & $\fy(t) \eqd \fx(t) + \fv(t)$                         & (\prope{additive noise})
  \\(C). & $\fv(t)\sim\pN(0,\sigma^2)$                            & (\prope{Gaussian})
  \end{array}}}{additive \prope{Gaussian} channel}
\implies
\brb{\begin{array}{rclD}
   \pE\fdotv_n           &=   & 0                               & \\%\text{\scriptsize(noise projection is zero-mean                )} \\
   \pE(\fdoty_n|\theta)  &=   & \fdotx_n(\theta)                & \\%\text{\scriptsize(expected receiver projection = transmitted projection )} \\
   \fdotv_n              &\sim& \pN{0}{\sigma^2}                & (noise projections are \prope{Gaussian}        ) \\
   \fdoty_n|\theta       &\sim& \pN{\fdotx_n(\theta)}{\sigma^2} & (receiver projections are \prope{Gaussian}     ) \\
\end{array}}
}
\end{theorem}
\begin{proof}
\begin{align*}
   \pE\fdotv_{n}
     &= \pE\inprod{\fn(t)}{\fpsi_n(t)}
   \\&= \inprod{\pE\fn(t)}{\fpsi_n(t)}
   \\&= \inprod{0}{\fpsi_n(t)}
   \\&= 0
\\
\\
   (\fdoty_n |\theta)
     &\eqd {\inprod{\rvy(t)}{\fpsi_n(t)}}  |\theta
   \\&=    {\inprod{\rvx(t;\theta)+\fn(t)}{\fpsi_n(t)}}
   \\&=    {\inprod{\rvx(t;\theta)}{\fpsi_n(t)}} +   {\inprod{\fn(t)}{\fpsi_n(t)}}
   \\&=    \inprod{\sum_{k=1}^\xN \fdotx_k(\theta) \fpsi_k(t)}{\fpsi_n(t)} + \fdotv_n
   \\&=    \sum_{k=1}^\xN \fdotx_k(\theta) \inprod{\fpsi_k(t)}{\fpsi_n(t)} + \fdotv_n
   \\&=    \fdotx_n(\theta)  + \fdotv_n
\\ \\
   \pE({\fdoty_n} | \theta)
     &= \pE\brs{\fdotx_n(\theta)  + \fdotv_n}
   \\&= \pE{\fdotx_n(\theta) } +   \pE{\fdotv_n}
   \\&= \fdotx_n(\theta)
\end{align*}

The distributions follow because they are linear operations on
Gaussian processes.
\end{proof}

%======================================
%\subsection{Additive white noise channel}
%\label{sec:opCawn}
%======================================
%---------------------------------------
\begin{theorem}%[Additive white noise projection statistics]
\label{thm:awn_stats}
\index{projection statistics!Additive white noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \mcom{\brb{\begin{array}{FMD}
    (A). & $\Psi$ is an \structe{orthonormal basis} for $\fx(t)$ & and
  \\(B). & $\fy(t) \eqd \fx(t) + \fv(t)$                         & (\prope{additive noise})
  \\(C). & $\fv(t)\sim\pN(0,\sigma^2)$                           & (\prope{Gaussian})
  \end{array}}}{\prope{additive Gaussian} channel}
\implies
\brb{\begin{array}{lclD}
   \pE\fdotv_n                            &=& 0                     & (noise projection is zero-mean) \\
   \pE(\fdoty_n|\theta)                   &=& \fdotx_n(\theta)      & (expected receiver projection = transmitted projection) \\
   \cov{\fdotv_n}{\fdotv_m}               &=& \sigma^2 \kdelta_{nm} & (noise projections are \prope{uncorrelated}) \\
   \cov{\fdoty_n|\theta}{\fdoty_m|\theta }&=& \sigma^2 \kdelta_{nm} & (receiver projections are \prope{uncorrelated})
\end{array}}
}
\end{theorem}
\begin{proof}
Because the noise is additive (see \prefpp{thm:an_stats})
\begin{align*}
   \pE\fdotv_{n}           &= 0  \\
   (\fdoty_n |\theta)      &= \fdotx_n(\theta)  + \fdotv_n \\
   \pE({\fdoty_n} | \theta) &= \fdotx_n(\theta).
\end{align*}

Because the noise is also \prope{white},
\begin{align*}
   \cov{\fdotv_m}{\fdotv_n}
      &= \cov{\inprod{\fn(t)}{\fpsi_m(t)}}{\inprod{\fn(t)}{\fpsi_n(t)}}
    \\&= \pE\brs{\inprod{\fn(t)}{\fpsi_m(t)} \inprod{\fn(t)}{\fpsi_n(t)}}
    \\&= \pE\brs{\inprod{\fn(t)}{\fpsi_m(t)} \inprod{n(\estT)}{\fpsi_n(\estT)}}
    \\&= \pE\brs{ \inprod{n(\estT)\inprod{\fn(t)}{\fpsi_m(t)}}{\fpsi_n(\estT)}}
    \\&= \pE\brs{ \inprod{\inprod{n(\estT)\fn(t)}{\fpsi_m(t)}}{\fpsi_n(\estT)}}
    \\&= \inprod{\inprod{\pE\brs{ n(\estT)\fn(t)}}{\fpsi_m(t)}}{\fpsi_n(\estT)}
    \\&= \inprod{\inprod{\sigma^2 \delta(t-\estT)}{\fpsi_m(t)}}{\fpsi_n(\estT)}
    \\&= \sigma^2 \inprod{\fpsi_n(t)}{\fpsi_m(t)}
    \\&= \brbl{\begin{array}{lM}
             \sigma^2 & for $n=m$ \\
             0        & for $n\ne m$.
          \end{array}}
\\
\\
   \cov{\fdoty_n|\theta}{\fdoty_m|\theta }
      &= \pE\brs{\fdoty_n \fdoty_m |\theta} - [\pE\fdoty_n|\theta][\pE\fdoty_m|\theta ]
    \\&= \pE\brs{(\fdotx_n(\theta) +\fdotv_n)(\fdotx_m(\theta) +\fdotv_m)} - \fdotx_n(\theta) \fdotx_m(\theta)
    \\&= \pE\brs{(\fdotx_n(\theta) +\fdotv_n)(\fdotx_m(\theta) +\fdotv_m)} - \fdotx_n(\theta) \fdotx_m(\theta)
    \\&= \pE\brs{\fdotx_n(\theta) \fdotx_m(\theta) +\fdotx_n(\theta) \fdotv_m+ \fdotv_n\fdotx_m(\theta) +\fdotv_n\fdotv_m } - \fdotx_n(\theta) \fdotx_m(\theta)
    \\&= \fdotx_n(\theta) \fdotx_m(\theta) + \fdotx_n(\theta) \pE\brs{\fdotv_m}+ \pE\brs{\fdotv_n}\fdotx_m(\theta) +\pE\brs{\fdotv_n\fdotv_m}  - \fdotx_n(\theta) \fdotx_m(\theta)
    \\&= 0 + \fdotx_n(\theta) \cdot0 + 0\cdot\fdotx_m(\theta) + \cov{\fdotv_n}{\fdotv_m}+[\pE\fdotv_n][\pE\fdotv_m]
    \\&= \sigma^2 \kdelta_{nm} + 0\cdot0
    \\&= \brbl{\begin{array}{lM}
             \sigma^2 & for $n=m$ \\
             0        & for $n\ne m$.
          \end{array}}
\end{align*}
\end{proof}

%======================================
%\subsection{Additive white gaussian noise channel}
%\label{sec:opCawgn}
%======================================
\begin{figure}[ht] \color{figcolor}
\centering%
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotv_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdoty_n|\theta_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdoty_n|\theta_2)$} }
\end{picture}
\caption{
  Additive white \prope{Gaussian} noise channel statistics
   \label{fig:awgn_stats}
   }
\end{figure}

%---------------------------------------
\begin{theorem}%[AWGN projection statistics]
\label{thm:awgn_stats}
\label{thm:ms_stats}
\index{projection statistics!Additive white \prope{Gaussian} noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \mcom{\brb{\begin{array}{FMD}
    (A). & $\Psi$ is an \structe{orthonormal basis} for $\fx(t)$ & and
  \\(B). & $\fy(t) \eqd \fx(t) + \fv(t)$                         & (\prope{additive noise})
  \\(C). & $\fv(t)$ is                                           & (\prope{white noise})
  \\(D). & $\fv(t)\sim\pN(0,\sigma^2)$                           & (\prope{Gaussian})
  \end{array}}}{\prope{additive white Gaussian} channel}
\implies
\brb{\begin{array}{FrclD}
     (1). & \fdotv_n                            &\sim& \pN{0}{\sigma^2}                     & (noise projections are \prope{Gaussian}        )
   \\(2). & \fdoty_n|\theta                     &\sim& \pN{\fdotx_n(\theta)}{\sigma^2}      & (receiver projections are \prope{Gaussian}     )
   \\(3). & \cov{\fdotv_n}{\fdotv_m}            &=   & \sigma^2 \kdelta_{nm}                & (noise projections are \prope{uncorrelated}    )
   \\(4). & \cov{\fdoty_n}{\fdoty_m }           &=   & \sigma^2 \kdelta_{nm}                & (receiver projections are \prope{uncorrelated} )
   \\(5). & \psp\{\fdotv_n=a \land \fdotv_m=b\} &=   & \psp\{\fdotv_n=a\}\psp\{\fdotv_m=b\} & (noise    projections are \prope{independent}  )
   \\(6). & \psp\{\fdoty_n=a \land \fdoty_m=b\} &=   & \psp\{\fdoty_n=a\}\psp\{\fdoty_m=b\} & (receiver projections are \prope{independent}  )
\end{array}}
}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Proof for (1) and (2) follow because the operations are \prope{linear} on \prope{Gaussian} processes.

  \item By \prefpp{thm:awn_stats} (for AWN channel)
    \begin{align*}
       \pE{\fdotv_{n}} &= 0
     \\\cov{\fdotv_m}{\fdotv_n} &= \sigma^2 \kdelta_{mn}
     \\\fdoty_n &= \fdotx_n  + \fdotv_n
     \\\pE{\fdoty_n} &= \fdotx_n
     \\\cov{\fdoty_n}{\fdoty_m } &= \sigma^2 \kdelta_{mn}
    \end{align*}

  \item Because the processes are \prope{Gaussian},
        \prope{uncorrelated} implies \prope{independent}.
\end{enumerate}
\end{proof}

%======================================
\section{Optimal symbol estimation}
\label{sec:awgn_est}
\index{maximum likelihood estimation}
%======================================
The AWGN projection statistics provided by
\prefpp{thm:awgn_stats} help generate the optimal
ML-estimates for a number of communication systems.
These ML-estimates can be expressed in either of two standard forms:
\begin{liste}
  \item {\bf Spectral decompostion}:
     The optimal estimate is expressed in terms of \hie{projections}
     of signals onto orthonormal basis functions.
  \item {\bf Matched signal}:
     The optimal estimate is expressed in terms of the (noisy)
     received signal correlated with (``matched" with)
     the (noiseless) transmitted signal.
\end{liste}
\prefpp{thm:estML_general} (next) expresses the general
optimal \fncte{ML estimate} in both of these forms.

Parameter detection is a special case of parameter estimation.
In parameter detection, the estimate is a member of an finite set.
In parameter estimation, the estimate is a member of an infinite set
\xref{sec:awgn_est}.


%---------------------------------------
\begin{theorem}[\thmd{General ML estimation}]
\index{maximum likelihood estimation!general}
\label{thm:estML_general}
\label{thm:ml_est_det}
%\citepp{srv}{157}{158}
%---------------------------------------
Let $\Psi$ be an orthonormal set spanning $\rvx(t)$ such that
\\\indentx$\begin{array}{rcl}
    \Psi     &\eqd& \seqn{\fpsi_1(t), \fpsi_2(t), \ldots, \fpsi_n(t)}
  \\\fdoty_n &\eqd& \inprod{\rvy(t)}{\fpsi_n(t)}
  \\\fdotx_n &\eqd& \inprod{\rvx(t)}{\fpsi_n(t)}
  \\\rvy(t)  &=   & \rvx(t;\estT) + \fn(t).
\end{array}$
\\
Then the optimal ML-estimate $\estML$ of parameter $\theta$ is
\thmbox{\begin{array}{rc>{\ds}l@{\qquad}D}
   \estML
     &=& \argmin_{\estT} \left[ \sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
       & (spectral decomposition)
   \\&=& \argmax_{\estT}
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\estT)}-\norm{\rvx(t;\estT)}^2 \right]
       & (matched signal)
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML
     &= \argmax_{\estT} \psP\set{\rvy(t)}{\rvx(t;\estT)}
   \\&= \argmax_{\estT} \psP\set{\fdoty_1,\fdoty_2,\ldots,\fdoty_n}{\rvx(t;\estT)}
     && \text{by \prefpp{thm:sstat}}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN \psP\set{\fdoty_n}{\rvx(t;\estT)}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN \pdfpb{\fdoty_n|\rvx(t;\estT)}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN
         \frac{1}{\sqrt{2\pi\sigma^2}}
         \exp{\frac{[\fdoty_n-\fdotx_n(\estT)]^2}{-2\sigma^2} }
     && \text{by \prefpp{thm:awgn_stats}}
   \\&= \argmax_{\estT}
         \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
         \exp{\frac{-1}{2\sigma^2} \sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 }
   \\&= \argmax_{\estT}
         \left[ -\sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
\\ \\
   \\&= \argmax_{\estT}
         \left[ -\lim_{N\to\infty}\sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
     && \text{by \prefpp{thm:sstat}}
   \\&= \argmax_{\estT}
         \left[ -\norm{\rvy(t)-\rvx(t;\estT)}^2 \right]
     && \text{by \thme{Plancheral's formula}}
     && \text{\xref{thm:plancherel}}
   \\&= \argmax_{\estT}
         \left[ -\norm{\rvy(t)}^2 +2\Real\inprod{\rvy(t)}{\rvx(t;\estT)}-\norm{\rvx(t;\estT)}^2 \right]
   \\&= \argmax_{\estT}
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\estT)}-\norm{\rvx(t;\estT)}^2 \right]
     && \text{because $\rvy(t)$ \prope{independent} of $\estT$}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{ML amplitude estimation}]
\label{thm:estML_amplitude}
\footnote{
  \citerppg{srv}{158}{159}{013125295X}
  }
\index{maximum likelihood estimation!amplitude}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
\begin{align*}
   \rvy(t)     &=     [\opCawgn\fx](t) = \rvx(t;a) + \fn(t) \\
   \rvx(t;a)   &\eqd  a  \sym(t).
\end{align*}
Then
\thmbox{\begin{array}{rclM}
  \estML[a]    &=&  \frac{1}{\norm{\lambda(t)}^2} \inprod{\rvy(t)}{\lambda(t)}
               &    (optimal ML-estimate of $a$)
             \\&=&  \frac{1}{\norm{\sym(t)}^2} \sum_{n=1}^\xN \fdoty_n \fdotlam_n
               & %\text{(optimal ML-estimate of $a$)}
\\\pE\estML[a] &=& a
               &   ($\estML[a]$ is \propb{unbiased})
\\\var\estML[a]&=& \frac{\sigma^2}{\norm{\sym(t)}^2}
               &  (variance of estimate $\estML[a]$)
\\\var\estML[a]&=& \mbox{CR lower bound}
               &   ($\estML[a]$ is an {\bf \prope{efficient} estimate})
\end{array}}
\end{theorem}

\begin{proof}\\
\begin{enumerate}
\item \fncte{ML estimate} in ``matched signal" form:
\begin{align*}
   \estML[a]
     &= \argmax_a
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\estT)}-\norm{\rvx(t;\phi)}^2 \right]
     && \text{by \prefpp{thm:estML_general}}
   \\&= \argmax_a
         \left[ 2\inprod{\rvy(t)}{a\lambda(t)}-\norm{a\lambda(t)}^2 \right]
     && \text{by hypothesis}
   \\&= \arg_a
         \left[ \pderiv{}{a}2a\inprod{\rvy(t)}{\lambda(t)}-\pderiv{}{a}a^2\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ 2\inprod{\rvy(t)}{\lambda(t)}-2a\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ \inprod{\rvy(t)}{\lambda(t)}=a\norm{\lambda(t)}^2 \right]
   \\&= \frac{1}{\norm{\lambda(t)}^2} \inprod{\rvy(t)}{\lambda(t)}
\end{align*}

\item \fncte{ML estimate} in ``spectral decomposition" form:
\begin{align*}
   \estML[a]
     &= \argmin_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}^2 }
     && \text{by \prefpp{thm:estML_general}}
   \\&= \arg_a
         \brp{ \pderiv{}{ a }\sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}^2=0 }
   \\&= \arg_a
         \brp{ 2\sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}\pderiv{}{ a }\fdotx_n( a )=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n - \inprod{ a \lambda(t)}{\fpsi_n(t)}}\pderiv{}{ a }\inprod{ a \lambda(t)}{\fpsi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \inprod{\lambda(t)}{\fpsi_n(t)}}\pderiv{}{ a }( a \inprod{\lambda(t)}{\fpsi_n(t)})=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \fdotlam_n } \inprod{\lambda(t)}{\fpsi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \fdotlam_n } \fdotlam_n=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \fdoty_n\fdotlam_n = \sum_{n=1}^\xN  a \fdotlam_n^2 }
   \\&= \brp{\frac{1}{\sum_{n=1}^\xN \fdotlam_n^2}}
         \sum_{n=1}^\xN \fdoty_n\fdotlam_n
   \\&= \frac{1}{\norm{\lambda(t)}^2}
         \sum_{n=1}^\xN \fdoty_n\fdotlam_n
\end{align*}

\item Prove that the estimate $\estML[a]$ is \propb{unbiased}:

\begin{align*}
   \pE\estML[a]
     &= \pE\frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} \rvy(t)\sym(t) \dt
     && \text{by previous result}
   \\&= \pE\frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} [ a \sym(t)+\fn(t)]\sym(t) \dt
     && \text{by hypothesis}
   \\&= \frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} \pE[ a \sym(t)+\fn(t)]\sym(t) \dt
     && \text{by linearity of $\int\cdot\dt$ and $\pE$}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \int_{t\in\R} \sym^2(t) \dt
     && \text{by $\pE$ operation}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \norm{\sym(t)}^2
     && \text{by definition of $\norm{\cdot}^2$}
   \\&=   a
\end{align*}

\item Compute the variance of $\estML[a]$:
\begin{align*}
  \pE \estML[a]^2
    &= \pE \left[ \frac{1}{\norm{\lambda(t)}^2} \int_{t\in\R} \rvy(t)\lambda(t) \dt\right]^2
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \rvy(t)\lambda(t) \dt \int_v \rvy(v)\lambda(v) \dv
        \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v [a\lambda(t) + \fn(t)][a\lambda(v) + \fn(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v
        [a^2\lambda(t)\lambda(v) + a\lambda(t)\fn(v) + a\lambda(v)\fn(t) + \fn(t)\fn(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v
        [a^2\lambda(t)\lambda(v) + 0 + 0 + \sigma^2\delta(t-v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v a^2\lambda^2(t)\lambda^2(v) \dv\dt +
        \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v \sigma^2\delta(t-v) \lambda(t) \lambda(v) \dv\dt
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        a^2 \int_{t\in\R} \lambda^2(t) \dt \int_v \lambda^2(v) \dv +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2\int_{t\in\R} \lambda^2(t) \dt
  \\&= a^2 \frac{1}{\norm{\lambda(t)}^4}
        \norm{\lambda(t)}^2 \norm{\lambda(v)}^2 +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2 \norm{\lambda(t)}^2
  \\&= a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}
\\
\\
  \var\estML[a]
    &= \pE \estML[a]^2 - (\pE \estML[a])^2
  \\&= \left.\left.\left(a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}\right) - \right( a^2 \right)
  \\&= \frac{\sigma^2}{\norm{\lambda(t)}^2}
\end{align*}

\item Compute the \ineq{Cram/'er-Rao Bound}:
\begin{align*}
   \pdfpb{\rvy(t)|\fx(t; a)}
     &=  \pdfpb{\fdoty_1, \fdoty_2,\ldots,\fdoty_{\xN}|\fx(t; a)}
   \\&=  \prod_{n=1}^\xN \frac{1}{\sqrt{2\pi\sigma^2}}
          \exp{\frac{(\fdoty_n- a\fdotlam_n)^2}{-2\sigma^2}}
   \\&=  \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
\\
\\
   \pderiv{}{a}\ln\pdfpb{\rvy(t)|\fx(t; a)}
     &=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
       +  \pderiv{}{a}\ln
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}
          \brs{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \frac{1}{-2\sigma^2} \sum_{n=1}^\xN 2(\fdoty_n- a\fdotlam_n)(- \fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdoty_n- a\fdotlam_n)
\\
\\
   \pderiv{^2}{a^2}\ln\pdfpb{\rvy(t)|\fx(t; a)}
     &=  \pderiv{}{a}\pderiv{}{a}\ln\pdfpb{\rvy(t)|\fx(t; a)}
   \\&=  \pderiv{}{a}
          \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdoty_n- a\fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(-\fdotlam_n)
   \\&=  \frac{-1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n^2
   \\&=  \frac{-\norm{\lambda(t)}^2}{\sigma^2}
\\
\\
   \var\estML[a]
     &\eqd \pE\brs{\estML[a]-\pE\estML[a]}^2
   \\&=    \pE\brs{\estML[a]- a}^2
   \\&\ge  \frac{-1}{\pE\brp{\pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t)|\fx(t; a)}}}
   \\&=    \frac{-1}{\pE\brp{\frac{-\norm{\lambda(t)}^2}{\sigma^2}}}
   \\&=    \frac{\sigma^2}{\norm{\lambda(t)}^2}
     \qquad\text{(Cram/'er-Rao lower bound of the variance)}
\end{align*}

\item Proof that $\estML[a]$ is an \prope{efficient} estimate:

An estimate is \prope{efficient} if
$\var\estML[a]=\mbox{CR lower bound}$.
We have already proven this, so $\estML[a]$ is an \prope{efficient} estimate.

Also, even without explicitly computing the variance of $\estML[a]$,
the variance equals the \vale{Cram/'er-Rao lower bound}
(and hence $\estML[a]$ is an \prope{efficient} estimate)
if and only if
\\\indentx$\ds\estML[a] -  a =
   \brp{\frac{-1}{\pE\brs{
              \pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t)|\fx(t; a)}
           }}}
   \brp{\pderiv{}{ a} \ln \pdfpb{\rvy(t)|\fx(t; a)}}
  $
\begin{align*}
   \brp{\frac{-1}{\pE\brp{\pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t)|\fx(t; a)}}}}
   \brp{\pderiv{}{ a} \ln \pdfpb{\rvy(t)|\fx(t; a)}}
     &= \left(\frac{\sigma^2}{\norm{\lambda(t)}^2}\right)
         \left(
           \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam( \fdoty - a \fdotlam)
         \right)
   \\&= \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam \fdoty -
         \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam^2
   \\&= \estML[a] - a
\end{align*}
\end{enumerate}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{ML phase estimation}]
\label{thm:estML_phase}
\footnote{
  \citerppg{srv}{159}{160}{013125295X}
  }
\index{maximum likelihood estimation!phase}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
$\begin{array}[t]{rcl}
   \rvy(t)     &=& [\opCawgn\fx](t) = \fx(t;\phi) + \fn(t) \\
   \rvx(t;\phi) &=& A\cos(2\pi f_ct +  \phi).
\end{array}$
\\
Then the optimal ML-estimate of parameter $ \phi $ is
\thmbox{
   \estML[\phi]
      =   -\atan\left(
           \frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           \right)
   }
\end{theorem}
\begin{proof}
\begin{align*}
   \estML[\phi]
     &= \argmax_\phi
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\estT)}-\norm{\rvx(t;\phi)}^2 \right]
     && \text{by \prefpp{thm:estML_general}}
   \\&= \argmax_\phi
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\phi)} \right]
     && \text{because $\norm{\rvx(t;\phi)}$ does not depend on $\phi$}
   \\&= \arg_\phi
         \left[ \pderiv{}{\phi} \inprod{\rvy(t)}{\rvx(t;\phi)} = 0 \right]
   \\&= \arg_\phi
         \left[ \inprod{\rvy(t)}{\pderiv{}{\phi} \rvx(t;\phi)} = 0 \right]
     && \text{because $\inprod{\cdot}{\cdot}$ is a linear operator}
   \\&= \arg_\phi
         \left[ \inprod{\rvy(t)}{\pderiv{}{\phi} A\cos(2\pi f_ct+\phi)} = 0 \right]
   \\&= \arg_\phi
         \left[ \inprod{\rvy(t)}{-A\sin(2\pi f_ct+\phi)} = 0 \right]
   \\&= \arg_\phi
         \left[ -A\inprod{\rvy(t)}{\cos(2\pi f_ct)\sin\phi+\sin(2\pi f_ct)\cos\phi} = 0 \right]
   \\&= \arg_\phi \left[
           \sin\phi\inprod{\rvy(t)}{\cos(2\pi f_ct)} =
          -\cos\phi\inprod{\rvy(t)}{\sin(2\pi f_ct)}
           \right]
   \\&= \arg_\phi \left[
           \frac{\sin\phi}{\cos\phi} =
          -\frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           \right]
   \\&= \arg_\phi \left[
           \tan\phi =
          -\frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           \right]
   \\&=  -\atan\left(
           \frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           \right)
\end{align*}
\end{proof}


%---------------------------------------
\begin{theorem}[\thmd{ML estimation of a function of a parameter}]
\footnote{
  \citerppg{srv}{142}{143}{013125295X}
  }
\label{thm:estML-CR}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
$\begin{array}[t]{rcl}
   \rvy(t)     &=& [\opCawgn\fx](t) = \rvx(t;\theta) + \fv(t) \\
   \rvx(t;\theta)   &=& \fg(\theta)
\end{array}$\\
and $\fg$ is \prope{one-to-one and onto} (\prope{invertible}).
\\
\thmbox{\begin{array}{M>{\ds}rc>{\ds}l}
  Then the optimal ML-estimate of parameter $\theta$ is
   & \estML &=& \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right).
  \\
  If an \fncte{ML estimate} $\estML$ is unbiased ($\pE \estML = \theta$) then
    & \var\estML &\ge&
      \frac{\sigma^2}{\xN}
      \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}.
  \\
  If $\fg(\theta) = \theta$ then $\estML$ is an \propb{efficient} estimate such that
   & \var\estML &=& \frac{\sigma^2}{\xN}.
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML
     &= \argmin_{\theta}
         \brs{\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }
     && \text{by \prefp{thm:estML_general}}
   \\&= \arg_{\theta}\brs{
            \pderiv{}{\theta}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 = 0
         }
     && \text{because form is \prope{quadratic}}
   \\&= \arg_{\theta}\left[
             2\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]\pderiv{}{\theta}\fg(\theta) = 0
         \right]
   \\&= \arg_{\theta}\left[
             2\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)] = 0
         \right]
   \\&= \arg_{\theta}\left[
             \sum_{n=1}^\xN \fdoty_n = \xN \fg(\theta)
         \right]
   \\&= \arg_{\theta}\left[
             \fg(\theta) = \frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n
         \right]
   \\&= \arg_{\theta}\brs{
              \theta  = \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right)
         }
   \\&= \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right)
\end{align*}


If $\estML$ is unbiased ($\pE\estML=\theta$), we can use
the \vale{Cram/'er-Rao bound} to find a lower bound on the variance:

\begin{align*}
   \var\estML
     &\eqd \pE\brs{\estML-\pE\estML}^2
   \\&= \pE\brs{\estML-\theta}^2
   \\&\ge \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t)|\fx(t;\theta)}
           }}
     && \text{by \ineqe{Cram/'er-Rao Inequality}}
   \\&=   \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln
              \pdfpb{\fdoty_1, \fdoty_2,\ldots,\fdoty_{\xN}|\fx(t;\theta)}
           }}
     && \text{by \thme{Sufficient Statistic Theorem}}
     && \text{\xref{thm:sstat}}
   \\&=   \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
              \exp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }\right]
           }}
     && \begin{array}{@{}M}
         by \prope{AWGN} hypothesis 
         and \prefp{thm:awgn_stats}
        \end{array}
   \\&=   \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN \right]
              +
              \pderiv{^2}{\theta^2} \ln\left[
              \exp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }\right]
           }}
  \\&=   \frac{-1}{\ds\pE\brp{
             \pderiv{^2}{\theta^2}
             \left(\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 \right)
          }}
  \\&=   \frac{2\sigma^2}{\ds\pE\brp{
             \pderiv{}{\theta} \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2
          }}
  \\&=   \frac{2\sigma^2}{\ds\pE\brp{
             -2\pderiv{}{\theta}
             \pderiv{\fg(\theta)}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
          }}
    && \text{by \thme{Chain Rule}}
  \\&=   \frac{-\sigma^2}{\ds\pE\brp{
             \pderiv{\fg^2(\theta)}{\theta^2}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
             +
             \pderiv{\fg(\theta)}{\theta}
             \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
          }}
     && \text{by \thme{Product Rule}}
   \\&=   \frac{-\sigma^2}{\ds\pE\brp{
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
              -N
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }}
   \\&=   \frac{-\sigma^2}{\ds
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN \pE[\fdoty_n-\fg(\theta)]
              -\xN
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
   \\&=   \frac{-\sigma^2}{
              -\xN
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
     && \text{because derivative of constant = 0}
   \\&=   \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}
\end{align*}

The inequality becomes equality (an \prope{efficient} estimate)
if and only if
\[ \estML - \theta =
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t)|\fx(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\rvy(t)|\fx(t;\theta)} \right).
\]

\begin{align*}
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t)|\fx(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\rvy(t)|\fx(t;\theta)} \right)
     &= \left(
         \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2} \right)
         \left(\frac{-1}{2\sigma^2}(2)\pderiv{\fg(\theta)}{\theta}
         \sum_{n=1}^\xN [\fdoty_n - \fg(\theta) ]\right)
   \\&= -\frac{1}{\xN}
         \frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\sum_{n=1}^\xN [\fdoty_n - \fg(\theta) ] \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n - \fg(\theta) \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\estML - \fg(\theta) \right)
   \\&= -(\estML - \theta)
\end{align*}
\end{proof}

%======================================
\section{Colored noise}
\index{noise!colored}
%======================================
This chapter presented several theorems whose results depended on the
noise being white.
However if the noise is {\bf colored}, then these results are
invalid.
But there is still hope for colored noise.
Processing colored signals can be accomplished using two techniques:
\begin{enume}
   \item Karhunen-Lo\`{e}ve basis functions
      \footnote{{\bf Karhunen-Lo\`{e}ve}: \prefpp{sec:KL}}
   \item whitening filter
\footnote{
   \begin{tabular}[t]{ll}
      \ope{Continuous data whitening}: & \prefp{sec:whiten}  \\
      \ope{Discrete data whitening}:   & \prefp{sec:d-whiten}
   \end{tabular}
   }
\end{enume}

\paragraph{Karhunen-Lo\`{e}ve.}
If the noise is white, the set $\{\inprod{\rvy(t)}{\fpsi_n(t)}\}$
is a \prope{sufficient statistic} regardless of which
set $\{\fpsi_n(t)\}$ of orthonormal basis functions are used.
If the noise is colored, and if $\{\fpsi_n(t)\}$ satisfy the
Karhunen-Lo\`{e}ve criterion
   \[ \int_{t_2}\Rxx(t_1,t_2)\fpsi_n(t_2)\dd{t_2} = \lambda_n \fpsi_n(t_1) \]
then $\{\inprod{\rvy(t)}{\fpsi_n(t)}\}$ is still a sufficient statistic.

\paragraph{Whitening filter.}
The whitening filter makes the received signal $\rvy(t)$ statistically white
(uncorrelated in time). In this case,
any orthonormal basis set can be used to generate sufficient statistics.




%======================================
\section{Signal matching}
\index{matched filter}
%======================================
\paragraph{Detection methods.}
There are basically two types of detection methods:
\begin{enume}
   \item signal matching
   \item orthonormal decomposition.
\end{enume}

Let $\setS$ be the set of transmitted waveforms and
$\setY$ be a set of orthonormal basis functions that span $\setS$.
\hie{Signal matching} computes the innerproducts of a
received signal $\rvy(t)$ with each signal from $\setS$.
\hie{Orthonormal decomposition} computes the innerproducts of
$\rvy(t)$ with each signal from the set $\setY$.

In the case where $\seto{\setS}$ is large, often $\seto{\setY}<<\seto{\setS}$
making orthonormal decomposition much easier to implement.
For example, in a QAM-64 modulation system,
signal matching requires $\seto{\setS}=64$ innerproduct calculations,
while orthonormal decomposition only requires $\seto{\setY}=2$
innerproduct calculations because all 64 signals in $\setS$ can be spanned
by just 2 orthonormal basis functions.

\paragraph{Maximizing SNR.}
\prefpp{thm:sstat} shows that the innerproducts of $\rvy(t)$ with
basis functions of $\setY$ is \prope{sufficient} for optimal detection.
\prefpp{thm:mf_maxSNR} (next) shows that a receiver can
maximize the SNR of a received signal when signal matching is used.

%--------------------------------------
\begin{theorem}
\label{thm:mf_maxSNR}
%--------------------------------------
Let $\rvx(t)$ be a transmitted signal, $\fn(t)$ noise, and $\rvy(t)$ the received signal
in an AWGN channel.
Let the \hie{signal to noise ratio} SNR be defined as
\\\indentx$\ds
      \snr[\rvy(t)] \eqd \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                            {\pE\brs{\abs{\inprod{\fn(t)}{\rvx(t)}}^2}}.
          $
\thmboxt{
  $\ds\snr[\rvy(t)] \le \frac{2\norm{\rvx(t)}^2}{N_o }$
  \qquad
  and is maximized (equality) when $\rvx(t)=a\rvx(t)$, where $a\in\R$.
  }
\end{theorem}

\begin{proof}
\begin{align*}
   \snr[\rvy(t)]
     &\eqd \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\pE\brs{\abs{\inprod{\fn(t)}{\rvx(t)}}^2}}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{f(t)}}^2}
                {\pE\brs{\left[\int_{t\in\R} \fn(t)\rvx^\ast(t)\;dt\right]
                      \left[\int_{\estT} n(\estT)f^\ast(\estT)\;du\right]^\ast}
                }
   \\&=    \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\pE\brs{\int_{t\in\R} \int_{\estT} \fn(t)n^\ast(\estT)\rvx^\ast(t)\rvx(\estT)\;dtdu}}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{f(t)}}^2}
                {\int_{t\in\R} \int_{\estT} \pE\brs{\fn(t)n^\ast(\estT)}\rvx^\ast(t)\rvx(\estT)\;dtdu}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\int_{t\in\R} \int_{\estT} \frac{1}{2}N_o\delta(t-\estT) \rvx^\ast(t)\rvx(\estT)\;dtdu}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\frac{1}{2}N_o \int_{t\in\R} \rvx^\ast(t)\rvx(t)\dt}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\frac{1}{2}N_o \norm{\rvx(t)}^2}
   \\&\le  \frac{\abs{\norm{\rvx(t)}\;\norm{\rvx(t)}}^2}
                {\frac{1}{2}N_o \norm{\rvx(t)}^2}
     &&    \text{by \thme{Cauchy-Schwarz Inequality}}
     &&    \text{\ifsxref{vsinprod}{thm:cs}}
   \\&=    \frac{2\norm{\rvx(t)}^2}
                {N_o }
\end{align*}
The Cauchy-Schwarz Inequality becomes an equality
($\snr$ is maximized) when $\rvx(t)=a\rvx(t)$.
\end{proof}

\paragraph{Implementation.}
The innerproduct operations can be implemented using either
  \begin{dingautolist}{"C0}
     \item a correlator or
     \item a matched filter.
  \end{dingautolist}

A correlator is simply an integrator of the form
   $\ds\inprod{\rvy(t)}{f(t)} = \int_0^T \rvy(t)f(t)\dt.$

A matched filter introduces a function $\fh(t)$ such that
$\fh(t) =\rvx(T-t)$ (which implies $\rvx(t)=h(T-t)$) giving
  \[
    \mcom{\inprod{\rvy(t)}{\rvx(t)} = \int_0^T \rvy(t)\rvx(t)\dt }
         {correlator}
    =
    \mcom{\brlr{\int_0^\infty \rvx(\tau)h(t-\tau)\dtau}_{t=T}
            = \brlr{\rvx(t)\conv \fh(t)}_{t=T}
         }{matched filter}.
  \]

This shows that $\fh(t)$ is the impulse response of a filter operation
sampled at time $T$. % (see \prefpp{fig:mf}).
By \prefpp{thm:mf_maxSNR}, the optimal impulse response is
$\fh(T-t)=f(t)=\rvx(t)$.
That is, the optimal $\fh(t)$ is just a ``flipped" and shifted version of $\rvx(t)$.




