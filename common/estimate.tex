%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================

%======================================
\chapter{Estimation Theory }
\label{app:est}
\index{estimation theory}
%======================================
%--------------------------------------
\paragraph{Estimation types.}
%--------------------------------------
Let $\fs(t;\theta)$ be a waveform with parameter $\theta$.
There are three basic types of estimation on $s$:

\begin{enume}
   \item \ope{detection}:
      \begin{liste}
         \item The waveform $\fs(t;\theta_n)$ is known except for the value of parameter $\theta_n$.
         \item The parameter $\theta_n$ is one of a finite set of values.
         \item Estimate $\theta_n$ and thereby also estimate $\fs(t;\theta)$.
      \end{liste}
   \item \prope{parametric} estimation:
      \begin{liste}
         \item The waveform $\fs(t;\theta)$ is known except for the value of parameter $\theta$.
         \item The parameter $\theta$ is one of an infinite set of values.
         \item Estimate $\theta$ and thereby also estimate $\fs(t;\theta)$.
      \end{liste}
   \item \prope{nonparametric} estimation:
      \begin{liste}
         \item The waveform $\fs(t)$ is unknown.
         \item Estimate $\fs(t)$.
      \end{liste}
\end{enume}

%--------------------------------------
\paragraph{Estimation criterion.}
%--------------------------------------
Optimization requires a criterion against which the quality of an
estimate is measured.\footnote{\citerc{srv}{chapters 3, 5}.}
The most demanding and general criterion is the \prope{Bayesian} criterion.
The Bayesian criterion requires knowledge of the probability
distribution functions and the definition of a \fncte{cost function}.
Other criterion are special cases of the Bayesian criterion
such that the cost function is defined in a special way,
no cost function is defined, and/or the distribution is not known 
\xref{fig:est-tech}.

%--------------------------------------
\paragraph{Estimation techniques.}
\label{ref:sec:parameter-est}
%--------------------------------------
Estimation techniques can be classified into 
five groups \xref{fig:est-tech}:\footnote{%
  \citerpgc{nelles2001}{26}{3540673695}{``Fig 2.2 Overview of linear and nonlinear optimization techniques"},
  \citerpgc{nelles2001}{33}{3540673695}{``Fig 2.5 The Bayes method is the most general approach but\ldots"},
  \citerpgc{nelles2001}{63}{3540673695}{``Table 3.3 Relationship between linear recursive and nonlinear optimization techniques"},
  \citerpg{nelles2001}{66}{3540673695}
  }
\begin{enume}
   \item sequential decoding
   \item norm minimization
   \item gradient search
   \item inner product analysis
   \item direct search
\end{enume}

Sequential decoding is a non-linear estimation family.
Perhaps the most famous of these is the Veterbi algorithm which
uses a trellis to calculate the estimate.
The Verterbi algorithm has been shown to yield an optimal estimate
in the maximal likelihood (ML) sense.
Norm minimization and gradient search algorithms are all linear algorithms.
While this restriction to linear operations often simplifies calculations,
it often yields an estimate that is not optimal in the ML sense.

%--------------------------------------
\section{Estimation criterion}
\label{sec:est_criterion}
%--------------------------------------
\begin{figure}[ht]
\begin{center}
\begin{fsK}
\setlength{\unitlength}{0.2mm}
\begin{picture}(650,350)(-400,-250)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(- 50,  50){\framebox(100, 50)[c]{Bayesian}          }
  \put(  60,  50){\makebox (200, 50)[l]{pdf known and cost function defined}}
  \put(   0,   0){\line    (  0,  1)   { 50}               }
  \put(-200,   0){\line    (  1,  0)   {400}               }
  \put(-200,- 50){\line    (  0,  1)   { 50}               }
  \put( 200,- 50){\line    (  0,  1)   { 50}               }

  \put(-190,- 50){\makebox (100, 50)[l]{pdf known}         }
  \put( 210,- 50){\makebox (100, 50)[l]{pdf not known}     }
  \put(-350,- 50){\line    (  1,  0)   {300}               }
  \put(-350,-100){\line    (  0,  1)   { 50}               }
  \put(-200,-100){\line    (  0,  1)   { 50}               }
  \put(- 50,-100){\line    (  0,  1)   { 50}               }
  \put( 200,-100){\line    (  0,  1)   {100}               }

  \put(-340,- 90){\makebox (100, 30)[t]{uniform cost}      }
  \put(-190,- 90){\makebox (100, 30)[t]{absolute cost}     }
  \put(- 40,- 90){\makebox (100, 30)[t]{squared cost}      }
  \put(-340,- 90){\makebox (100, 30)[b]{(mode of pdf)}     }
  \put(-190,- 90){\makebox (100, 30)[b]{(median of pdf)}   }
  \put(- 40,- 90){\makebox (100, 30)[b]{(mean of pdf)}     }

  \put(-400,-150){\framebox(100, 50)[c]{MAP}               }
  \put(-250,-150){\framebox(100, 50)[c]{---}               }
  \put(-100,-150){\framebox(100, 50)[c]{mean square}       }
  \put( 150,-150){\framebox(100, 50)[c]{mini-max}          }

  \put(-340,-200){\makebox (100, 50)[l]{no prior probability information}}
  \put(-350,-200){\line    (  0,  1)   { 50}               }
  \put(-400,-250){\framebox(100, 50)[c]{ML}                }

  \put( 210,-200){\makebox (100, 50)[l]{no cost function}  }
  \put( 200,-200){\line    (  0,  1)   { 50}               }
  \put( 150,-250){\framebox(100, 50)[c]{}                  }
  \put( 150,-240){\makebox (100, 30)[t]{Neyman-}    }
  \put( 150,-240){\makebox (100, 30)[b]{Pearson}    }

\end{picture}
\end{fsK}
\end{center}
\caption{
   Estimation criterion
   \label{fig:est-criterion}
   }
\end{figure}

%--------------------------------------
\begin{definition}
\index{MAP}
\index{ML}
\index{maximum a-posteriori}
\index{maximum likelihood}
\label{def:MAP}
\label{def:ML}
\label{def:estB}   
\label{def:estMS}  
\label{def:estMM}  
\label{def:estMAP} 
\label{def:estML}  
%--------------------------------------
Let\\
$\begin{array}{FlM}
    (A).& x(t;\theta)          & be a function with unknown parameter $\theta$
  \\(B).& y(t)                 & a known function which is statistically dependent on $x(t;\theta)$
  \\(C).& C(\theta,\pdfp(x,y)) & be a cost function.
\end{array}$
\tbox{
\begin{fsK}
\setlength{\unitlength}{0.15mm}
\begin{picture}(200,100)(-50,-50)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(- 50,  10){\makebox ( 50, 50)[b]{$x(t)$}            }
  \put(- 50,   0){\vector  (  1,  0)   {50}                }
  \put(   0, -50){\framebox(100,100)[c]{}                  }
  \put(   0, -20){\makebox (100, 40)[t]{stochastic}        }
  \put(   0, -20){\makebox (100, 40)[b]{operator}          }
  \put( 100,  10){\makebox ( 50, 50)[b]{$y(t)$}            }
  \put( 100,   0){\vector  (  1,  0)   {50}                }
\end{picture}
\end{fsK}
}
\\
Then the following \fnctd{estimate}s are defined as follows:
\defbox{\begin{array}{FMMlc>{\ds}l}
     (1).&\fnctd{Bayesian estimate}                         &                          & \estB   &\eqd& \arg\min_{\theta} C(\theta,\pdfp(x,y)) 
   \\(2).&\fnctd{Mean square estimate}                      &(``\fnctd{MS  estimate}") & \estMS  &\eqd& \arg\min_{\theta} \E\norm{C(\theta,\pdfp(x,y))}^2 
   \\(3).&\fnctd{mini-max estimate}                         &(``\fnctd{MM  estimate}") & \estMM  &\eqd& \arg\min_{\theta}\max_{\pdfp} C(\theta,\pdfp(x,y)) 
   \\(4).&\mc{2}{M}{\begin{tabular}[t]{@{}l}\fnctd{maximum a-posteriori probability estimate}\\
                                         (``\fnctd{MAP estimate}")
                    \end{tabular}}
         & \estMAP &\eqd& \arg\max_{\theta} \pP{x(t;\theta)|y(t)} 
   \\(5).&\fnctd{maximum likelihood estimate}               &(``\fnctd{ML  estimate}") & \estML  &\eqd& \arg\max_{\theta} \pP{y(t)|x(t;\theta)} 
\end{array}}
\end{definition}

%--------------------------------------
\begin{theorem}
\label{thm:map=ml}
%--------------------------------------
Let $\fs(t;\theta)$ be a function with unknown parameter $\theta$.
\thmbox{
  \brb{\text{$\pP{\theta}=$\prope{constant}}}
  \quad\implies\quad
  \brb{\estMAP = \estML}
  }
\end{theorem}
\begin{proof}
\begin{align*}
   \estMAP
     &\eqd \arg\max_{\theta} \pP{s(t;\theta)|r(t)}
     &&    \text{by definition of $\estMAP$}
     &&    \text{\xref{def:estMAP}}
   \\&=    \arg\max_{\theta} \frac{\pP{s(t;\theta) \land \fr(t)}}
                               {\pP{r(t)}}
   \\&=    \arg\max_{\theta} \frac{\pP{r(t) | \fs(t;\theta) }\pP{s(t;\theta) }}
                               {\pP{r(t)}}
   \\&=    \arg\max_{\theta} \pP{r(t) | \fs(t;\theta) }\pP{s(t;\theta) }
   \\&=    \arg\max_{\theta} \pP{r(t) | \fs(t;\theta) }
   \\&\eqd \estML
     &&  \text{by definition of $\estML$}
     &&  \text{\xref{def:estML}}
\end{align*}
\end{proof}




%======================================
\section{System model}
%======================================
\begin{figure}[ht] \color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.20mm}
\begin{picture}(700,150)(-100,-50)
  \thicklines
  %\graphpaper[10](0,0)(500,100)
  \put(-100 ,  60 ){\makebox( 100,0)[b]{$\su$} }
  \put(-100 ,  50 ){\vector(1,0){100} }

  \put(  00 ,  10 ){\makebox( 100, 80)[t]{transmit} }
  \put(  00 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put(  00 ,  10 ){\makebox( 100, 80)[b]{$\opT$} }
  \put(  00 ,  00 ){\framebox( 100,100){} }

  \put( 100 ,  60 ){\makebox( 100,0)[b]{$\fs(t;\vu)$} }
  \put( 100 ,  50 ){\vector(1,0){100} }

  \put( 200 ,  10 ){\makebox( 100, 80)[t]{channel} }
  \put( 200 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put( 200 ,  10 ){\makebox( 100, 80)[b]{$\opC$} }
  \put( 200 ,  00 ){\framebox(100,100){} }

  \put( 300 ,  60 ){\makebox( 100,0)[b]{$\fr(t;\vu)$} }
  \put( 300 ,  50 ){\vector(1,0){100} }

  \put( 400 ,  00 ){\framebox(100,100){} }
  \put( 400 ,  10 ){\makebox( 100, 80)[t]{receive} }
  \put( 400 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put( 400 ,  10 ){\makebox( 100, 80)[b]{$\opR$} }

  \put( 500 ,  60 ){\makebox( 100,0)[b]{$\sue$} }
  \put( 500 ,  50 ){\vector(1,0){100} }

  \put(- 90 , -10 ){\makebox( 0, 0)[tl]{$\vu\eqd\su$} }
  \put( 110 , -10 ){\makebox( 0, 0)[tl]{$\fs(t;\vu)=\opT\vu$} }
  \put( 310 , -10 ){\makebox( 0, 0)[tl]{$\fr(t;\vu)=\opC\opT\vu$} }
  \put( 510 , -10 ){\makebox( 0, 0)[tl]{$\sue=\opSys\vu=\opR\opC\opT\vu$} }

\end{picture}
\end{fsL}
\end{center}
\caption{
   Communication system model
   \label{fig:sys_model}
   }
\end{figure}

A communication system is an operator.
%\footnote{
%   \begin{tabular}[t]{lll}
%      \hie{operator}:                & \pref{def:operator} & page~\pageref{def:operator}\\
%      \hie{operator multiplication}: & \pref{def:op+x}     & page~\pageref{def:op+x}
%   \end{tabular}
%   }
$\opSys$ over an information sequence $\su$ that generates an
estimated information sequence $\sue$.
The system operator factors into a
receive operator $\opR$, a channel operator $\opC$, and
a transmit operator $\opT$ such that
   \[ \opSys = \opR\opC\opT. \]
The transmit operator
operates on an information sequence $\su$ to generate
a channel signal $\fs(t;\vu)$.
The channel operator operates on the transmitted signal $\fs(t;\vu)$
to generate the received signal $\fr(t;\vu)$.
The receive operator operates on the received signal $\fr(t;\vu)$
to generate the estimate $\sue$
(see \prefpp{fig:sys_model}).


%---------------------------------------
\begin{definition}
\label{def:comsys}
\index{communication system}
%---------------------------------------
Let $U$ be the set of all sequences $\su$ and let
\defbox{
\begin{array}{lllll}
   \imark & \opSys &:U    &\to U    & \text{ (system   operator)  } \\
   \imark & \opT   &:U    &\to \vRc & \text{ (transmit operator)} \\
   \imark & \opC   &:\vRc &\to \vRc & \text{ (channel  operator) } \\
   \imark & \opR   &:\vRc &\to U    & \text{ (receive  operator) }
\end{array}
}\\
be operators.
A {\bf digital communication system} is the operation $\opSys$
on the set of information sequences $U$ such that
$\opSys \eqd \opR\opC\opT$.
\end{definition}

Communication systems can be continuous or discrete valued in
time and/or amplitude:

\begin{center}
   \begin{tabular}{|c||c|c|}
      \hline
         $\fs(t)=a(t)\psi(t)$   &  continuous time $t$      & discrete time $t$   \\
      \hline
      \hline
         continuous amplitude $a(t)$ & analog communications   & discrete-time communications  \\
      \hline
         discrete amplitude $a(t)$   &---                    & digital communications \\
      \hline
   \end{tabular}
\end{center}

{\bf In this document, we normally take the approach that}
\begin{enume}
   \item $\opC$ is stochastic
         %\footnote{
         %An exception to this is
         %in Chapter~\ref{chp:isi} (ISI) page~\pageref{chp:isi},
         %where $\opC$ is deterministic
         %($\opC$ is simply a bandlimiting operation).
         %However in any real-world system, $\opC$ is always stochastic.
         %}
   \item There is no structural constraint on $\opR$.
   \item $\opR$ is optimum with respect to the ML-criterion.
\end{enume}
These characteristics are explained more fully below.

%======================================
\subsection{Channel operator}
%======================================
Real-world physical channels perform a number of operations on
a signal.
Often these operations are closely modeled by a channel
operator $\opC$.
Properties that characterize a particular
channel operator associated with some physical channel include
\begin{liste}
   \item linear or non-linear
   \item time-invariant or time-variant
   \item memoryless or non-memoryless
   \item deterministic or stochastic.
\end{liste}
Examples of physical channels include
free space, air, water, soil, copper wire, and fiber optic cable.
Information is carried through a channel using some physical process.
These processes include:

\begin{tabular}{cl|ll}
             & Process          & Example             \\
   \hline
   $\imark$ & electromagnetic waves & free space, air    &\\% (see Appendix~\ref{app:em}, page~\pageref{app:em})  \\
   $\imark$ & acoustic waves        & water, soil         \\
   $\imark$ & electric field potential (voltage)  & wire                \\
   $\imark$ & light            & fiber optic cable   \\
   $\imark$ & quantum mechanics         &
\end{tabular}


%======================================
\subsection{Receive operator}
%======================================
Let $\opI$ be the \hie{identity operator}.\footnote{
  $\opI$ is the \hie{identity operator} if for any operator $\opX$,
  $\opX\opI = \opI\opX = \opX.$
  }
Ideally, $\opR$ is selected such that
   $\opR\opC\opT = \opI$.
In this case we say that $\opR$ is the \hie{left inverse}\footnote{
  \begin{tabular}[t]{llll}
    $\opXi{X}$ is the & \hie{left  inverse}& of $\opX$ if & $\opXi{X}\opX=\opI$. \\
    $\opXi{X}$ is the & \hie{right inverse}& of $\opX$ if & $\opX\opXi{X}=\opI$. \\
    $\opXi{X}$ is the & \hie{      inverse}& of $\opX$ if & $\opXi{X}\opX=\opX\opXi{X}=\opI$.
  \end{tabular}
   }
of $\opC\opT$ and denote this left inverse by $\opCTi$.
One example of a system where this inverse exists is the
noiseless ISI system.  %(Chapter~\ref{chp:isi} page~\pageref{chp:isi}).
While this is quite useful for mathematical analysis and system design,
$\opCTi$ does not actually exist for any real-world system.

When $\opCTi$ does not exist, the ``ideal" $\opR$ is one that is
optimum
  \begin{enume}
    \item with respect to some \hie{criterion} (or cost function)
    \item and sometimes under some structural \hie{constraint}.
  \end{enume}
When a structural constraint is imposed on $\opR$,
the solution is called \hie{structured}; otherwise,
it is called \hie{non-structured}.\footnote{\citerpg{vantrees1}{12}{0471095176}}
A common example of a structured approach is the use of a
transversal filter (FIR filter in DSP) in which optimal coefficients
are found for the filter.
A structured $\opR$ is only optimal with respect to the
imposed constraint.
Even though $\opR$ may be optimal with respect to this structure,
$\opR$ may not be optimal in general;
that is, there may be another structure that would lead to a ``better"
solution.
In a non-structured approach, $\opR$ is free to take any form
whatsoever (practical or impractical) and therefore leads to the
best of the best solutions.

The nature of $\opR$ depends heavily on the nature of $\opC$.
If $\opCTi$ does not exist,
then the ideal $\opR$ is one that is optimal with respect to some criterion.
If $\opC$ is deterministic,
then appropriate optimization criterion may include
\begin{liste}
   \item least square error (LSE) criterion
   \item minimum absolute error criterion
   \item minimum peak distortion criterion.
\end{liste}
If $\opC$ is stochastic
%\footnote{
%  An example of a stochastic $\opC$ includes the additive noise channel
%  operator $\opCan$ and the AWGN channel operator $\opCawgn$.
%  (Chapter~\ref{chp:awgn} page~\pageref{chp:awgn})
%  }
then appropriate optimization criterion may include\footnote{
  \hie{Estimation}: Appendix~\ref{app:est} page~\pageref{app:est}
  }

\begin{tabular}{llll}
   $\imark$ & Bayes:                                  & pdf known and cost function defined \\
   $\imark$ & Maximum aposteriori probability (MAP):  & pdf known and uniform cost function \\
   $\imark$ & Maximum likelihood (ML):                & pdf known and no prior probability information\\
   $\imark$ & mini-max:                               & pdf not known but a cost function is defined \\
   $\imark$ & Neyman-Pearson:                         & pdf not known and no cost function defined.
\end{tabular}

Making $\opR$ optimum with respect to one of these criterion leads to
an \hie{estimate} $\sue = \opR\opC\opT\vu$ that is also optimum
with respect to the same criterion.\footnote{
   \hie{Optimal estimates}:
   \prefpp{def:ML} page~\pageref{def:ML}
   }
\\{\defbox{\begin{array}{MM>{\ds}lc>{\ds}l}
   \ope{Bayesian}:                        &             & \estB   &\eqd& \arg\min_{\theta} C(\theta,\pdfp(s,r)) \\
   \ope{Mean square}                      &(\ope{MS}):  & \estMS  &\eqd& \arg\min_{\theta} \pE\norm{C(\theta,\pdfp(s,r))}^2 \\
   \ope{mini-max}                         &(\ope{MM}):  & \estMM  &\eqd& \arg\min_{\theta}\max_{\pdfp} C(\theta,\pdfp(s,r)) \\
   \ope{maximum a-posteriori probability} &(\ope{MAP}): & \estMAP &\eqd& \arg\max_{\theta} \pP{s(t;\theta)|\fr(t)} \\
   \ope{maximum likelihood}               &(\ope{ML}):  & \estML  &\eqd& \arg\max_{\theta} \pP{\fr(t)|s(t;\theta)}
\end{array}}

%======================================
\section{Optimization in the case of additional operations}
%======================================
\begin{figure}[ht] \color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.15mm}
\begin{picture}(900,150)(-100,-50)
  \thicklines
  %\graphpaper[10](0,0)(500,100)
  \put(-100 ,  60 ){\makebox( 100,0)[b]{$\su$} }
  \put(-100 ,  50 ){\vector(1,0){100} }

  \put(  00 ,  10 ){\makebox( 100, 80)[t]{transmit} }
  \put(  00 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put(  00 ,  10 ){\makebox( 100, 80)[b]{$\opT$} }
  \put(  00 ,  00 ){\framebox( 100,100){} }

  \put( 100 ,  60 ){\makebox( 100,0)[b]{$\fs(t;\vu)$} }
  \put( 100 ,  50 ){\vector(1,0){100} }

  \put( 200 ,  10 ){\makebox( 100, 80)[t]{channel} }
  \put( 200 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put( 200 ,  10 ){\makebox( 100, 80)[b]{$\opC$} }
  \put( 200 ,  00 ){\framebox(100,100){} }

  \put( 300 ,  60 ){\makebox( 100,0)[b]{$\fr(t;\vu)$} }
  \put( 300 ,  50 ){\vector(1,0){100} }

  \put( 400 ,  00 ){\framebox(100,100){} }
  \put( 400 ,  10 ){\makebox( 100, 80)[t]{} }
  \put( 400 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put( 400 ,  10 ){\makebox( 100, 80)[b]{$\opX$} }

 %\put( 500 ,  60 ){\makebox( 100,0)[b]{$\sue$} }
  \put( 500 ,  50 ){\vector(1,0){100} }

  \put( 600 ,  00 ){\framebox(100,100){} }
  \put( 600 ,  10 ){\makebox( 100, 80)[t]{receive} }
  \put( 600 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put( 600 ,  10 ){\makebox( 100, 80)[b]{$\opR$} }

  \put( 700 ,  60 ){\makebox( 100,0)[b]{$\sue$} }
  \put( 700 ,  50 ){\vector(1,0){100} }

  \put(- 90 , -10 ){\makebox( 0, 0)[tl]{$\vu\eqd\su$} }
  \put( 110 , -10 ){\makebox( 0, 0)[tl]{$\fs(t;\vu)=\opT\vu$} }
  \put( 310 , -10 ){\makebox( 0, 0)[tl]{$\fr(t;\vu)=\opC\opT\vu$} }
  \put( 510 , -10 ){\makebox( 0, 0)[tl]{$\opX\opC\opT\vu$} }
  \put( 710 , -10 ){\makebox( 0, 0)[tl]{$\sue=\opR\opX\opC\opT\vu$} }

\end{picture}
\end{fsL}
\end{center}
\caption{
   Theorem of reversibility
   \label{fig:thm_rev}
   }
\end{figure}

Often in communication systems, an additional  operator $\opX$ is
inserted such that (see \prefpp{fig:thm_rev})
   \[ \opSys = \opR\opX\opC\opT.\]
An example of such an operator $\opX$ is a receive filter.
Is it still possible to find an $\opR$ that will perform as well as
the case where $\opX$ is not inserted?
In general, the answer is ``no".
For example, if $\opX r=0$, then all received information is lost
and obviously there is no $\opR$ that can recover from this event.
However, in the case where the right inverse $\opXi{X}$ of $\opX$ exists,
then the answer to the question is ``yes" and an optimum  $\opR$
still exists.
That is, it doesn't matter if an $\opX$ is inserted into system
as long as $\opX$ is invertible.
This is stated formally in the next theorem.
%--------------------------------------
\begin{theorem}[\thmd{Theorem of Reversibility}]
\footnote{
  \citerppg{vantrees1}{289}{290}{0471095176}
  }
\label{thm:reversibility}
%--------------------------------------
Let
\begin{liste}
   \item $\est = \opR\opC\opT\vu$ be the optimum estimate of $\vu$
   \item $\opX$ be an operator with right inverse $\opXi{X}$.
\end{liste}
Then there exists some $\opR'$ such that
   \thmbox{ \est = \opR'\opX\opC\opT\vu. }
\end{theorem}
\begin{proof}
  Let $\opR'=\opR\opXi{X}$.
  Then
  \[ \opR'\opX\opC\opT\vu = \opR\opXi{X}\opC\opT\vu = \opR\opC\opT\vu = \est. \]
\end{proof}





%======================================
\section{Channel Statistics}
%======================================
The receiver needs to make a decision as to what
sequence $\seqn{u}$ the transmitter has sent.
This decision should be optimal in some sense.
Very often the optimization criterion is chosen to be
the \hie{maximal likelihood (ML)} criterion.
The information that the receiver can use to make an optimal
decision is the received signal $\fr(t)$.

If the symbols in $\fr(t)$ are statistically \hie{independent},
then the optimal estimate of the current symbol depends only on the
current symbol period of $\fr(t)$.
Using other symbol periods of $\fr(t)$ has absolutely no
additional benefit.
Note that the AWGN channel is \hie{memoryless};
that is, the way the channel treats the current symbol has
nothing to do with the way it has treated any other symbol.
Therefore, if the symbols sent by the transmitter into the channel
are independent, the symbols coming out of the channel are also
independent.

However, also note that the symbols sent by the transmitter
are often very intentionally not independent;
but rather a strong relationship between symbols is intentionally
introduced. This relationship is called \hie{channel coding}.
With proper channel coding, it is theoretically possible
to reduce the probability of communication error to any
arbitrarily small value as long as the channel is operating below its
\hie{channel capacity}.

This chapter assumes that the received symbols are
statistically independent;
and therefore optimal decisions at the receiver
for the current symbol are made
only from the current symbol period of $\fr(t)$.

The received signal $\fr(t)$ over a single symbol period
contains an uncountably infinite number of points.
That is a lot.
It would be nice if the receiver did not have to look
at all those uncountably infinite number of points
when making an optimal decision.
And in fact the receiver does indeed not have to.
As it turns out, a single finite set of \hie{statistics}
$\setn{\fdotr_1,\;\fdotr_2,\ldots,\fdotr_N}$
is sufficient for the receiver to make an optimal decision as to
which value the transmitter sent.

%---------------------------------------
\begin{definition}
\label{def:chan_stats}
\index{channel statistics}
%---------------------------------------
Let $\opC$ be an additive noise channel
%such that $\fr(t)=[\opC\fs](t)=\fs(t)+\fn(t)$
%and $\set{\fpsi_n}{n=1,2,\ldots,N}$ be a basis for $\fs(t)$.
%\defbox{\begin{array}{lll}
%  \fdotr_n    &\eqd& \inprod{\fr(t)}  {\psi_n(t)}  \\
  %\fdots_n(u) &\eqd& \inprod{\fs(t;u)}{\psi_n(t)} \\
%  \fdotn_n    &\eqd& \inprod{\fn(t)}  {\psi_n(t)}
%\end{array}}
\end{definition}



\prefpp{thm:sstat} (next) shows that the finite set
$\set{\fdotr_n}{n=1,2,\ldots,N}$ provides just as
much information as having the entire $\fr(t)$ waveform
(an uncountably infinite number of values)
with respect to the following cases:
\begin{enume}
   \item the conditional probability of $\fs(t;u)$ given $\fr(t)$
   \item the MAP estimate of the information sequence
   \item the ML estimate of the information sequence.
\end{enume}
That is, even with a drastic reduction in the amount of information
from uncountably infinite to finite $\xN$,
no information is lost with respect to the quantities listed above.

This amazing result is very useful in practical system implementation
and also for proving other theoretical results
(notably estimation and detection theorems which come later
in this chapter).


%---------------------------------------
\begin{theorem}[\thmd{Sufficient statistic theorem}]
\label{thm:sstat}
\index{sufficient statistic theorem}
\index{theorems!sufficient statistic theorem}
\index{optimal receiver}
\index{MAP} \index{maximum a-posteriori probability estimation}
\index{ML}  \index{maximum likelihood estimation}
%---------------------------------------
Let $\opSys$ be an additive White Gaussian noise system and
$\Psi$ an orthonormal basis for $\fs(t;u)$ such that
\begin{align*}
   \fr(t)      &=   & [\opCawgn s](t) = \fs(t;u) + \fn(t)      \\
   \Psi      &= \set{\psi_n}{n=1,2,\ldots, N}
\end{align*}
\thmbox{\begin{array}{Frc>{\ds}l}
   1. & P\set{ \fs(t;u)}{\fr(t)} &=&    P\set{\fs(t;u)}{\fdotr_1,\;\fdotr_2,\ldots,\fdotr_N}            \\
   2. & \estMAP[u]               &\eqd& \arg\max_{u} P\set{\fs(t;u)}{\fr(t)} \\
      &                          &=&    \arg\max_{u} P\set{\fs(t;u)}{\fdotr_1,\;\fdotr_2,\ldots,\fdotr_N} \\
   3. & \estML[u]                &\eqd& \arg\max_{u} P\set{\fr(t)}{\fs(t;u)} \\
      &                          &=&    \arg\max_{u} P\set{\fdotr_1,\;\fdotr_2,\ldots,\fdotr_N}{\fs(t;u)}
\end{array}}
\end{theorem}
\begin{proof}
Let
\begin{align*}
  \fn'(t) &\eqd \fn(t) - \sum_{n=1}^\xN \fdotn_n \psi_n(t) \\
  R       &\eqd \set{\fdotr_n}{n=1,2,\ldots,N}
\end{align*}

\begin{enumerate}
\item The relationship between $\setR$ and $\fn'(t)$ is given by
\begin{align*}
   \fr(t)
     &= \sum_{n=1}^\xN \inprod{\fr(t)}{\psi_n(t)}\psi_n(t) +
        \left[\fr(t)- \sum_{n=1}^\xN \inprod{\fr(t)}{\psi_n(t)}\psi_n(t) \right]
   \\&= \sum_{n=1}^\xN \inprod{\fr(t)}{\psi_n(t)}\psi_n(t) +
        \left[\fr(t)- \sum_{n=1}^\xN \inprod{\fs(t)+\fn(t)}{\psi_n(t)}\psi_n(t) \right]
   \\&= \sum_{n=1}^\xN \fdotr_n\psi_n(t) +
        \left[\fs(t)+\fn(t) - \sum_{n=1}^\xN \inprod{\fs(t)}{\psi_n(t)}\psi_n(t)
                        - \sum_{n=1}^\xN \inprod{\fn(t)}{\psi_n(t)}\psi_n(t) \right]
   \\&= \sum_{n=1}^\xN \fdotr_n\psi_n(t) +
        \fs(t)+\fn(t) - \fs(t) - \left[ \fn(t) - \fn'(t)\right]
   \\&= \sum_{n=1}^\xN \fdotr_n\psi_n(t) + \fn'(t).
\end{align*}

\item The set of statistics $\setR$ and the random process $\fn'(t)$ are
uncorrelated:
\begin{align*}
   \pEb{\fdotr_n \fn'(t)}
     &= \pEb{\inprod{\fr(t)}{\psi_n(t)}\left( \fn(t)-\sum_{n=1}^\xN \inprod{\fn(t)}{\psi_n(t)}\psi_n(t)\right)}
   \\&= \pEb{\inprod{\fs(t)+\fn(t)}{\psi_n(t)}\left( \fn(t)-\sum_{n=1}^\xN \inprod{\fn(t)}{\psi_n(t)}\psi_n(t)\right)}
   \\&= \pEb{\Bigg(\inprod{\fs(t)}{\psi_n(t)}+\inprod{\fn(t)}{\psi_n(t)}\Bigg)
            \left( \fn(t)-\sum_{n=1}^\xN \inprod{\fn(t)}{\psi_n(t)}\psi_n(t)\right)}
   \\&= \pEb{\Bigg(\fdots_n+\fdotn_n\Bigg)
            \left( \fn(t)-\sum_{n=1}^\xN \fdotn_n\psi_n(t)\right)}
   \\&= \pEb{\fdots_n \fn(t) - \fdots_n \sum_{n=1}^\xN \fdotn_n\psi_n(t)
            +\fdotn_n \fn(t) - \fdotn_n \sum_{n=1}^\xN \fdotn_n\psi_n(t) }
   \\&= \pEb{\fdots_n \fn(t)} -
        \pEb{\fdots_n \sum_{n=1}^\xN \fdotn_n\psi_n(t)} +
        \pEb{\inprod{\fn(t)}{\psi_n(t)} \fn(t)} -
        \pEb{\sum_{m=1}^\xN \fdotn_n \fdotn_m\psi_m(t)}
   \\&= \fdots_n \cancelto{0}{\pE{\fn(t)}} -
        \fdots_n \sum_{n=1}^\xN \cancelto{0}{\pEb{\fdotn_n}}\psi_n(t) +
        \pEb{\inprod{\fn(t)\fn(u)}{\psi_n(u)} } -
        \sum_{m=1}^\xN \pEb{\fdotn_n \fdotn_m}\psi_m(t)
   \\&= 0 - 0 +
        \inprod{\pEb{\fn(t)n(u)}}{\psi_n(u)} -
        \sum_{m=1}^\xN \xN_o\kdelta_{mn} \psi_m(t)
     \qquad\text{(because $\dot{\fn}_n$ is white)}
   \\&= \inprod{\xN_o\delta(t-u)}{\psi_n(u)} - \xN_o\psi_n(t)
   \\&= \xN_o\psi_n(t) - \xN_o\psi_n(t)
   \\&= 0
\end{align*}

\item This implies $\fdotr_n$ and $\fn'(t)$ are uncorrelated.
Since they are Gaussian processes (due to channel operator hypothesis),
they are also independent.

\item Proof that $P\set{\fs(t;u)}{\fr(t)}=P\set{\fs(t;u)}{\fdotr_1,\;\fdotr_2,\ldots,\fdotr_N}$:
\begin{align*}
   P\set{\fs(t;u)}{\fr(t)}
     &= P\set{\fs(t;u)}{\sum_{n=1}^\xN\fdotr_n \psi_n(t) + \fn'(t)}
   \\&= P\set{\fs(t;u)}{R, \fn'(t)}
     && \text{because $\setR$ and $\fn'(t)$ can be extracted by $\inprod{\cdots}{\fpsi_n(t)}$}
   \\&= \frac{P\set{R, \fn'(t)}{\fs(t;u)}  P\setn{\fs(t;u)} }
             {P\setn{R,\fn'(t)}}
   \\&= \frac{\pP{ R|\fs(t;u)}\pP{ \fn'(t)|\fs(t;u)}\pP{\fs(t;u)}}
             {\pP{R}\pP{\fn'(t)}}
     && \text{by independence of $\setR$ and $\fn'(t)$}
   \\&= \frac{\pP{ R|\fs(t;u)}\pP{ \fn'(t)}\pP{\fs(t;u)}}
             {\pP{R}\pP{\fn'(t)}}
   \\&= \frac{\pP{ R|\fs(t;u)} \pP{\fs(t;u)}}
             {\pP{R}}
   \\&= \frac{\pP{ R,\fs(t;u)}}
             {\pP{R}}
   \\&= P\set{\fs(t;u)}{R}
\end{align*}

\item Proof that $\setR$ is a sufficient statistic for the MAP estimate:
\begin{align*}
   \estMAP[u]
     &\eqd \arg\max_u \pP{\fs(t;u)|\fr(t)}
     &&    \text{by definition of MAP estimate}
   \\&=    \arg\max_u \pP{\fs(t;u)|R}
     &&    \text{by result 4.}
\end{align*}

\item Proof that $\setR$ is a sufficient statistic for the ML estimate:
\begin{align*}
   \estML[u]
     &\eqd \arg\max_u \pP{\fr(t)|\fs(t;u)}
     &&    \text{by definition of ML estimate}
   \\&=    \arg\max_u \pP{\sum_{n=1}^\xN\fdotr_n\psi_n(t)+\fn'(t)|\fs(t;u)}
   \\&=    \arg\max_u \pP{R,\fn'(t)|\fs(t;u)}
     &&    \text{because $\setR$ and $\fn'(t)$ can be extracted by  $\inprod{\cdots}{\fpsi_n(t)}$}
   \\&=    \arg\max_u \pP{R|\fs(t;u)}\pP{\fn'(t)|\fs(t;u)}
     &&    \text{by independence of $\setR$ and $\fn'(t)$}
   \\&=    \arg\max_u \pP{R|\fs(t;u)}\pP{\fn'(t)}
     &&    \text{by independence of $\fs(t)$ and $\fn'(t)$}
   \\&=    \arg\max_u \pP{R|\fs(t;u)}
     &&    \text{by independence of $\fn'(t)$ and $u$}
\end{align*}
\end{enumerate}
\end{proof}


Depending on the nature of the channel (additive, white, and/or Gaussian)
we can know certain characteristics of the noise and received statistics.
These are described in the next four theorems.
%======================================
%\subsection{Additive noise channel}
%\label{sec:opCan}
%======================================


%---------------------------------------
\begin{theorem}%[Additive noise projection statistics]
\label{thm:an_stats}
\index{projection statistics!Additive noise channel}
%---------------------------------------
Let $\opC=\opCan$ be an additive noise channel.
\thmbox{
\mcom{\opC=\opCan}{additive noise channel}
\implies
\left\{
\begin{array}{lrcl}
   \pE(\fdotr_n|\theta)       &= \fdots_n(\theta) + \pE \fdotn_n
\end{array}
\right.
}
\end{theorem}
\begin{proof}
\begin{align*}
   (\fdotr_n |\theta)
     &\eqd {\inprod{\fr(t)}{\psi_n(t)}}  |\theta
   \\&=    {\inprod{\fs(t;\theta)+\fn(t)}{\psi_n(t)}}
   \\&=    {\inprod{\fs(t;\theta)}{\psi_n(t)}} +   {\inprod{\fn(t)}{\psi_n(t)}}
   \\&=    \inprod{\sum_{k=1}^\xN \fdots_k(\theta) \psi_k(t)}{\psi_n(t)} + \fdotn_n
   \\&=    \sum_{k=1}^\xN \fdots_k(\theta) \inprod{\psi_k(t)}{\psi_n(t)} + \fdotn_n
   \\&=    \fdots_n(\theta)  + \fdotn_n
\\ \\
   \pE({\fdotr_n} | \theta)
     &= \pEb{\fdots_n(\theta)  + \fdotn_n}
   \\&= \pE{\fdots_n(\theta) } +   \pE{\fdotn_n}
   \\&= \fdots_n(\theta)
\end{align*}
\end{proof}


%======================================
%\subsection{Additive gaussian noise channel}
%\label{sec:opCagn}
%======================================
\begin{figure}[ht] \color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotn_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdots_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdotr_n|\theta_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdots_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdotr_n|\theta_2)$} }
\end{picture}
\end{fsL}
\end{center}
\caption{
  Additive Gaussian noise channel Statistics
   %\label{fig:awgn_stats}
   }
\end{figure}



%---------------------------------------
\begin{theorem}%[Additive Gaussian noise projection statistics]
\label{thm:agn_stats}
\index{projection statistics!Additive Gaussian noise channel}
%---------------------------------------
Let $\opC=\opCagn$ be an additive gaussian noise channel
with distribution $\fn(t)\sim\pN{0}{\sigma^2}$ for all $t$.
%(see \prefpp{fig:agn_stats})
\thmbox{
\mcom{\opC=\opCagn}{additive Gaussian channel}
\implies
\left\{
\begin{array}{rclD}
   \pE\fdotn_n           &=   & 0                               & \\%\text{\scriptsize(noise projection is zero-mean                )} \\
   \pE(\fdotr_n|\theta)  &=   & \fdots_n(\theta)                & \\%\text{\scriptsize(expected receiver projection = transmitted projection )} \\
   \fdotn_n              &\sim& \pN{0}{\sigma^2}                & (noise projections are Gaussian        ) \\
   \fdotr_n|\theta       &\sim& \pN{\fdots_n(\theta)}{\sigma^2} & (receiver projections are Gaussian     ) \\
\end{array}
\right.
}
\end{theorem}
\begin{proof}
\begin{align*}
   \pE\fdotn_{n}
     &= \pE\inprod{\fn(t)}{\psi_n(t)}
   \\&= \inprod{\pE\fn(t)}{\psi_n(t)}
   \\&= \inprod{0}{\psi_n(t)}
   \\&= 0
\\
\\
   (\fdotr_n |\theta)
     &\eqd {\inprod{\fr(t)}{\psi_n(t)}}  |\theta
   \\&=    {\inprod{\fs(t;\theta)+\fn(t)}{\psi_n(t)}}
   \\&=    {\inprod{\fs(t;\theta)}{\psi_n(t)}} +   {\inprod{\fn(t)}{\psi_n(t)}}
   \\&=    \inprod{\sum_{k=1}^\xN \fdots_k(\theta) \psi_k(t)}{\psi_n(t)} + \fdotn_n
   \\&=    \sum_{k=1}^\xN \fdots_k(\theta) \inprod{\psi_k(t)}{\psi_n(t)} + \fdotn_n
   \\&=    \fdots_n(\theta)  + \fdotn_n
\\ \\
   \pE({\fdotr_n} | \theta)
     &= \pEb{\fdots_n(\theta)  + \fdotn_n}
   \\&= \pE{\fdots_n(\theta) } +   \pE{\fdotn_n}
   \\&= \fdots_n(\theta)
\end{align*}

The distributions follow because they are linear operations on
Gaussian processes.
\end{proof}




%======================================
%\subsection{Additive white noise channel}
%\label{sec:opCawn}
%======================================


%---------------------------------------
\begin{theorem}%[Additive white noise projection statistics]
\label{thm:awn_stats}
\index{projection statistics!Additive white noise channel}
%---------------------------------------
Let $\opC=\opCawn$ be an additive white noise channel.
\thmbox{
\mcom{\opC=\opCawn}{additive white channel}
\implies
\left\{
\begin{array}{lclD}
   \pE\fdotn_n                            &=& 0                     & (noise projection is zero-mean) \\
   \pE(\fdotr_n|\theta)                   &=& \fdots_n(\theta)      & (expected receiver projection = transmitted projection) \\
   \cov{\fdotn_n}{\fdotn_m}               &=& \sigma^2 \kdelta_{nm} & (noise projections are uncorrelated) \\
   \cov{\fdotr_n|\theta}{\fdotr_m|\theta }&=& \sigma^2 \kdelta_{nm} & (receiver projections are uncorrelated)
\end{array}
\right.
}
\end{theorem}

\begin{proof}
Because the noise is additive (see \prefpp{thm:an_stats})
\begin{align*}
   \pE\fdotn_{n}           &= 0  \\
   (\fdotr_n |\theta)      &= \fdots_n(\theta)  + \fdotn_n \\
   \pE({\fdotr_n} | \theta) &= \fdots_n(\theta).
\end{align*}

Because the noise is also white,
\begin{align*}
   \cov{\fdotn_m}{\fdotn_n}
      &= \cov{\inprod{\fn(t)}{\psi_m(t)}}{\inprod{\fn(t)}{\psi_n(t)}}
    \\&= \pEb{\inprod{\fn(t)}{\psi_m(t)} \inprod{\fn(t)}{\psi_n(t)}}
    \\&= \pEb{\inprod{\fn(t)}{\psi_m(t)} \inprod{n(u)}{\psi_n(u)}}
    \\&= \pEb{ \inprod{n(u)\inprod{\fn(t)}{\psi_m(t)}}{\psi_n(u)}}
    \\&= \pEb{ \inprod{\inprod{n(u)\fn(t)}{\psi_m(t)}}{\psi_n(u)}}
    \\&= \inprod{\inprod{\pEb{ n(u)\fn(t)}}{\psi_m(t)}}{\psi_n(u)}
    \\&= \inprod{\inprod{\sigma^2 \delta(t-u)}{\psi_m(t)}}{\psi_n(u)}
    \\&= \sigma^2 \inprod{\psi_n(t)}{\psi_m(t)}
    \\&= \left\{
          \begin{tabular}{ll}
             $\sigma^2$ & for $n=m$ \\
             $0$   & for $n\ne m$.
          \end{tabular}
          \right.
\\
\\
   \cov{\fdotr_n|\theta}{\fdotr_m|\theta }
      &= \pEb{\fdotr_n \fdotr_m |\theta} - [\pE\fdotr_n|\theta][\pE\fdotr_m|\theta ]
    \\&= \pEb{(\fdots_n(\theta) +\fdotn_n)(\fdots_m(\theta) +\fdotn_m)} - \fdots_n(\theta) \fdots_m(\theta)
    \\&= \pEb{(\fdots_n(\theta) +\fdotn_n)(\fdots_m(\theta) +\fdotn_m)} - \fdots_n(\theta) \fdots_m(\theta)
    \\&= \pEb{\fdots_n(\theta) \fdots_m(\theta) +\fdots_n(\theta) \fdotn_m+ \fdotn_n\fdots_m(\theta) +\fdotn_n\fdotn_m } - \fdots_n(\theta) \fdots_m(\theta)
    \\&= \fdots_n(\theta) \fdots_m(\theta) + \fdots_n(\theta) \pEb{\fdotn_m}+ \pEb{\fdotn_n}\fdots_m(\theta) +\pEb{\fdotn_n\fdotn_m}  - \fdots_n(\theta) \fdots_m(\theta)
    \\&= 0 + \fdots_n(\theta) \cdot0 + 0\cdot\fdots_m(\theta) + \cov{\fdotn_n}{\fdotn_m}+[\pE\fdotn_n][\pE\fdotn_m]
    \\&= \sigma^2 \kdelta_{nm} + 0\cdot0
    \\&= \left\{
          \begin{tabular}{ll}
             $\sigma^2$ & for $n=m$ \\
             $0$   & for $n\ne m$.
          \end{tabular}
          \right.
\end{align*}
\end{proof}


%======================================
%\subsection{Additive white gaussian noise channel}
%\label{sec:opCawgn}
%======================================
\begin{figure}[ht] \color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotn_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdots_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdotr_n|\theta_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdots_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdotr_n|\theta_2)$} }
\end{picture}
\end{fsL}
\end{center}
\caption{
  Additive white Gaussian noise channel statistics
   \label{fig:awgn_stats}
   }
\end{figure}



%---------------------------------------
\begin{theorem}%[AWGN projection statistics]
\label{thm:awgn_stats}
\label{thm:ms_stats}
\index{projection statistics!Additive white Gaussian noise channel}
%---------------------------------------
Let $\opC=\opCawgn$ be an additive gaussian noise channel
with distribution $\fn(t)\sim\pN{0}{\sigma^2}$ for all $t$.
\thmbox{
\mcom{\opC=\opCawgn}{AWGN}
\implies
\left\{
\begin{array}{rclD}
   \fdotn_n                            &\sim& \pN{0}{\sigma^2}                     & (noise projections are Gaussian        ) \\
   \fdotr_n|\theta                     &\sim& \pN{\fdots_n(\theta)}{\sigma^2}      & (receiver projections are Gaussian     ) \\
   \cov{\fdotn_n}{\fdotn_m}            &=   & \sigma^2 \kdelta_{nm}                & (noise projections are uncorrelated    ) \\
   \cov{\fdotr_n}{\fdotr_m }           &=   & \sigma^2 \kdelta_{nm}                & (receiver projections are uncorrelated ) \\
   \psp\{\fdotn_n=a \land \fdotn_m=b\} &=   & \psp\{\fdotn_n=a\}\psp\{\fdotn_m=b\} & (noise    projections are independent  ) \\
   \psp\{\fdotr_n=a \land \fdotr_m=b\} &=   & \psp\{\fdotr_n=a\}\psp\{\fdotr_m=b\} & (receiver projections are independent  )
\end{array}
\right.
}
\end{theorem}

\begin{proof}
The distributions follow because they are linear operations on
Gaussian processes.

By \prefpp{thm:awn_stats} (for AWN channel)
\begin{align*}
   \pE{\fdotn_{n}} &= 0
\\
   \cov{\fdotn_m}{\fdotn_n} &= \sigma^2 \kdelta_{mn}
\\
   \fdotr_n &= \fdots_n  + \fdotn_n
\\
   \pE{\fdotr_n} &= \fdots_n
\\
   \cov{\fdotr_n}{\fdotr_m } &= \sigma^2 \kdelta_{mn}
\end{align*}
Because the processes are Gaussian,
uncorrelatedness implies independence.
\end{proof}




%======================================
\section{Optimal symbol estimation}
\label{sec:awgn_est}
\index{maximum likelihood estimation}
%======================================
The AWGN projection statistics provided by
\prefpp{thm:awgn_stats} help generate the optimal
ML-estimates for a number of communication systems.
These ML-estimates can be expressed in either of two standard forms:
\begin{liste}
  \item {\bf Spectral decompostion}:
     The optimal estimate is expressed in terms of \hie{projections}
     of signals onto orthonormal basis functions.
  \item {\bf Matched signal}:
     The optimal estimate is expressed in terms of the (noisy)
     received signal correlated with (``matched" with)
     the (noiseless) transmitted signal.
\end{liste}
\prefpp{thm:estML_general} (next) expresses the general
optimal ML estimate in both of these forms.

Parameter detection is a special case of parameter estimation.
In parameter detection, the estimate is a member of an finite set.
In parameter estimation, the estimate is a member of an infinite set
(Section~\ref{sec:awgn_est} page~\pageref{sec:awgn_est}).


%---------------------------------------
\begin{theorem}[General ML estimation]
\index{maximum likelihood estimation!general}
\label{thm:estML_general}
\label{thm:ml_est_det}
%\citepp{srv}{157}{158}
%---------------------------------------
Let $\Psi$ be an orthonormal set spanning $\fs(t)$ such that
\begin{align*}
  \Psi     &\eqd& \{ \psi_1(t), \psi_2(t), \ldots, \psi_n(t)\} \\
  \fdotr_n &\eqd& \inprod{\fr(t)}{\psi_n(t)}                   \\
  \fdots_n &\eqd& \inprod{\fs(t)}{\psi_n(t)}                   \\
  \fr(t)     &=    [\opCawgn s](t) = \fs(t;u) + \fn(t).
\end{align*}
Then the optimal ML-estimate $\estML[u]$ of parameter $ u $ is
\thmbox{\begin{array}{rc>{\ds}l@{\qquad}D}
   \estML[u]
     &=& \arg\min_u \left[ \sum_{n=1}^\xN [\fdotr_n-\fdots_n(u)]^2 \right]
       & (spectral decomposition)
   \\&=& \arg\max_u
         \left[ 2\inprod{\fr(t)}{\fs(t;u)}-\norm{\fs(t;u)}^2 \right]
       & (matched signal)
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML[u]
     &= \arg\max_u \pP{\fr(t)|\fs(t;u)}
   \\&= \arg\max_u \pP{\fdotr_1,\fdotr_2,\ldots,\fdotr_n|\fs(t;u)}
     && \text{by \prefpp{thm:sstat}}
   \\&= \arg\max_u \prod_{n=1}^\xN \pP{\fdotr_n|\fs(t;u)}
   \\&= \arg\max_u \prod_{n=1}^\xN \pdfpb{\fdotr_n|\fs(t;u)}
   \\&= \arg\max_u \prod_{n=1}^\xN
         \frac{1}{\sqrt{2\pi\sigma^2}}
         \exp{\frac{[\fdotr_n-\fdots_n(u)]^2}{-2\sigma^2} }
     && \text{by \prefpp{thm:awgn_stats}}
   \\&= \arg\max_u
         \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
         \exp{\frac{-1}{2\sigma^2} \sum_{n=1}^\xN [\fdotr_n-\fdots_n(u)]^2 }
   \\&= \arg\max_u
         \left[ -\sum_{n=1}^\xN [\fdotr_n-\fdots_n(u)]^2 \right]
\\ \\
   \\&= \arg\max_u
         \left[ -\lim_{N\to\infty}\sum_{n=1}^\xN [\fdotr_n-\fdots_n(u)]^2 \right]
     && \text{by \prefpp{thm:sstat}}
   \\&= \arg\max_u
         \left[ -\norm{\fr(t)-\fs(t;u)}^2 \right]
     && \text{by Plancheral's formula} %\xref{thm:inprod_plancheral}
   \\&= \arg\max_u
         \left[ -\norm{\fr(t)}^2 +2\Re\inprod{\fr(t)}{\fs(t;u)}-\norm{\fs(t;u)}^2 \right]
   \\&= \arg\max_u
         \left[ 2\inprod{\fr(t)}{\fs(t;u)}-\norm{\fs(t;u)}^2 \right]
     && \text{because $\fr(t)$ independent of $u$}
\end{align*}
\end{proof}



%---------------------------------------
\begin{theorem}[\thmd{ML amplitude estimation}]
\label{thm:estML_amplitude}
\index{maximum likelihood estimation!amplitude}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that\citepp{srv}{158}{159}
\begin{align*}
   \fr(t)     &=     [\opCawgn s](t) = \fs(t;a) + \fn(t) \\
   \fs(t;a)   &\eqd  a  \sym(t).
\end{align*}
Then
\thmbox{\begin{array}{rclM}
  \estML[a]    &=&  \frac{1}{\norm{\lambda(t)}^2} \inprod{\fr(t)}{\lambda(t)}
               &    (optimal ML-estimate of $a$)
             \\&=&  \frac{1}{\norm{\sym(t)}^2} \sum_{n=1}^\xN \fdotr_n \fdotlam_n
               & %\text{(optimal ML-estimate of $a$)}
\\\pE\estML[a] &=& a
               &   ($\estML[a]$ is {\bf unbiased})
\\\var\estML[a]&=& \frac{\sigma^2}{\norm{\sym(t)}^2}
               &  (variance of estimate $\estML[a]$)
\\\var\estML[a]&=& \mbox{CR lower bound}
               &   ($\estML[a]$ is an {\bf efficient estimate})
\end{array}}
\end{theorem}

\begin{proof}\\
\begin{enumerate}
\item ML estimate in ``matched signal" form:
\begin{align*}
   \estML[a]
     &= \arg\max_a
         \left[ 2\inprod{\fr(t)}{\fs(t;u)}-\norm{\fs(t;\phi)}^2 \right]
         \hspace{8ex}\mbox{by \prefpp{thm:estML_general}}
   \\&= \arg\max_a
         \left[ 2\inprod{\fr(t)}{a\lambda(t)}-\norm{a\lambda(t)}^2 \right]
         \hspace{8ex}\mbox{by hypothesis}
   \\&= \arg_a
         \left[ \pderiv{}{a}2a\inprod{\fr(t)}{\lambda(t)}-\pderiv{}{a}a^2\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ 2\inprod{\fr(t)}{\lambda(t)}-2a\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ \inprod{\fr(t)}{\lambda(t)}=a\norm{\lambda(t)}^2 \right]
   \\&= \frac{1}{\norm{\lambda(t)}^2} \inprod{\fr(t)}{\lambda(t)}
\end{align*}

\item ML estimate in ``spectral decomposition" form:
\begin{align*}
   \estML[a]
     &= \arg\min_a
         \brp{ \sum_{n=1}^\xN \brs{\fdotr_n - \fdots_n( a )}^2 }
         \hspace{4ex} \mbox{by \prefpp{thm:estML_general}}
   \\&= \arg_a
         \brp{ \pderiv{}{ a }\sum_{n=1}^\xN \brs{\fdotr_n - \fdots_n( a )}^2=0 }
   \\&= \arg_a
         \brp{ 2\sum_{n=1}^\xN \brs{\fdotr_n - \fdots_n( a )}\pderiv{}{ a }\fdots_n( a )=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdotr_n - \inprod{ a \lambda(t)}{\psi_n(t)}}\pderiv{}{ a }\inprod{ a \lambda(t)}{\psi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdotr_n -  a \inprod{\lambda(t)}{\psi_n(t)}}\pderiv{}{ a }( a \inprod{\lambda(t)}{\psi_n(t)})=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdotr_n -  a \fdotlam_n } \inprod{\lambda(t)}{\psi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdotr_n -  a \fdotlam_n } \fdotlam_n=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \fdotr_n\fdotlam_n = \sum_{n=1}^\xN  a \fdotlam_n^2 }
   \\&= \brp{\frac{1}{\sum_{n=1}^\xN \fdotlam_n^2}}
         \sum_{n=1}^\xN \fdotr_n\fdotlam_n
   \\&= \frac{1}{\norm{\lambda(t)}^2}
         \sum_{n=1}^\xN \fdotr_n\fdotlam_n
\end{align*}

\item Prove that the estimate $\estML[a]$ is {\bf unbiased}:

\begin{align*}
   \pE\estML[a]
     &= \pE\frac{1}{\norm{\sym(t)}^2} \int_t \fr(t)\sym(t) \dt
     && \text{by previous result}
   \\&= \pE\frac{1}{\norm{\sym(t)}^2} \int_t [ a \sym(t)+\fn(t)]\sym(t) \dt
     && \text{by hypothesis}
   \\&= \frac{1}{\norm{\sym(t)}^2} \int_t \pE[ a \sym(t)+\fn(t)]\sym(t) \dt
     && \text{by linearity of $\int\cdot\dt$ and $\pE$}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \int_t \sym^2(t) \dt
     && \text{by $\pE$ operation}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \norm{\sym(t)}^2
     && \text{by definition of $\norm{\cdot}^2$}
   \\&=   a
\end{align*}

\item Compute the variance of $\estML[a]$:
\begin{align*}
  \pE \estML[a]^2
    &= \pE \left[ \frac{1}{\norm{\lambda(t)}^2} \int_t \fr(t)\lambda(t) \dt\right]^2
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_t \fr(t)\lambda(t) \dt \int_v \fr(v)\lambda(v) \dv
        \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_t \int_v [a\lambda(t) + \fn(t)][a\lambda(v) + \fn(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_t \int_v
        [a^2\lambda(t)\lambda(v) + a\lambda(t)\fn(v) + a\lambda(v)\fn(t) + \fn(t)\fn(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_t \int_v
        [a^2\lambda(t)\lambda(v) + 0 + 0 + \sigma^2\delta(t-v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        \int_t \int_v a^2\lambda^2(t)\lambda^2(v) \dv\dt +
        \frac{1}{\norm{\lambda(t)}^4}
        \int_t \int_v \sigma^2\delta(t-v) \lambda(t) \lambda(v) \dv\dt
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        a^2 \int_t \lambda^2(t) \dt \int_v \lambda^2(v) \dv +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2\int_t \lambda^2(t) \dt
  \\&= a^2 \frac{1}{\norm{\lambda(t)}^4}
        \norm{\lambda(t)}^2 \norm{\lambda(v)}^2 +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2 \norm{\lambda(t)}^2
  \\&= a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}
\\
\\
  \var\estML[a]
    &= \pE \estML[a]^2 - (\pE \estML[a])^2
  \\&= \left.\left.\left(a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}\right) - \right( a^2 \right)
  \\&= \frac{\sigma^2}{\norm{\lambda(t)}^2}
\end{align*}

\item Compute the Cram\'er-Rao Bound:
\begin{align*}
   \pdfpb{\fr(t)|s(t; a)}
     &=  \pdfpb{\fdotr_1, \fdotr_2,\ldots,\fdotr_N|s(t; a)}
   \\&=  \prod_{n=1}^\xN \frac{1}{\sqrt{2\pi\sigma^2}}
          \exp{\frac{(\fdotr_n- a\fdotlam_n)^2}{-2\sigma^2}}
   \\&=  \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdotr_n- a\fdotlam_n)^2}
\\
\\
   \pderiv{}{a}\ln\pdfpb{\fr(t)|s(t; a)}
     &=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdotr_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
       +  \pderiv{}{a}\ln
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdotr_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}
          \frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdotr_n- a\fdotlam_n)^2
   \\&=  \frac{1}{-2\sigma^2} \sum_{n=1}^\xN 2(\fdotr_n- a\fdotlam_n)(- \fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdotr_n- a\fdotlam_n)
\\
\\
   \pderiv{^2}{a^2}\ln\pdfpb{\fr(t)|s(t; a)}
     &=  \pderiv{}{a}\pderiv{}{a}\ln\pdfpb{\fr(t)|s(t; a)}
   \\&=  \pderiv{}{a}
          \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdotr_n- a\fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(-\fdotlam_n)
   \\&=  \frac{-1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n^2
   \\&=  \frac{-\norm{\lambda(t)}^2}{\sigma^2}
\\
\\
   \var\estML[a]
     &\eqd \pEb{\estML[a]-\pE\estML[a]}^2
   \\&=    \pEb{\estML[a]- a}^2
   \\&\ge  \frac{-1}{\pE\brp{\pderiv{^2}{ a^2} \ln \pdfpb{\fr(t)|s(t; a)}}}
   \\&=    \frac{-1}{\pE\brp{\frac{-\norm{\lambda(t)}^2}{\sigma^2}}}
   \\&=    \frac{\sigma^2}{\norm{\lambda(t)}^2}
     \qquad\text{(Cram\'er-Rao lower bound of the variance)}
\end{align*}

\item Prove that $\estML[a]$ is an {\bf efficient estimate}:

A estimate is efficient if
$\var\estML[a]=\mbox{CR lower bound}$.
We have already proven this, so $\estML[a]$ is an efficient estimate.

Also, even without explicitly computing the variance of $\estML[a]$,
the variance equals the Cram\'er-Rao lower bound
(and hence $\estML[a]$ is an efficient estimate)
if and only if
\[ \estML[a] -  a =
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{ a^2} \ln \pdfpb{\fr(t)|s(t; a)}
           \right)} \right)
   \left(\pderiv{}{ a} \ln \pdfpb{\fr(t)|s(t; a)} \right).
\]

\begin{align*}
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{ a^2} \ln \pdfpb{\fr(t)|s(t; a)}
           \right)} \right)
   \left(\pderiv{}{ a} \ln \pdfpb{\fr(t)|s(t; a)} \right)
     &= \left(\frac{\sigma^2}{\norm{\lambda(t)}^2}\right)
         \left(
           \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam( \fdotr - a \fdotlam)
         \right)
   \\&= \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam \fdotr -
         \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam^2
   \\&= \estML[a] - a
\end{align*}


\end{enumerate}
\end{proof}


%---------------------------------------
\begin{theorem}[\thmd{ML phase estimation}]
\label{thm:estML_phase}
\index{maximum likelihood estimation!phase}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that\citepp{srv}{159}{160}
$\begin{array}[t]{rcl}
   \fr(t)     &=& [\opCawgn s](t) = \fs(t;\phi) + \fn(t) \\
   \fs(t;\phi) &=& A\cos(2\pi f_ct +  \phi).
\end{array}$
\\
Then the optimal ML-estimate of parameter $ \phi $ is
\thmbox{
   \estML[\phi]
      =   -\atan\left(
           \frac{\inprod{\fr(t)}{\sin(2\pi f_ct)}}
                {\inprod{\fr(t)}{\cos(2\pi f_ct)}}
           \right)
   }
\end{theorem}
\begin{proof}
\begin{align*}
   \estML[\phi]
     &= \arg\max_\phi
         \left[ 2\inprod{\fr(t)}{\fs(t;u)}-\norm{\fs(t;\phi)}^2 \right]
         \hspace{3ex}\mbox{by \prefpp{thm:estML_general}}
   \\&= \arg\max_\phi
         \left[ 2\inprod{\fr(t)}{\fs(t;\phi)} \right]
         \hspace{3ex}\mbox{because $\norm{\fs(t;\phi)}$ does not depend on $\phi$}
   \\&= \arg_\phi
         \left[ \pderiv{}{\phi} \inprod{\fr(t)}{\fs(t;\phi)} = 0 \right]
   \\&= \arg_\phi
         \left[ \inprod{\fr(t)}{\pderiv{}{\phi} \fs(t;\phi)} = 0 \right]
         \hspace{3ex}\mbox{because $\inprod{\cdot}{\cdot}$ is a linear operator}
   \\&= \arg_\phi
         \left[ \inprod{\fr(t)}{\pderiv{}{\phi} A\cos(2\pi f_ct+\phi)} = 0 \right]
   \\&= \arg_\phi
         \left[ \inprod{\fr(t)}{-A\sin(2\pi f_ct+\phi)} = 0 \right]
   \\&= \arg_\phi
         \left[ -A\inprod{\fr(t)}{\cos(2\pi f_ct)\sin\phi+\sin(2\pi f_ct)\cos\phi} = 0 \right]
   \\&= \arg_\phi \left[
           \sin\phi\inprod{\fr(t)}{\cos(2\pi f_ct)} =
          -\cos\phi\inprod{\fr(t)}{\sin(2\pi f_ct)}
           \right]
   \\&= \arg_\phi \left[
           \frac{\sin\phi}{\cos\phi} =
          -\frac{\inprod{\fr(t)}{\sin(2\pi f_ct)}}
                {\inprod{\fr(t)}{\cos(2\pi f_ct)}}
           \right]
   \\&= \arg_\phi \left[
           \tan\phi =
          -\frac{\inprod{\fr(t)}{\sin(2\pi f_ct)}}
                {\inprod{\fr(t)}{\cos(2\pi f_ct)}}
           \right]
   \\&=  -\atan\left(
           \frac{\inprod{\fr(t)}{\sin(2\pi f_ct)}}
                {\inprod{\fr(t)}{\cos(2\pi f_ct)}}
           \right)
\end{align*}
\end{proof}


%---------------------------------------
\begin{theorem}[\thmd{ML estimation of a function of a parameter}]
\label{thm:estML-CR}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that\citepp{srv}{142}{143}
$\begin{array}[t]{rcl}
   \fr(t)     &=& [\opCawgn s](t) = \fs(t;u) + \fn(t) \\
   \fs(t;u)   &=& \fg( u )
\end{array}$\\
where $\fg$ is one-to-one and onto (invertible).
\\
\thmbox{\begin{array}{M>{\ds}rc>{\ds}l}
  Then the optimal ML-estimate of parameter $ u $ is
   & \estML[u] &=& \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdotr_n\right).
  \\
  If an ML estimate $\estML[u]$ is unbiased ($\pE \estML[u] = \theta$) then
    & \var\estML[u] &\ge&
      \frac{\sigma^2}{\xN}
      \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}.
  \\
  If $\fg(\theta) = \theta$ then $\estML[u]$ is an {\bf efficient estimate} such that
   & \var\estML[u] &=& \frac{\sigma^2}{\xN}.
\end{array}}
\end{theorem}

Notice that the variance of the estimator approaches $0$
(approaches a perfect estimator) as $\xN\to\infty$
or as $\sigma^2\to 0$ (noise power decreases).


\begin{proof}
\begin{align*}
   \estML[u]
     &= \arg\min_u
         \left[ \sum_{n=1}^\xN [\fdotr_n-\fg( u )]^2 \right]
   \\&= \arg_u\left[
            \pderiv{}{ u }\sum_{n=1}^\xN [\fdotr_n-\fg( u )]^2 = 0
         \right]
   \\&= \arg_u\left[
             2\sum_{n=1}^\xN [\fdotr_n-\fg( u )]\pderiv{}{ u }\fg( u ) = 0
         \right]
   \\&= \arg_u\left[
             2\sum_{n=1}^\xN [\fdotr_n-\fg( u )] = 0
         \right]
   \\&= \arg_u\left[
             \sum_{n=1}^\xN \fdotr_n = \xN \fg( u )
         \right]
   \\&= \arg_u\left[
             \fg( u ) = \frac{1}{\xN}\sum_{n=1}^\xN \fdotr_n
         \right]
   \\&= \arg_u\left[
              u  = \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdotr_n\right)
         \right]
   \\&= \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdotr_n\right)
\end{align*}


If $\estML[u]$ is unbiased ($\pE\estML[u]= u $), we can use
the Cram\'er-Rao bound to find a lower bound on the variance:

\begin{align*}
   \var\estML[u]
     &\eqd \pEb{\estML[u]-\pE\estML[u]}^2
   \\&= \pEb{\estML[u]-\theta}^2
   \\&\ge \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\fr(t)|s(t;\theta)}
           \right)}
   \\&=   \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln
              \pdfpb{\fdotr_1, \fdotr_2,\ldots,\fdotr_N|s(t;\theta)}
           \right)}
   \\&=   \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
              \exp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdotr_n-\fg(\theta)]^2 }\right]
           \right)}
   \\&=   \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN \right]
              +
              \pderiv{^2}{\theta^2} \ln\left[
              \exp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdotr_n-\fg(\theta)]^2 }\right]
           \right)}
  \\&=   \frac{-1}{\pE\left(
             \pderiv{^2}{\theta^2}
             \left(\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdotr_n-\fg(\theta)]^2 \right)
          \right)}
  \\&=   \frac{2\sigma^2}{\pE\left(
             \pderiv{}{\theta} \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdotr_n-\fg(\theta)]^2
          \right)}
  \\&=   \frac{2\sigma^2}{\pE\left(
             -2\pderiv{}{\theta}
             \pderiv{\fg(\theta)}{\theta}
             \sum_{n=1}^\xN [\fdotr_n-\fg(\theta)]
          \right)}
  \\&=   \frac{-\sigma^2}{\pE\left(
             \pderiv{\fg^2(\theta)}{\theta^2}
             \sum_{n=1}^\xN [\fdotr_n-\fg(\theta)]
             +
             \pderiv{\fg(\theta)}{\theta}
             \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdotr_n-\fg(\theta)]
          \right)}
   \\&=   \frac{-\sigma^2}{\pE\left(
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN [\fdotr_n-\fg(\theta)]
              -N
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           \right)}
   \\&=   \frac{-\sigma^2}{
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN \pE[\fdotr_n-\fg(\theta)]
              -N
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
   \\&=   \frac{-\sigma^2}{
              -N
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
   \\&=   \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}
\end{align*}

The inequality becomes equality (an efficient estimate)
if and only if
\[ \estML[u] - \theta =
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\fr(t)|s(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\fr(t)|s(t;\theta)} \right).
\]

\begin{align*}
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\fr(t)|s(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\fr(t)|s(t;\theta)} \right)
     &= \left(
         \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2} \right)
         \left(\frac{-1}{2\sigma^2}(2)\pderiv{\fg(\theta)}{\theta}
         \sum_{n=1}^\xN [\fdotr_n - \fg(\theta) ]\right)
   \\&= -\frac{1}{\xN}
         \frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\sum_{n=1}^\xN [\fdotr_n - \fg(\theta) ] \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\frac{1}{\xN}\sum_{n=1}^\xN \fdotr_n - \fg(\theta) \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\estML[u] - \fg(\theta) \right)
   \\&= -(\estML[u] - \theta)
\end{align*}
\end{proof}



%=======================================
\section{Optimal symbol detection}
%=======================================
%=======================================
\subsection{Generalized coherent modulation}
\index{orthonormal basis}
%=======================================
\begin{figure}[ht]\color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,200)(-100,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-100,   0 ){\line(1,0){300} }
  %\put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](  0,0)(  0, 60)(  0,120)
  \qbezier[30](100,0)(100, 60)(100,120)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -10 ){\makebox(0,0)[t]{$\fdots_n(m)$} }
  \put( 100, -10 ){\makebox(0,0)[t]{$\fdots_p(m)$} }
  \put(   0, 130 ){\makebox(0,0)[bl]{$(\fdotr_n|m)$} }
  \put( 100, 130 ){\makebox(0,0)[bl]{$(\fdotr_p|m)$} }
\end{picture}
\end{fsL}
\end{center}
\caption{
  Distributions of orthonormal components
   \label{fig:gcm_pdf}
   }
\end{figure}



%---------------------------------------
\begin{theorem}
%---------------------------------------
Let
\begin{liste}
   \item $(V, \inprod{\cdot}{\cdot}, S)$ be a modulation space
   \item $\Psi\eqd\{ \psi_n(t): n=1,2,\ldots,N\}$
         be a set of orthonormal functions that span $\setS$
   \item $\fdotr_n\eqd \inprod{\fr(t)}{\psi_n(t)}$
   \item $R \eqd \{ \fdotr_n: n=1,2,\ldots, N\}$
   \item $\fdots_n(m)\eqd \inprod{s(t;m)}{\psi_n(t)}$
\end{liste}

and let $V$ be partitioned into {\bf decision regions}
\[ \{ D_m: m=1,2,\ldots, |S|\} \]
such that
\[ \fr(t)\in D_{\hat{m}} \iff \hat{m}=\arg\max_m\pP{s(t;m)|\fr(t)}. \]

Then the {\bf probability of detection error} is
\thmbox{
   \pP{\mbox{error}} =
     1 - \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
         \sum_m \pP{\mbox{$m$ sent}}
         \int_{\vr \in D_m}
         \exp{\frac{-1}{2\sigma^2}
                   \sum_{n=1}^\xN [\fdotr_n-\fdots_n(m)]^2 }
         \;d\vr .
 }
\end{theorem}

\begin{proof}
\begin{align*}
   \pP{\mbox{error}}
     &= 1 - \pP{\mbox{no error}}
   \\&= 1 - \sum_m \pP{\mbox{($m$ sent)}\land\mbox{($\hat{m}=m$ detected)}}
   \\&= 1 - \sum_m \pP{\mbox{($\hat{m}=m$ detected)}|\mbox{($m$ sent)}}
                    \pP{\mbox{$m$ sent}}
   \\&= 1 - \sum_m \pP{\mbox{$m$ sent}}
                    \pP{\vr |\mbox{($m$ sent)}}
   \\&= 1 - \sum_m \pP{\mbox{$m$ sent}}
                    \int_{\vr \in D_m}\pdfpb{\vr |\mbox{($m$ sent)}} d\vr 
   \\&= 1 - \sum_m \pP{\mbox{$m$ sent}}
                    \int_{\vr \in D_m} \prod_n \pdfpb{\fdotr_n|m} \;d\vr 
   \\&= 1 - \sum_m \pP{\mbox{$m$ sent}}
                    \int_{\vr \in D_m} \prod_{n=1}^\xN
                    \frac{1}{\sqrt{2\pi\sigma^2}}
                    \exp{\frac{-[\fdotr_n-\pE\fdotr_n]^2}{2\sigma^2}}
                    \;d\vr 
   \\&= 1 - \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
         \sum_m \pP{\mbox{$m$ sent}}
         \int_{\vr \in D_m}
         \exp{\frac{-1}{2\sigma^2}
                   \sum_{n=1}^\xN [\fdotr_n-\fdots_n(m)]^2 }
         \;d\vr 
\end{align*}
\end{proof}



%=======================================
\subsection{Frequency Shift Keying (FSK)}
\index{Frequency Shift Keying!coherent}
\index{FSK!coherent}
%=======================================
%---------------------------------------
\begin{theorem}
%---------------------------------------
In an FSK modulation space,
the optimal ML estimator of $m$ is
   \thmbox{ \hat{m} = \arg\max_m \fdotr_m. }
\end{theorem}

\begin{proof}
\begin{align*}
   \hat{m}
     &=  \arg\max_m \pP{\fr(t)|s(t;m)}
   \\&=  \arg\min_m \sum_{n=1}^\xN [\fdotr_n - \fdots_n(m)]^2
       & \ds \mbox{ by \prefpp{thm:ml_est_det} }
   \\&=  \arg\min_m \sum_{n=1}^\xN [\fdotr^2_n -2\fdotr_n\fdots_n(m)+\fdots^2_n(m)]
   \\&=  \arg\min_m \sum_{n=1}^\xN [ -2\fdotr_n\fdots_n(m)+\fdots^2_n(m)]
       & \ds \mbox{ $\fdotr^2_n$ is independent of $m$}
   \\&=  \arg\min_m \sum_{n=1}^\xN [ -2\fdotr_n a\kdelta_{mn}+ a^2\kdelta_{mn}]
       & %\ds \mbox{ by \prefpp{def:fsk}}
   \\&=  \arg\min_m [ -2a\fdotr_m + a^2]
   \\&=  \arg\min_m [ -\fdotr_m ]
       & \ds \mbox{ $a$ and $2$ independent of $m$}
   \\&=  \arg\max_m [ \fdotr_m ]
\end{align*}
\end{proof}


%---------------------------------------
\begin{theorem}
%---------------------------------------
If an FSK modulation space let
\[
\begin{array}{rcl l| lllll}
   z_2 &\eqd& \fdotr_{1}(1) - \fdotr_{2}(1) &\hspace{1cm}& z_2>0 &\implies& \fdotr_1>\fdotr_2 &|& m=1 \\
   z_3 &\eqd& \fdotr_{1}(1) - \fdotr_{3}(1) &\hspace{1cm}& z_3>0 &\implies& \fdotr_1>\fdotr_3 &|& m=1 \\
       &\vdots&                               &            & \\
   z_M &\eqd& \fdotr_{1}(1) - \fdotr_{M}(1) &\hspace{1cm}& z_M>0 &\implies& \fdotr_1>\fdotr_M &|& m=1 \\
\end{array}
\]

Then the {\bf probability of detection error} is
\thmbox{
   \pP{\mbox{error}}
     = 1 - \frac{M-1}{M} \int_0^\infty\int_0^\infty \cdots \int_0^\infty
                           \pdfp(z_2,z_3,\ldots,z_M) \; dz_2 dz_3 \cdots dz_M
}
where

\begin{math}
\pdfp(z_2,z_3,\ldots,z_M)
   =
   \frac{1}{(2\pi)^\frac{M-1}{2}\sqrt{\mathrm{det}{R}}}
   \exp{
      -\frac{1}{2}
      \left[
      \begin{array}{c}
         z_2 - \fdots \\
         z_3 - \fdots \\
         \vdots \\
         z_M - \fdots
      \end{array}
      \right]^T
      R^{-1}
      \left[
      \begin{array}{c}
         z_2 - \fdots \\
         z_3 - \fdots \\
         \vdots \\
         z_M - \fdots
      \end{array}
      \right]
   }
\end{math}

and

\[
   R =
   \left[
   \begin{array}{cccc}
      \cov{z_2}{z_2}  & \cov{z_2}{z_3} & \cdots & \cov{z_2}{z_M}  \\
      \cov{z_3}{z_2}  & \cov{z_3}{z_3} & \cdots & \cov{z_3}{z_M}  \\
      \vdots          & \vdots         & \ddots & \vdots          \\
      \cov{z_M}{z_2}  & \cov{z_M}{z_3} & \cdots & \cov{z_M}{z_M}
   \end{array}
   \right]
%   =
%   \left[
%   \begin{array}{cccc}
%      2\xN_o    &  \xN_o   &  \cdots &  \xN_o   \\
%       \xN_o    & 2\xN_o   &  \cdots &  \xN_o   \\
%       \vdots & \vdots &  \ddots & \vdots \\
%       \xN_o    &  \xN_o   &  \cdots &  2\xN_o
%   \end{array}
%   \right]
   =
   \xN_o \left[
   \begin{array}{cccc}
      2      & 1      &  \cdots &  1     \\
      1      & 2      &  \cdots &  1     \\
      \vdots & \vdots &  \ddots & \vdots \\
      1      & 1      &  \cdots &  2
   \end{array}
   \right]
\]


The inverse matrix $R^{-1}$ is equivalent to (????) \attention

\[
   R^{-1} \eqq
   \frac{1}{M\xN_o} \left[
   \begin{array}{cccc}
      M-1     & -1      &  \cdots &  -1     \\
      -1      & M-1     &  \cdots &  -1     \\
      \vdots  & \vdots  &  \ddots & \vdots  \\
      -1      & -1      &  \cdots & M-1
   \end{array}
   \right]
\]
\end{theorem}


\begin{proof}
\begin{align*}
   \pE{z_k}
     &= \pEb{\fdotr_{11}-\fdotr_{1k}}
   \\&= \pE{\fdotr_{11}}- \pE{\fdotr_{1k}}
   \\&= \fdots - 0
   \\&= \fdots
\end{align*}

\begin{align*}
   \cov{z_m}{z_n}
     &= \pEb{z_m z_n} - [\pE z_m][\pE z_n]
   \\&= \pEb{(\fdotr_{11}-\fdotr_{1m})(\fdotr_{11}-\fdotr_{1n})} - \fdots^2
   \\&= \pEb{\fdotr_{11}^2 -\fdotr_{11}\fdotr_{1n} - \fdotr_{1m}\fdotr_{11} \fdotr_{1m}\fdotr_{1n}} - \fdots^2
   \\&= [\var \fdotr_{11} + (\pE \fdotr_{11})^2] - \pEb{\fdotr_{11}}\pEb{\fdotr_{1n}} - \pEb{\fdotr_{1m}}\pEb{\fdotr_{11}} + [\cov{\fdotr_{1m}}{\fdotr_{1n}} + (\pE \fdotr_{1m})(\pE \fdotr_{1n})] - \fdots^2
   \\&= [\var \fdotr_{11} + \fdots^2] - a\cdot0 - 0\cdot a + [\cov{\fdotr_{1m}}{\fdotr_{1n}} + 0\cdot0] - \fdots^2
   \\&= \var \fdotr_{11} + \cov{\fdotr_{1m}}{\fdotr_{1n}}
   \\&= \xN_o + \cov{\fdotr_{1m}}{\fdotr_{1n}}
   \\&= \left\{
         \begin{tabular}{ll}
            $2\xN_o$ & for $m=n$ \\
            $\xN_o$  & for $m\ne n$.
         \end{tabular}
         \right.
\end{align*}

\begin{align*}
   P\{\mbox{error}\}
     &= 1 - P\{\mbox{no error}\}
   \\&= 1 - \sum_{m=1}^M P\{\mbox{m transmitted)}\land (\forall k\ne m, \fdotr_m > \fdotr_k \}
   \\&= 1 - (M-1) P\{\mbox{1 transmitted)}\land (\fdotr_{11}>\fdotr_{12}) \land (\fdotr_{11}>\fdotr_{13}) \land\cdots\land (\fdotr_{11}>\fdotr_{1M})  \}
   \\&= 1 - (M-1) P\{(\fdotr_{11}-\fdotr_{12}>0) \land (\fdotr_{11}-\fdotr_{13}>0) \land\cdots\land (\fdotr_{11}-\fdotr_{1M}>0)|\mbox{1 transmitted)}\}P\{\mbox{1 transmitted)}  \}
   \\&= 1 - \frac{M-1}{M} P\{(z_2>0) \land (z_3>0) \land\cdots\land (z_M>0)|\mbox{1 transmitted)}\}
   \\&= 1 - \frac{M-1}{M}
   \int_0^\infty\int_0^\infty \cdots \int_0^\infty
        \pdfp(z_2,z_3,\ldots,z_M) \;
   dz_2 dz_3 \cdots dz_M.
\end{align*}

\end{proof}





%---------------------------------------
\subsection{Quadrature Amplitude Modulation (QAM)}
\index{Quadrature Amplitude Modulation}
\index{QAM}
\index{Quadrature Amplitude Modulation}
\index{QAM}
%---------------------------------------

%---------------------------------------
\subsubsection{Receiver statistics}
%---------------------------------------
\begin{figure}[ht]
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,200)(-100,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-100,   0 ){\line(1,0){300} }
  %\put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](  0,0)(  0, 60)(  0,120)
  \qbezier[30](100,0)(100, 60)(100,120)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -10 ){\makebox(0,0)[t]{$a_m$} }
  \put( 100, -10 ){\makebox(0,0)[t]{$b_m$} }
  \put( -30, 100 ){\makebox(0,0)[br]{$(\fdotr_c|m)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdotr_s|m)$} }
\end{picture}
\end{fsL}
\end{center}
\caption{
  Distributions of QAM components
   \label{fig:qam_pdf}
   }
\end{figure}

%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $(V,\inprod{\cdot}{\cdot}$ be a QAM modulation space such that
\begin{align*}
   \fr(t) &= \fs(t;m) + \fn(t) \\
   \fdotr_c &\eqd& \inprod{\fr(t)}{\psi_c(t)} \\
   \fdotr_s &\eqd& \inprod{\fr(t)}{\psi_\fs(t)}.
\end{align*}

Then $(\fdotr_c|m)$ and $(\fdotr_s|m)$ are {\bf independent}
and have {\bf marginal distributions}
\begin{align*}
   (\fdotr_c|m) &\sim& \pN{a_m}{\sigma^2} = \pN{r_m\cos\theta_m}{\sigma^2}  \\
   (\fdotr_s|m) &\sim& \pN{b_m}{\sigma^2} = \pN{r_m\sin\theta_m}{\sigma^2}.
\end{align*}
\end{theorem}

\begin{proof}
See \prefpp{thm:ms_stats} page~\pageref{thm:ms_stats}.
\end{proof}



%---------------------------------------
\subsubsection{Detection}
%---------------------------------------
%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $(V,\inprod{\cdot}{\cdot},S)$ be a QAM modulation space with
\begin{align*}
   \fr(t) = \fs(t;m)+\fn(t) \\
   \fdotr_c &\eqd& \inprod{\fr(t)}{\psi_c(t)} \\
   \fdotr_s &\eqd& \inprod{\fr(t)}{\psi_\fs(t)}.
\end{align*}

Then $\{\fdotr_c,\fdotr_s\}$ are sufficient statistics for
optimal ML detection and the optimal ML estimate of $m$ is
\[ \estML[u][m] = \arg\min_m
      \left[
         (\fdotr_c-a_m)^2  + (\fdotr_s-b_m)^2
      \right].
\]
\end{theorem}
\begin{proof}
\begin{align*}
   \estML[u][m]
     &=  \arg\max_m \pP{\fr(t)|s(t;m)}
     && \text{by \prefpp{def:ML}}
   \\&=  \arg\min_m \sum_{n=1}^\xN [\fdotr_n - \fdots_n(m)]^2
     && \text{by \prefpp{thm:ml_est_det}}
   \\&=  \arg\min_m \brs{(\fdotr_c-a_m)^2  + (\fdotr_s-b_m)^2}
     %&& \text{by \prefpp{def:qam}}.
\end{align*}
\end{proof}


%---------------------------------------
\subsubsection{Probability of error}
%---------------------------------------
\begin{figure}[ht]
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.1mm}
\begin{picture}(400,400)(-200,-200)
  %\graphpaper[10](0,0)(400,400)
  \thicklines
  \put(-180 ,   0 ){\line(1,0){360} }
  \put(   0 ,-180 ){\line(0,1){360} }

  \put( 190 ,  -5 ){$\psi_c$}
  \put( -10 , 190 ){$\psi_s$}

  \put(-150 , 150 ){\circle*{10}}
  \put( -50 , 150 ){\circle*{10}}
  \put(  50 , 150 ){\circle*{10}}
  \put( 150 , 150 ){\circle*{10}}

  \put(-150 ,  50 ){\circle*{10}}
  \put( -50 ,  50 ){\circle*{10}}
  \put(  50 ,  50 ){\circle*{10}}
  \put( 150 ,  50 ){\circle*{10}}

  \put(-150 , -50 ){\circle*{10}}
  \put( -50 , -50 ){\circle*{10}}
  \put(  50 , -50 ){\circle*{10}}
  \put( 150 , -50 ){\circle*{10}}

  \put(-150 ,-150 ){\circle*{10}}
  \put( -50 ,-150 ){\circle*{10}}
  \put(  50 ,-150 ){\circle*{10}}
  \put( 150 ,-150 ){\circle*{10}}
\end{picture}
\hspace{2cm}
\begin{picture}(400,400)(-200,-200)
  %\graphpaper[10](0,0)(400,400)
  \thicklines
  \put(-180 ,   0 ){\line(1,0){360} }
  \put(   0 ,-180 ){\line(0,1){360} }
  \thicklines
  \put(-180 , 100 ){\line(1,0){360} }
  \put(-180 ,-100 ){\line(1,0){360} }
  \put(-100 ,-180 ){\line(0,1){360} }
  \put( 100 ,-180 ){\line(0,1){360} }

  \put( 190 ,  -5 ){$\psi_c$}
  \put( -10 , 190 ){$\psi_s$}

  \put(-160 , 150 ){$D_{ 1}$ }
  \put( -60 , 150 ){$D_{ 2}$ }
  \put(  40 , 150 ){$D_{ 3}$ }
  \put( 140 , 150 ){$D_{ 4}$ }

  \put(-160 ,  50 ){$D_{ 5}$ }
  \put( -60 ,  50 ){$D_{ 6}$ }
  \put(  40 ,  50 ){$D_{ 7}$ }
  \put( 140 ,  50 ){$D_{ 8}$ }

  \put(-160 , -50 ){$D_{ 9}$ }
  \put( -60 , -50 ){$D_{10}$ }
  \put(  40 , -50 ){$D_{11}$ }
  \put( 140 , -50 ){$D_{12}$ }

  \put(-160 ,-150 ){$D_{13}$ }
  \put( -60 ,-150 ){$D_{14}$ }
  \put(  40 ,-150 ){$D_{15}$ }
  \put( 140 ,-150 ){$D_{16}$ }
\end{picture}
\end{fsL}
\end{center}
\caption{
   QAM-16 cosstellation and decision regions
   \label{fig:QAM-16}
   }
\end{figure}


%---------------------------------------
\begin{theorem}
%---------------------------------------
In a QAM-16 constellation as shown in \prefpp{fig:QAM-16},
the probability of error is
\[ \pP{\mbox{error}} = \frac{9}{4} Q^2\left(\frac{\fdots_{21}-\fdots_{11}}{2\xN_o}\right).\]
\end{theorem}

\begin{proof}
Let
\[ d \eqd \fdots_{21}-\fdots_{11}.\]

Then
\begin{align*}
   \pP{\mbox{error}}
     &= \sum_{m=1}^M \pP{[s(t;m)\mbox{ transmitted }]\land
                            [(\fdotr_1,\fdotr_2)\notin D_m] }
   \\&= \sum_{m=1}^M \pP{[(\fdotr_1,\fdotr_2)\notin D_m]|
                            [s(t;m)\mbox{ transmitted }] }
                      \pP{[s(t;m)\mbox{ transmitted }]}
   \\&= \frac{1}{M}
         \sum_{m=1}^M \pP{[(\fdotr_1,\fdotr_2)\notin D_m]|
                            [s(t;m)\mbox{ transmitted }] }
   \\&= \frac{1}{M}\left[
         4 \pP{(\fdotr_1,\fdotr_2)\notin D_1 | s_1(t)} +
         8 \pP{(\fdotr_1,\fdotr_2)\notin D_2 | s_2(t)} +
         4 \pP{(\fdotr_1,\fdotr_2)\notin D_6 | s_6(t)}
         \right]
   \\&= \frac{1}{M}\left[
         4 \int\int_{(x,y)\notin D_1} \pdf_{xy|1}(x,y)\dx\dy +
         8 \int\int_{(x,y)\notin D_2} \pdf_{xy|2}(x,y)\dx\dy + \right.
         \\& \left.4 \int\int_{(x,y)\notin D_6} \pdf_{xy|6}(x,y)\dx\dy
         \right]
   \\&= \frac{1}{M}\left[
         4 \int\int_{(x,y)\notin D_1} \pdf_{x|1}(x) \pdf_{y|1}(y)\dx\dy +
         8 \int\int_{(x,y)\notin D_2} \pdf_{x|2}(x) \pdf_{y|2}(y)\dx\dy + \right.
         \\& \left.4 \int\int_{(x,y)\notin D_6} \pdf_{x|6}(x) \pdf_{y|6}(y)\dx\dy
         \right]
   \\&= \frac{1}{M} \left[
         4 Q\left(\frac{d}{2\xN_o}\right) Q\left(\frac{d}{2\xN_o}\right) +
         8 Q\left(\frac{d}{2\xN_o}\right) 2Q\left(\frac{d}{2\xN_o}\right) +
         4\cdot2 Q\left(\frac{d}{2\xN_o}\right) 2Q\left(\frac{d}{2\xN_o}\right)
         \right]
   \\&= \frac{9}{4} Q^2\left(\frac{d}{2\xN_o}\right)
\end{align*}
\end{proof}






%=======================================
\subsection{Phase Shift Keying (PSK)}
\index{Phase Shift Keying}
\index{PSK}
%=======================================



%---------------------------------------
\subsubsection{Receiver statistics}
%---------------------------------------
\begin{figure}[ht]
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,200)(-100,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-100,   0 ){\line(1,0){300} }
  %\put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](  0,0)(  0, 60)(  0,120)
  \qbezier[30](100,0)(100, 60)(100,120)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -10 ){\makebox(0,0)[t]{$r\cos\theta_m$} }
  \put( 100, -10 ){\makebox(0,0)[t]{$r\sin\theta_m$} }
  \put( -30, 100 ){\makebox(0,0)[br]{$(\fdotr_c|m)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdotr_s|m)$} }
\end{picture}
\end{fsL}
\end{center}
\caption{
  Distributions of PSK components
   \label{fig:psk_pdf}
   }
\end{figure}

%---------------------------------------
\begin{theorem}
%---------------------------------------
Let
\begin{align*}
   \fdotr_c   &\eqd& \inprod{\fr(t)}{\psi_c(t)} \\
   \fdotr_s   &\eqd& \inprod{\fr(t)}{\psi_\fs(t)} \\
   \theta_m       &\eqd& \atan\left[\frac{\fdotr_s(m)}{\fdotr_c(m)}\right].
\end{align*}

The statistics $(\fdotr_c|m)$ and $(\fdotr_s|m)$ are {\bf independent}
with marginal distributions
\begin{align*}
   (\fdotr_c|m) &\sim& \pN{r\cos\theta_m}{\sigma^2} \\
   (\fdotr_s|m) &\sim& \pN{r\sin\theta_m}{\sigma^2} \\
   \pdf_{\theta_m}(\theta|m)  &=  \int_0^\infty x \pdf_{\fdotr_c}(x|m)
                                                 \pdf_{\fdotr_s}(x\tan\theta|m) dx.
\end{align*}
\end{theorem}

\begin{proof}

Indepence and marginal distributions of $\fdotr_1(m)$ and $\fdotr_2(m)$
follow directly from
\prefpp{thm:ms_stats} (page~\pageref{thm:ms_stats}).


Let $X\eqd\fdotr_1(m)$, $Y\eqd\fdotr_2(m)$ and $\Theta\eqd\theta_m$.
Then\footnote{A similar example is in \citerp{papoulis}{138}}
\begin{align*}
   \pdft(\theta)d\theta
     &\eqd \pP{\theta < \Theta \le \theta + d\theta}
   \\&=    \pP{\theta < \atan\frac{Y}{X} \le \theta + d\theta}
   \\&=    \pP{\tan(\theta) < \frac{Y}{X} \le \tan(\theta + d\theta)}
   \\&=    \pP{\tan(\theta) < \frac{Y}{X} \le \tan\theta + (1+\tan^2\theta)\dth}
   \\&=    \int_0^\infty \pPa{\tan\theta < \frac{Y}{X} \le \tan\theta + (1+\tan^2\theta)\dth}{(x<X\le x+\dx)}
   \\&=    \int_0^\infty \pPc{\tan\theta < \frac{Y}{x} \le \tan\theta + (1+\tan^2\theta)\dth}{x<X\le x+dx}\pP{x<X\le x+\dx}
   \\&=    \int_0^\infty \pPc{x\tan\theta < Y \le x\tan\theta + x(1+\tan^2\theta)\dth)}{X=x} \pdfx(x)\dx
   \\&=    \int_0^\infty [\ppy(x\tan\theta)x(1+\tan^2\theta)] \ppx(x) \dx \dth
   \\&=    (1+\tan^2\theta) \int_0^\infty x\ppy(x\tan\theta) \ppx(x) \dx \dth
\\\implies\\
   \pdft(\theta)d\theta
     &= (1+\tan^2\theta) \int_0^\infty x\ppy(x\tan\theta) \ppx(x) \dx
\end{align*}
\attention
\end{proof}



%---------------------------------------
\subsubsection{Detection}
%---------------------------------------
%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $(V,\inprod{\cdot}{\cdot},S)$ be a PSK modulation space with
\begin{align*}
   \fr(t) = \fs(t;m)+\fn(t) \\
   \fdotr_c &\eqd& \inprod{\fr(t)}{\psi_c(t)} \\
   \fdotr_s &\eqd& \inprod{\fr(t)}{\psi_\fs(t)}.
\end{align*}

Then $\{\fdotr_c,\fdotr_s\}$ are sufficient statistics for
optimal ML detection and the optimal ML estimate of $m$ is
\[ \estML[u][m] = \arg\min_m
      \left[
         (\fdotr_1-r\cos\theta_m)^2  +
         (\fdotr_2-r\sin\theta_m)^2
      \right].
\]
\end{theorem}

\begin{proof}
\begin{align*}
   \estML[u][m]
     &=  \arg\max_m \pP{\fr(t)|s(t;m)}
     && \text{by \prefpp{def:ML}}
   \\&=  \arg\min_m \sum_{n=1}^\xN [\fdotr_n - \fdots_n(m)]^2
     &&  \text{by \prefpp{thm:ml_est_det}}
   \\&=  \arg\min_m
      \left[
         (\fdotr_1-r\cos\theta_m)^2  +
         (\fdotr_2-r\sin\theta_m)^2
      \right].
    %&& \text{by \prefpp{def:psk}}.
\end{align*}
\end{proof}




%---------------------------------------
\subsubsection{Probability of error}
%---------------------------------------
\begin{figure}[ht]
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.2mm}
\begin{picture}(240,240)(-120,-120)%
  %\color{graphpaper}{\graphpaper[10](-120,-120)(240,240)}%
  \thicklines%
  \color{black}%
    \put(  30 ,   0 ){\makebox(0,0)[l]{$\theta$} }%
  \color{axis}%
    \put(   0 ,   0 ){\line( 2, 1){80}}%
    \put(   0 ,   0 ){\line( 1, 2){40}}%
    \put(   0 ,   0 ){\line(-1, 2){40}}%
    \put(   0 ,   0 ){\line(-2, 1){80}}%
    \put(   0 ,   0 ){\line(-2,-1){80}}%
    \put(   0 ,   0 ){\line(-1,-2){40}}%
    \put(   0 ,   0 ){\line( 1,-2){40}}%
    \put(   0 ,   0 ){\line( 2,-1){80}}%
  \color{blue}%
    \put( 100 ,   0 ){$D_1$}%
    \put(  75 ,  75 ){$D_2$}%
    \put(   0 , 100 ){$D_3$}%
    \put( -75 ,  75 ){$D_4$}%
    \put(-110 ,   0 ){$D_5$}%
    \put( -80 , -80 ){$D_6$}%
    \put(   0 ,-100 ){$D_7$}%
    \put(  75 , -75 ){$D_8$}%
  \color{zero}%
    \put(  80 ,   0 ){\circle{10}}%
    \put(  57 ,  57 ){\circle{10}}%
    \put(   0 ,  80 ){\circle{10}}%
    \put( -57 ,  57 ){\circle{10}}%
    \put( -80 ,   0 ){\circle{10}}%
    \put( -57 , -57 ){\circle{10}}%
    \put(   0 , -80 ){\circle{10}}%
    \put(  57 , -57 ){\circle{10}}%
  \setlength{\unitlength}{0.16mm}%
  \color{red}%
    \input{../common/circle.inp}%
\end{picture}%
\end{fsL}
\end{center}
\caption{
   PSK-8 Decision regions
   \label{fig:PSK_Dm}
   }
\end{figure}

%---------------------------------------
\begin{theorem}
%---------------------------------------
The probability of error using PSK modulation is
\begin{align*}
   \pP{\mbox{error}}
     &= M \left[
              1 - \int_{\frac{2\pi}{M}\left(m-\frac{3}{2}\right)}^{\frac{2\pi}{M}\left(m-\frac{1}{2}\right)}
                  \pdf_{\theta_1}(\theta) \; d\theta
           \right].
\end{align*}
\end{theorem}

\begin{proof}
See \prefpp{fig:PSK_Dm}.

\begin{align*}
   \pP{\mbox{error}}
     &= \sum_{m=1}^M \pP{\mbox{error}|s(t;m) \mbox{ was transmitted}}
   \\&= M \pP{\mbox{error}|s_1(t) \mbox{ was transmitted}}
   \\&= M \left[
              1 - \int_{\frac{2\pi}{M}\left(m-\frac{3}{2}\right)}^{\frac{2\pi}{M}\left(m-\frac{1}{2}\right)}
                  \pdf_{\theta_1}(\theta) \; d\theta
           \right].
\end{align*}
\end{proof}







%---------------------------------------
\subsection{Pulse Amplitude Modulation (PAM)}
\index{Pulse Amplitude Modulation}
\index{PAM}
\index{Pulse Amplitude Modulation}
\index{PAM}
%---------------------------------------


%---------------------------------------
\subsubsection{Receiver statistics}
%---------------------------------------
\begin{figure}[ht]
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.2mm}
\begin{picture}(200,200)(-100,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-100,   0 ){\line(1,0){200} }
  %\put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](  0,0)(  0, 60)(  0,120)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \put(   0, -10 ){\makebox(0,0)[t]{$a_m$} }
  \put( -30, 100 ){\makebox(0,0)[br]{$(\fdotr|m)$} }
\end{picture}
\end{fsL}
\end{center}
\caption{
  Distribution of PAM component
   \label{fig:pam_pdf}
   }
\end{figure}

%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $(V,\inprod{\cdot}{\cdot}$ be a PAM modulation space such that
\begin{align*}
   \fr(t) &= \fs(t;m) + \fn(t) \\
   \fdotr_c &\eqd& \inprod{\fr(t)}{\psi_c(t)} \\
   \fdotr_s &\eqd& \inprod{\fr(t)}{\psi_\fs(t)}.
\end{align*}

Then $(\fdotr|m)$ has {\bf distribution}
\[ \fdotr(m) \sim  \pN{a_m}{\sigma^2}.  \]
\end{theorem}

\begin{proof}
This follows directly from
\prefpp{thm:ms_stats} (page~\pageref{thm:ms_stats}).
\end{proof}


%---------------------------------------
\subsubsection{Detection}
%---------------------------------------
%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $(V,\inprod{\cdot}{\cdot},S)$ be a PAM modulation space with
\begin{align*}
   \fr(t)    &=    \fs(t;m)+\fn(t) \\
   \fdotr &\eqd& \inprod{\fr(t)}{\psi(t)}.
\end{align*}

Then $\fdotr$ is a sufficient statistic for the
optimal ML detection of $m$ and the optimal ML estimate of $m$ is
   \[ \estML[u][m] = \arg\min_m |\fdotr - a_m |. \]
\end{theorem}

\begin{proof}
\begin{align*}
   \estML[u][m]
     &=  \arg\max_m \pP{\fr(t)|a_m}
       & \ds \mbox{ by \prefpp{def:ML}}
   \\&=  \arg\min_m \sum_{n=1}^\xN [\fdotr_n - \fdots_n(m)]^2
       & \ds \mbox{ by \prefpp{thm:ml_est_det} }
   \\&=  \arg\min_m [\fdotr - \fdots(m)]^2
   \\&=  \arg\min_m |\fdotr - \fdots(m)|
\end{align*}
\end{proof}


%---------------------------------------
\subsubsection{Probability of error}
%---------------------------------------
%---------------------------------------
\begin{theorem}
%---------------------------------------
The probability of detection error in a PAM modulation space is
   \[ \pP{\mbox{error}} = 2\frac{M-1}{M} \Qb{\frac{a_2-a_1}{2\sqrt{\xN_o}}} .\]
\end{theorem}

\begin{proof}
Let $d\eqd a_2-a_1$ and $\sigma\eqd \sqrt{\var{\fdotr}}=\sqrt{\xN_o}$.
Also, let the decision regions $D_m$ be as illustrated in \prefpp{fig:PAM_norm}.
Then
\begin{align*}
   \pP{error}
     &= \sum_{m=1}^M \pP{s(t;m) \mbox{ sent } \land r\notin D_m}
   \\&= \sum_{m=1}^M \pP{\fdotr \notin D_m | \fs(t;m) \mbox{ sent } }\pP{s(t;m) \mbox{ sent }}
   \\&= \sum_{m=1}^M \pP{\fdotr_m \notin D_m } \frac{1}{M}
   \\&= \frac{1}{M}\left(
             \Qb{\frac{d}{2\sigma}} +
            2\Qb{\frac{d}{2\sigma}} +
            \ldots
            2\Qb{\frac{d}{2\sigma}} +
             \Qb{\frac{d}{2\sigma}}
         \right)
   \\&= 2\frac{M-1}{M} \Qb{\frac{d}{2\sigma}}
   \\&= 2\frac{M-1}{M} \Qb{\frac{\fdots_2-\fdots_1}{2\sqrt{\xN_o}}}
\end{align*}
\end{proof}




\begin{figure}[ht]
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.2mm}
\begin{picture}(500,160)(-100,-30)
  %\graphpaper[10](-100,0)(500,150)
  \thicklines
  \put(-100 ,   0 ){\line(1,0){500} }

  \put(  50 , -10 ){\line(0,-1){40} }
  \put( 150 , -10 ){\line(0,-1){40} }
  \put( 250 , -10 ){\line(0,-1){40} }

  \put(   0 , -30 ){$D_1$ }
  \put( 100 , -30 ){$D_2$ }
  \put( 200 , -30 ){$D_3$ }
  \put( 300 , -30 ){$D_4$ }

  %\qbezier[12]( 60,  0)( 60, 30)( 60, 60)
  %\qbezier[12](  0, 60)( 30, 60)( 60, 60)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \qbezier( 160,  60)( 200, 180)( 240,  60)
  \qbezier( 100,   0)( 140,   0)( 160,  60)
  \qbezier( 240,  60)( 260,   0)( 300,   0)

  \qbezier( 260,  60)( 300, 180)( 340,  60)
  \qbezier( 200,   0)( 240,   0)( 260,  60)
  \qbezier( 340,  60)( 360,   0)( 400,   0)
\end{picture}
\end{fsL}
\end{center}
\caption{
  4-ary PAM in AWGN channel
   \label{fig:PAM_norm}
   }
\end{figure}




%======================================
\section{Colored noise}
\index{noise!colored}
%======================================
This chapter presented several theorems whose results depended on the
noise being white.
However if the noise is {\bf colored}, then these results are
invalid.
But there is still hope for colored noise.
Processing colored signals can be accomplished using two techniques:
\begin{enume}
   \item Karhunen-Lo\`{e}ve basis functions
      \footnote{{\bf Karhunen-Lo\`{e}ve}: Section~\ref{sec:KL} page~\pageref{sec:KL}}
   \item whitening filter
\footnote{
   \begin{tabular}[t]{lll}
      {\bf Continuous data whitening}: & Section~\ref{sec:whiten}   & page~\pageref{sec:whiten}.\\
      {\bf Discrete data whitening}:   & Section~\ref{sec:d-whiten} & page~\pageref{sec:d-whiten}.
   \end{tabular}
   }
\end{enume}

\paragraph{Karhunen-Lo\`{e}ve.}
If the noise is white, the set $\{\inprod{\fr(t)}{\psi_n(t)}\}$
is a sufficient statistic regardless of which
set $\{\psi_n(t)\}$ of orthonormal basis functions are used.
If the noise is colored, and if $\{\psi_n(t)\}$ satisfy the
Karhunen-Lo\`{e}ve criterion
   \[ \int_{t_2}\Rxx(t_1,t_2)\psi_n(t_2)\dd{t_2} = \lambda_n \psi_n(t_1) \]
then $\{\inprod{\fr(t)}{\psi_n(t)}\}$ is still a sufficient statistic.

\paragraph{Whitening filter.}
The whitening filter makes the received signal $\fr(t)$ statistically white
(uncorrelated in time). In this case,
any orthonormal basis set can be used to generate sufficient statistics.




%======================================
\section{Signal matching}
\index{matched filter}
%======================================
\paragraph{Detection methods.}
There are basically two types of detection methods:
\begin{enume}
   \item signal matching
   \item orthonormal decomposition.
\end{enume}

Let $\setS$ be the set of transmitted waveforms and
$\setR$ be a set of orthonormal basis functions that span $\setS$.
\hie{Signal matching} computes the innerproducts of a
received signal $\fr(t)$ with each signal from $\setS$.
\hie{Orthonormal decomposition} computes the innerproducts of
$\fr(t)$ with each signal from the set $\setR$.

In the case where $|S|$ is large, often $|R|<<|S|$
making orthonormal decomposition much easier to implement.
For example, in a QAM-64 modulation system,
signal matching requires $|S|=64$ innerproduct calculations,
while orthonormal decomposition only requires $|R|=2$
innerproduct calculations because all 64 signals in $\setS$ can be spanned
by just 2 orthonormal basis functions.

\paragraph{Maximizing SNR.}
\prefpp{thm:sstat} shows that the innerproducts of $\fr(t)$ with
basis functions of $\setR$ is sufficient for optimal detection.
\prefpp{thm:mf_maxSNR} (next) shows that a receiver can
maximize the SNR of a received signal when signal matching is used.

%--------------------------------------
\begin{theorem}
\label{thm:mf_maxSNR}
%--------------------------------------
Let $\fs(t)$ be a transmitted signal, $\fn(t)$ noise, and $\fr(t)$ the received signal
in an AWGN channel.
Let the \hie{signal to noise ratio} SNR be defined as
   \[ \snr[\fr(t)] \eqd \frac{|\inprod{\fs(t)}{\fx(t)}|^2}
                            {\pEb{|\inprod{\fn(t)}{\fx(t)}|^2}}. \]
Then, $\snr[\fr(t)]$ is
\thmbox{
  \snr[\fr(t)] \le \frac{2\norm{\fs(t)}^2}{\xN_o }
  }
and is maximized (equality) when $\fx(t)=a\fs(t)$, where $a\in\R$.
\end{theorem}

\begin{proof}
\begin{align*}
   \snr[\fr(t)]
     &\eqd \frac{\abs{\inprod{\fs(t)}{\fx(t)}}^2}
                {\pEb{|\inprod{\fn(t)}{\fx(t)}|^2}}
   \\&=    \frac{\abs{\inprod{\fs(t)}{f(t)}}^2}
                {\pEb{\left[\int_t \fn(t)\fx^\ast(t)\;dt\right]
                      \left[\int_u n(u)f^\ast(u)\;du\right]^\ast}
                }
   \\&=    \frac{|\inprod{\fs(t)}{\fx(t)}|^2}
                {\pEb{\int_t \int_u \fn(t)n^\ast(u)\fx^\ast(t)\fx(u)\;dtdu}}
   \\&=    \frac{|\inprod{\fs(t)}{f(t)}|^2}
                {\int_t \int_u \pEb{\fn(t)n^\ast(u)}\fx^\ast(t)\fx(u)\;dtdu}
   \\&=    \frac{|\inprod{\fs(t)}{\fx(t)}|^2}
                {\int_t \int_u \frac{1}{2}\xN_o\delta(t-u) \fx^\ast(t)\fx(u)\;dtdu}
   \\&=    \frac{|\inprod{\fs(t)}{\fx(t)}|^2}
                {\frac{1}{2}\xN_o \int_t \fx^\ast(t)\fx(t)\dt}
   \\&=    \frac{|\inprod{\fs(t)}{\fx(t)}|^2}
                {\frac{1}{2}\xN_o \norm{\fx(t)}^2}
   \\&\le  \frac{|\norm{\fs(t)}\;\norm{\fx(t)}|^2}
                {\frac{1}{2}\xN_o \norm{\fx(t)}^2}
     &&    \text{by \thme{Cauchy-Schwarz Inequality}}
     &&    \text{\ifsxref{vsinprod}{thm:cs}}
   \\&=    \frac{2\norm{\fs(t)}^2}
                {\xN_o }
\end{align*}
The Cauchy-Schwarz Inequality becomes an equality
($\snr$ is maximized) when $\fx(t)=a\fs(t)$.
\end{proof}

\paragraph{Implementation.}
The innerproduct operations can be implemented using either
  \begin{dingautolist}{"C0}
     \item a correlator or
     \item a matched filter.
  \end{dingautolist}

A correlator is simply an integrator of the form
   \[ \inprod{\fr(t)}{f(t)} = \int_0^T \fr(t)f(t)\dt.\]

A matched filter introduces a function $\fh(t)$ such that
$\fh(t) =\fs(T-t)$ (which implies $\fs(t)=h(T-t)$) giving
  \[
    \mcom{\inprod{\fr(t)}{\fs(t)} = \int_0^T \fr(t)\fs(t)\dt }
         {correlator}
    =
    \mcom{\left.\int_0^\infty \fs(\tau)h(t-\tau)\dtau\right|_{t=T}
            = \left.\fs(t)\conv \fh(t)\right|_{t=T}
         }{matched filter}.
  \]

This shows that $\fh(t)$ is the impulse response of a filter operation
sampled at time $T$. % (see \prefpp{fig:mf}).
By \prefpp{thm:mf_maxSNR}, the optimal impulse response is
$\fh(T-t)=f(t)=\fs(t)$.
That is, the optimal $\fh(t)$ is just a ``flipped" and shifted version of $\fs(t)$.

%\begin{figure}[ht] \color{figcolor}
%\begin{center}
%\begin{fsK}
%\setlength{\unitlength}{0.15mm}
%\begin{picture}(350,100)
%  \thicklines
%  %\graphpaper[10](0,0)(700,100)
%  \put(   90,  60 ){\makebox( 0,50)[br]{$\fr(t)=\fs(t)+\fn(t)$} }
%  \put(    0,  50 ){\vector(1,0){100} }
%
%  \put( 100 ,  00 ){\framebox( 100,100){$\conv \fh(t)$} }
%  \put( 200 ,  50 ){\vector(1,0){ 50} }
%  \put( 250 ,  50 ){\usebox{\picSampler}}
%  \put( 350 ,  50 ){\vector(1,0){100} }
%\end{picture}
%\end{fsK}
%\end{center}
%\caption{
%   Matched Filter
%   \label{fig:mf}
%   }
%\end{figure}



%=======================================
\section{Channel Capacity}
%=======================================
\begin{figure}[ht]
\color{figcolor}
\begin{center}
\begin{fsK}
\setlength{\unitlength}{0.17mm}
\begin{picture}(900,200)(-100,-50)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(-100, 125 ){\vector(0,-1){40}}
  \put(-100, 130 ){\makebox(0,0)[bl]{$R_d$ (data rate)}}
  \put(-100,  60 ){\makebox(30,0)[br]{$\{u_n\}$} }
  \put(-100,  50 ){\vector(1,0){50} }

  \put(-70, -50 ){\dashbox{4}( 280,160){} }
  \put(-70, -40 ){\makebox( 280,160)[b]{transmitter} }
  \put(   50, 130 ){\makebox(0,0)[bl]{$R_c$ (signal rate)}}
  \put(   75, 125 ){\vector(0,-1){40}}
  \put(   50,  60 ){\makebox(50,50)[b]{$\{y_n\}$} }
  \put(   50,  50 ){\vector(1,0){50} }

  \put(- 50,  00 ){\framebox( 100,100){} }
  \put(- 50,  10 ){\makebox( 100,80)[t]{channel} }
  \put(- 50,  10 ){\makebox( 100,80)[ ]{coder} }
  \put(- 50,  10 ){\makebox( 100,80)[b]{$(L,N)$} }
  \put( 100,  00 ){\framebox( 100,100){modulator} }
  \put( 200,  50 ){\vector(1,0){100} }

  \put( 350, 125 ){\vector(0,-1){25}}
  \put( 300, 130 ){\makebox(0,0)[bl]{$\frac{L}{\xN}=\frac{R_d}{R_c}\eqd R<C$ (channel capacity)}}
  \put( 300,  00 ){\framebox( 100,100){} }
  \put( 300,  30 ){\makebox( 100, 40)[t]{channel} }
  \put( 300,  10 ){\makebox( 100, 40)[b]{$\pp(Y|X)$} }
  \put( 210,  60 ){\makebox( 90, 50)[b]{$\fs(t)$} }
  \put( 400,  60 ){\makebox( 80, 50)[b]{$\fr(t)$} }

  \put( 400,  50 ){\vector(1,0){100} }
  \put( 500,  00 ){\framebox(100,100){demodulator} }
  \put( 600,  60 ){\makebox(50,50)[b]{$\{\hat{y}_n\}$} }
  \put( 600,  50 ){\vector(1,0){50}}
  \put( 650,  00 ){\framebox(100,100){} }
  \put( 650,  30 ){\makebox(100,40)[t]{channel} }
  \put( 650,  30 ){\makebox(100,40)[b]{decoder} }
  \put( 480, -50 ){\dashbox{4}( 280,160){} }
  \put( 480, -40 ){\makebox( 280,160)[b]{receiver} }

  \put( 760,  60 ){\makebox(40,50)[b]{$\{\ue_n\}$} }
  \put( 750,  50 ){\vector(1,0){50}}
\end{picture}
\end{fsK}
\end{center}
\caption{
   Memoryless modulation system model
   %\label{fig:i_mod_model}
   }
\end{figure}






\begin{figure}[ht] \color{figcolor}
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.20mm}
\begin{picture}(700,150)(-100,0)
  \thicklines
  %\graphpaper[10](0,0)(500,100)
  \put(-100 ,  60 ){\makebox( 100,0)[b]{$\su$} }
  \put(-100 ,  50 ){\vector(1,0){100} }

  \put(  00 ,  10 ){\makebox( 100, 80)[t]{transmit} }
  \put(  00 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put(  00 ,  10 ){\makebox( 100, 80)[b]{$\opT$} }
  \put(  00 ,  00 ){\framebox( 100,100){} }

  \put( 100 ,  60 ){\makebox( 100,0)[b]{$X$} }
  \put( 100 ,  50 ){\vector(1,0){140} }


  \put( 200 ,  00 ){\makebox(100, 95)[t]{$Z$} }
  \put( 260,   50 ){\line  (1,0){ 45} }
  \put( 250 ,  80 ){\vector(0,-1){20} }
  \put( 250,   50) {\circle{20}                   }
  \put( 200 ,  00 ){\dashbox(100,100){$+$} }
  \put( 200 ,  10 ){\makebox(100, 90)[b]{channel $\opC$} }

  %\put( 200 ,  10 ){\makebox( 100, 80)[t]{channel} }
  %\put( 200 ,  10 ){\makebox( 100, 80)[c]{operation} }
  %\put( 200 ,  10 ){\makebox( 100, 80)[b]{\opC} }
  %\put( 200 ,  00 ){\framebox(100,100){} }

  \put( 300 ,  60 ){\makebox( 100,0)[b]{$Y$} }
  \put( 300 ,  50 ){\vector(1,0){100} }

  \put( 400 ,  00 ){\framebox(100,100){} }
  \put( 400 ,  10 ){\makebox( 100, 80)[t]{receive} }
  \put( 400 ,  10 ){\makebox( 100, 80)[c]{operation} }
  \put( 400 ,  10 ){\makebox( 100, 80)[b]{$\opR$} }

  \put( 500 ,  60 ){\makebox( 100,0)[b]{$\sue$} }
  \put( 500 ,  50 ){\vector(1,0){100} }

  %\put(- 90 , -10 ){\makebox( 0, 0)[tl]{$\vu\eqd\su$} }
  %\put( 110 , -10 ){\makebox( 0, 0)[tl]{$\fs(t;\vu)=\opT\vu$} }
  %\put( 310 , -10 ){\makebox( 0, 0)[tl]{$\fr(t;\vu)=\opC\opT\vu$} }
  %\put( 510 , -10 ){\makebox( 0, 0)[tl]{$\sue=\opR\opC\opT\vu$} }

\end{picture}
\end{fsL}
\end{center}
\caption{
   Additive noise system model
   %\label{fig:i_addNoise_model}
   }
\end{figure}

How much information can be reliably sent through the channel?
The answer depends on the \hie{channel capacity}\ifsxref{info}{def:iC}
$\iC$.
As proven by the \hie{Noisy Channel Coding Theorem} (NCCT)\ifxref{info}{thm:ncct},
each transmitted symbol can carry up to $\iC$ bits for any arbitrarily
small probability of error greater than zero.
The price for decreasing error is increasing the block code size.

Note that the NCCT does not say at what rate
(in bits/second) you can send data through the AWGN channel.
The AWGN channel knows nothing of time (and is therefore not a
realistic channel).
The NCCT channel merely gives a \hie{coding rate}.
That is, the number of information bits each symbol can carry.
Channels that limit the rate (in bits/second) that can be sent through
it are obviously aware of time and are often referred to as
\hie{bandlimited channels}.

%--------------------------------------
\begin{theorem}
%--------------------------------------
Let $Z\sim\pN{0}{\sigma^2}$. Then
\thmbox{  \iH(Z) = \frac{1}{2}\log_2 2\pi e \sigma^2  }
\end{theorem}
\begin{proof}
\begin{align*}
  \iH(Z)
    &= \pEz \log \frac{1}{\pp(Z)}
  \\&= -\pEz \log \pp(z)
  \\&= -\pEz
         \log \left[\frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-z^2}{2\sigma^2}} \right]
  \\&= -\pEz \left[
        -\frac{1}{2}\log(2\pi\sigma^2)
        + \frac{-z^2}{2\sigma^2} \log e
        \right]
  \\&= \frac{1}{2} \pEz \left[
        \log(2\pi\sigma^2)
        + \frac{\log e}{\sigma^2}z^2
        \right]
  \\&= \frac{1}{2} \left[
        \log(2\pi\sigma^2) + \frac{\log e}{\sigma^2}\pEz z^2
        \right]
  \\&= \frac{1}{2} \left[
        \log(2\pi\sigma^2) + \frac{\log e}{\sigma^2}(\sigma^2+0)
        \right]
  \\&= \frac{1}{2} \left[
        \log(2\pi\sigma^2) + \log e
        \right]
  \\&= \frac{1}{2} \log(2\pi e\sigma^2)
\end{align*}
\end{proof}

%--------------------------------------
\begin{theorem}
%--------------------------------------
Let $Y=X+Z$ be a Gaussian channel with $\pE X^2=P$ and
$Z\sim\pN{0}{\sigma^2}$. Then
\thmbox{
  \iI(X;Y) \le \frac{1}{2}\log\left( 1 + \frac{P}{\sigma^2}\right) = \iC
  %\hspace{1cm}\mbox{bits per usage}
  }
\end{theorem}
\begin{proof}
No proof at this time. \attention

Reference: \cite[page 241]{cover}
\end{proof}

%---------------------------------------
\begin{example}
%---------------------------------------

\begin{enumerate}
  \item If there is no transmitted energy ($P=0$), then the capacity of
        the channel to pass information is
    \begin{align*}
      \iC
        &= \frac{1}{2}\log_2\left( 1 + \frac{P}{\sigma^2}\right)
      \\&= \frac{1}{2}\log_2\left( 1 + \frac{0}{\sigma^2}\right)
      \\&= 0
    \end{align*}
  That is, the symbols cannot carry any information.

  \item If there is finite symbol energy and no noise ($\sigma^2=0$),
        then the capacity of the channel to pass information is
    \begin{align*}
      \iC
        &= \frac{1}{2}\log_2\left( 1 + \frac{P}{0}\right)
      \\&= \infty
    \end{align*}
  That is, each symbol can carry an infinite amount of information.
  That is, we can use a modulation scheme with an infinite
  number of of signaling waveforms (analog modulation)
  and thus each symbol can be represented by one of an
  infinite number of waveforms.

  \item If the transmitted energy is ($P=15\sigma^2$),
        then the capacity of the channel to pass information is
    \begin{align*}
      \iC
        &= \frac{1}{2}\log_2\left( 1 + \frac{15\sigma^2}{\sigma^2}\right)
      \\&= \frac{1}{2}\log_2\left( 1 + 15\right)
      \\&= \frac{1}{2} 4
      \\&= 2
    \end{align*}
  This means
  \begin{align*}
    2
      &= \iC
       >  \rchan
       \eqd \frac{\mbox{information bits}}{\mbox{symbol}}
       = \frac{\mbox{information bits}}{\mbox{coded bits}} \times
         \frac{\mbox{coded bits}}{\mbox{symbol}}
       = \rcode \rsym
  \end{align*}
  This means that if the coding rate is $\rcode=1/4$,
  then we must use a modulation with $256$ ($\rsym=8$ bits/symbol)
  or fewer waveforms.

  Conversely, if the modulation scheme uses $4$ waveforms, then
  $\rsym=2$ bits/symbol and so the code rate $\rcode$ can be
  up to $1$ (almost no coding redundancy is needed).


  \item If there is the transmitted energy ($P=\sigma^2$),
        then the capacity of the channel to pass information is
    \begin{align*}
      \iC
        &= \frac{1}{2}\log_2\left( 1 + \frac{\sigma^2}{\sigma^2}\right)
      \\&= \frac{1}{2}\log_2\left( 1 + 1\right)
      \\&= \frac{1}{2}
    \end{align*}
  That is, each symbol can carry just under $1/2$ bits of information.
  This means
  \begin{align*}
    \frac{1}{2}
      &= \iC
       >  \rchan
       \eqd \frac{\mbox{information bits}}{\mbox{symbol}}
       = \frac{\mbox{information bits}}{\mbox{coded bits}} \times
         \frac{\mbox{coded bits}}{\mbox{symbol}}
       = \rcode \rsym
  \end{align*}
  This means that if the coding rate is $\rcode=1/4$,
  then we must use a modulation with $4$ ($\rsym=2$ bits/symbol)
  or fewer waveforms.

  Conversely, if the modulation scheme uses $16$ waveforms, then
  $\rsym=4$ bits/symbol and so the code rate $\rcode$ must be
  less than $1/8$.
\end{enumerate}
\end{example}



%======================================
\subsection{ML estimate}
%======================================
%--------------------------------------
\begin{theorem}
\label{thm:estML_QAM}
%--------------------------------------
In an AWGN channel with received signal $\fr(t)=s(t;\phi)+n(t)$
Let
\begin{liste}
   \item $\fr(t)=s(t;\phi)+n(t)$ be the received signal in an AWGN channel
   \item $n(t)$ a Gaussian white noise process
   \item $\fs(t;\phi)$ the transmitted signal such that
       \[s(t;\phi) = \sum_n a_n \sym(t-nT)\cos(2\pi f_ct+\theta_n+\phi).\]
\end{liste}

Then the optimal ML estimate of $\phi$ is either of the two equivalent
expressions
\thmbox{\begin{array}{rcl}
   \estML[\phi]
     &=&  -\atan\left[
         \frac{\sum_n a_n \int_t \fr(t)\sym(t-nT)\sin(2\pi f_c t+\theta_n)\dt}
              {\sum_n a_n \int_t \fr(t)\sym(t-nT)\cos(2\pi f_c t+\theta_n)\dt}
         \right]
\\ \\
     &=&  \arg_\phi\left(
            \sum_n a_n \int_t \fr(t)\left[\sym(t-nT)\sin(2\pi f_ct+\theta_n+\phi)\right] \dt = 0
            \right).
\end{array}
}
\end{theorem}
\begin{proof}

\begin{align*}
   \estML[\phi]
     &=    \arg_\phi\left(
            2\int_t \fr(t)\left[\pderiv{}{\phi}s(t;\phi)\right] \dt =
            \pderiv{}{\phi}\int_t s^2(t;\phi) \dt
            \right)
            \hspace{1cm}\mbox{by Theorem~\ref{thm:estML_general} page~\pageref{thm:estML_general}}
   \\&=    \arg_\phi\left(
            2\int_t \fr(t)\left[\pderiv{}{\phi}s(t;\phi)\right] \dt =
            \pderiv{}{\phi}\norm{s(t;\phi)}^2 \dt
            \right)
   \\&=    \arg_\phi\left(
            2\int_t \fr(t)\left[\pderiv{}{\phi}s(t;\phi)\right] \dt = 0
            \right)
   \\&=    \arg_\phi\left(
            \int_t \fr(t)\left[\pderiv{}{\phi}\sum_n a_n \sym(t-nT)\cos(2\pi f_ct+\theta_n+\phi)\right] \dt = 0
            \right)
   \\&=    \arg_\phi\left(
            -\sum_n a_n \int_t \fr(t)\left[\sym(t-nT)\sin(2\pi f_ct+\theta_n+\phi)\right] \dt = 0
            \right)
   \\&=    \arg_\phi\left(
            \sum_n a_n \int_t \fr(t)\sym(t-nT)\left[
            \sin(2\pi f_ct+\theta_n)\cos(\phi) +
            \sin(\phi)\cos(2\pi f_ct+\theta_n)
            \right] \dt = 0
            \right)
   \\&=    \arg_\phi\left(
            \sum_n a_n \int_t \fr(t)\sym(t-nT)
            \sin(\phi)\cos(2\pi f_ct+\theta_n) \dt
            = -
            \sum_n a_n \int_t \fr(t)\sym(t-nT)
            \sin(2\pi f_ct+\theta_n)\cos(\phi) \dt
            \right)
   \\&=    \arg_\phi\left(
            \sin(\phi)\sum_n a_n \int_t \fr(t)\sym(t-nT)
            \cos(2\pi f_ct+\theta_n) \dt
            = -
            \cos(\phi)\sum_n a_n \int_t \fr(t)\sym(t-nT)
            \sin(2\pi f_ct+\theta_n) \dt
            \right)
   \\&=    \arg_\phi\left(
            \frac{\sin(\phi)}{\cos(\phi)}
            =-
            \frac{\sum_n a_n \int_t \fr(t)\sym(t-nT)\sin(2\pi f_ct+\theta_n) \dt}
                 {\sum_n a_n \int_t \fr(t)\sym(t-nT)\cos(2\pi f_ct+\theta_n) \dt}
            \right)
   \\&=    \arg_\phi\left(
            \tan(\phi)
            =-
            \frac{\sum_n a_n \int_t \fr(t)\sym(t-nT)\sin(2\pi f_ct+\theta_n) \dt}
                 {\sum_n a_n \int_t \fr(t)\sym(t-nT)\cos(2\pi f_ct+\theta_n) \dt}
            \right)
   \\&=    \arg_\phi\left(
            \phi =-\atan\left(
            \frac{\sum_n a_n \int_t \fr(t)\sym(t-nT)\sin(2\pi f_ct+\theta_n) \dt}
                 {\sum_n a_n \int_t \fr(t)\sym(t-nT)\cos(2\pi f_ct+\theta_n) \dt}
            \right)
            \right)
   \\&=    -\atan\left(
            \frac{\sum_n a_n \int_t \fr(t)\sym(t-nT)\sin(2\pi f_ct+\theta_n) \dt}
                 {\sum_n a_n \int_t \fr(t)\sym(t-nT)\cos(2\pi f_ct+\theta_n) \dt}
            \right)
\end{align*}
\end{proof}




%--------------------------------------
\section{Estimation techniques}
%--------------------------------------
\begin{figure}[ht]
\begin{center}
\begin{fsK}
\setlength{\unitlength}{0.15mm}
\begin{picture}(1000,400)(-150,-300)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put( 325,  50){\framebox(100, 50)[c]{techniques}          }

  \put(  50,   0){\line    (  1,  0)   {750}                 }
 %\put( 375,   0){\circle*             { 10}                 }

  \put(-100,-225){\line    (  1,  0)   {300}                 }
  \put(  50,-225){\circle*             {  5}                 }
  \put(-100,-250){\line    (  0,  1)   { 25}                 }
  \put(  50,-250){\line    (  0,  1)   {150}                 }
  \put( 200,-250){\line    (  0,  1)   { 25}                 }

  \put(  50,- 50){\line    (  0,  1)   { 50}                 }
  \put( 200,- 50){\line    (  0,  1)   { 50}                 }
  \put( 350,- 50){\line    (  0,  1)   { 25}                 }
  \put( 500,- 50){\line    (  0,  1)   { 25}                 }
  \put( 425,- 25){\line    (  0,  1)   { 25}                 }
 %\put( 425,- 25){\circle*             { 10}                 }
  \put( 350,- 25){\line    (  1,  0)   {150}                 }
  \put( 650,- 50){\line    (  0,  1)   { 50}                 }
  \put( 800,- 50){\line    (  0,  1)   { 50}                 }
  \put( 375,   0){\line    (  0,  1)   { 50}                 }

  \put(  50,-150){\line    (  0,  1)   { 50}                 }
  \put( 200,-150){\line    (  0,  1)   { 50}                 }
  \put( 350,-150){\line    (  0,  1)   { 50}                 }
  \put( 500,-150){\line    (  0,  1)   { 50}                 }

  \put( 500,-250){\line    (  0,  1)   { 50}                 }

  \put(  60,- 45){\makebox (100, 40)[lt]{sequential}         }
  \put(  60,- 45){\makebox (100, 40)[lb]{decoding}           }
  \put( 210,- 45){\makebox (100, 35)[lt]{norm}       }
  \put( 210,- 45){\makebox (100, 35)[lb]{minimization}       }
  \put( 435,- 25){\makebox (100, 25)[l]{gradient search}     }
  \put( 360,- 50){\makebox (100, 25)[l]{use $\grad_\vp$ only}}
  \put( 510,- 50){\makebox (100, 25)[l]{$\grad_\vp,\grad^2_\vp$}}
  \put( 660,- 50){\makebox (100, 50)[l]{inner product}       }
  \put( 810,- 50){\makebox (100, 50)[l]{direct search}       }

  \put(   0,-100){\framebox(100, 50)[c]{}                    }
  \put(   0,- 90){\makebox (100, 30)[t]{code}                }
  \put(   0,- 90){\makebox (100, 30)[b]{tree}                }
  \put(   0,-300){\framebox(100, 50)[c]{}                    }

  \put(-150,-295){\makebox (100, 40)[t]{trellis}             }
  \put(-150,-295){\makebox (100, 40)[b]{(Viterbi)}           }
  \put(-150,-300){\framebox(100, 50)[c]{}                    }
  \put(   0,-295){\makebox (100, 40)[t]{Fano}                }
  \put(   0,-295){\makebox (100, 40)[b]{algorithm}           }
  \put(   0,-300){\framebox(100, 50)[c]{}                    }
  \put( 150,-295){\makebox (100, 40)[t]{Stack}               }
  \put( 150,-295){\makebox (100, 40)[b]{algorithm}           }
  \put( 150,-300){\framebox(100, 50)[c]{}                    }

  \put( 150,-100){\framebox(100, 50)[c]{}                    }
  \put( 150,- 90){\makebox (100, 30)[t]{min. mean}           }
  \put( 150,- 90){\makebox (100, 30)[b]{squares}             }
  \put( 150,-200){\framebox(100, 50)[c]{}                    }
  \put( 150,-190){\makebox (100, 30)[t]{least}               }
  \put( 150,-190){\makebox (100, 30)[b]{square}             }

  \put( 300,-100){\framebox(100, 50)[c]{}                    }
  \put( 300,- 90){\makebox (100, 30)[t]{steepest}            }
  \put( 300,- 90){\makebox (100, 30)[b]{descent}             }
  \put( 300,-200){\framebox(100, 50)[c]{}                    }
  \put( 300,-190){\makebox (100, 30)[t]{least mean}          }
  \put( 300,-190){\makebox (100, 30)[b]{square}             }

  \put( 450,-100){\framebox(100, 50)[c]{}                    }
  \put( 450,- 90){\makebox (100, 30)[t]{Newton}              }
  \put( 450,- 90){\makebox (100, 30)[b]{method}              }
  \put( 450,-200){\framebox(100, 50)[c]{Kalman}              }
  \put( 450,-190){\makebox (100, 30)[t]{}                    }
  \put( 450,-190){\makebox (100, 30)[b]{}                    }
  \put( 450,-295){\makebox (100, 40)[t]{recursive}           }
  \put( 450,-300){\framebox(100, 50)[c]{least}               }
  \put( 450,-295){\makebox (100, 40)[b]{square}             }

  \put( 600,-100){\framebox(100, 50)[c]{wavelets}            }
  \put( 600,- 90){\makebox (100, 30)[t]{}                    }
  \put( 600,- 90){\makebox (100, 30)[b]{}                    }

\end{picture}
\end{fsK}
\end{center}
\caption{
   Estimation techniques
   \label{fig:est-tech}
   }
\end{figure}


%\begin{figure}[ht]
%\center{\epsfig{file=estimate.eps, width=12cm, clip=}}
%%\center{\includegraphics[0,0][4in,4in]{df1.eps}}
%\caption{
%   Estimation techniques
%   \label{fig:estimate}
%   }
%\end{figure}


%--------------------------------------
\subsection{Sequential decoding}
%--------------------------------------
\begin{figure}[ht]
\begin{center}
\begin{fsL}
\setlength{\unitlength}{0.15mm}
\begin{picture}(550,320)(-100,0)
  %\graphpaper[10](0,0)(300,300)
  \put( -10 , 300 ){\makebox(0,0)[r]{state $00$}}
  \put( -10 , 200 ){\makebox(0,0)[r]{state $01$}}
  \put( -10 , 100 ){\makebox(0,0)[r]{state $10$}}
  \put( -10 ,   0 ){\makebox(0,0)[r]{state $11$}}

  \thicklines
  \put(   0 ,   0 ){\circle*{10}}
  \put(   0 , 100 ){\circle*{10}}
  \put(   0 , 200 ){\circle*{10}}
  \put(   0 , 300 ){\circle*{10}}

\multiput(0,0)(100,0){5}{
  %\thicklines
  \linethickness{1mm}
  \put        (  0,300){\line( 1,-1){100}} % state0 path1
  \put        (  0,200){\line( 1,-1){100}} % state1 path1
  \put        (  0,100){\line( 1, 1){100}} % state2 path1
  \put        (  0,  0){\line( 1, 1){100}} % state3 path1

  %\thicklines
  \linethickness{0.1mm}
  %\put        (  0,300){\line( 1,-3){100}} % state0 path0
  %\put        (  0,200){\line( 1,-2){100}} % state1 path0
  %\put        (  0,100){\line( 1, 2){100}} % state2 path0
  %\put        (  0,  0){\line( 1, 3){100}} % state3 path0

  \qbezier[50](  0,300)(  0,300)(100,  0)  % state0 path0
  \qbezier[50](  0,200)(  0,200)(100,  0)  % state1 path0
  \qbezier[50](  0,100)(  0,100)(100,300)  % state2 path0
  \qbezier[50](  0,  0)(  0,  0)(100,300)  % state3 path0

  \put( 100 ,   0 ){\circle*{10}}
  \put( 100 , 100 ){\circle*{10}}
  \put( 100 , 200 ){\circle*{10}}
  \put( 100 , 300 ){\circle*{10}}
}
\end{picture}
\end{fsL}
\hspace{1cm}
\begin{tabular}{cl}
   $\cdots$ & $y_n=0$ \\
  ---     & $y_n=1$
\end{tabular}
\caption{
  Viterbi algorithm trellis
   \label{fig:est_trellis}
   }
\end{center}
\end{figure}

It has been shown that the Viterbi algorithm (trellis) produces
an optimal estimate in the maximal likelihood (ML) sense.
A Verterbi trellis is shown in Figure~\ref{fig:est_trellis}.





%======================================
\subsection{Norm minimization}
%======================================


\begin{figure}[ht]
\begin{center}
\begin{fsK}
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(-100,  10){\makebox ( 50,  0)[b]{$x$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{operator} }
  \put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vp$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn$}           }

  \put( 100,  85){\makebox ( 50,  0)[b]{$y$}               }
  \put( 100,  75){\line    (  1,  0)   { 50}               }
  %\put( 100,- 65){\makebox ( 50,  0)[b]{$U\estn$}            }
  \put( 100,- 75){\line    (  1,  0)   { 50}               }
  \put( 150,  75){\line    (  0, -1)   { 50}               }
  \put( 150,- 75){\line    (  0,  1)   { 50}               }
  \put( 150,  25){\vector  (  1,  0)   { 50}               }
  \put( 150, -25){\vector  (  1,  0)   { 50}               }

  \put( 200,- 50){\framebox(100,100)[c]{cost}    }
  \put( 300,  10){\makebox (100,  0)[b]{$C(\estn,x,y)$}      }
  \put( 300,   0){\vector  (  1,  0)   {100}               }
  \put( 400,- 50){\framebox(100,100)[c]{$\grad_\vp$}        }
  \put( 510,  10){\makebox (100,  0)[lb]{$\grad_\vp C(\vp,x,y)$}      }
  \put( 500,   0){\vector  (  1,  0)   {100}               }

\end{picture}
\end{fsK}
\end{center}
\caption{
   Estimation using gradient of cost function
   \label{fig:est-grad}
   }
\end{figure}




Norm minimization techniques are very powerful
in that an optimum solution can be computed
in one step without iteration or recursion.
In this section we present two types of norm minimization:
\footnote{
   The Least Squares algorithm is nothing new to mathematics.
   It was first developed in 1795 by Gauss who was also the first
   to discover the FFT.
   }

\begin{enume}
  \item minimum mean square estimation (MMSE): \\
        The MMS estimate is a \hie{stochastic} estimate.
        To compute the MMS estimate, the we do not need to know
        the actual data values, but we must know certain system statistics
        which are the
        input data autocorrelation and input/output crosscorrelation.
        The cost function is the expected value of the norm squared error.
   \item least square estimation (LSE): \\
        The LS estimate is a \hie{deterministic} estimate.
        To compute the LS estimate, we must know the actual data values
        (although these may be ``noisy" measurements).
        The cost function is the norm squared error.
\end{enume}

Solutions to both are given in terms of two matrices:

\begin{tabular}{lll}
   $\setR$: Autocorrelation matrix \\
   $W$: Crosscorrelation matrix.
\end{tabular}

%--------------------------------------
\subsubsection{Minimum mean square estimation}
\label{sec:est_mms}
%--------------------------------------
%--------------------------------------
\begin{definition}
\label{def:est_matrices}
%--------------------------------------
Let the following vectors, matrices, and functions
be defined as follows:

\defbox{\begin{tabular}{lcll}
   $\vx  $&$\in$&$ \vCm        $  & data vector                       \\
   $\vy  $&$\in$&$ \vCn        $  & processed data vector             \\
   $\vye $&$\in$&$ \vCn        $  & processed data estimate vector    \\
   $\ve  $&$\in$&$ \vCn        $  & error vector                     \\
   $\vp  $&$\in$&$ \vRm        $  & parameter vector                 \\
   $U    $&$\in$&$ \mCmn       $  & regression matrix                \\
   $R    $&$\in$&$ \mCmm       $  & autocorrelation matrix           \\
   $W    $&$\in$&$ \vCm        $  & cross-correlation vector         \\
   $C    $&$:  $&$ \vRm\to\Rnn $  & cost function
\end{tabular}}
\end{definition}

%--------------------------------------
\begin{theorem}[\thmd{Minimum mean square estimation}]
\label{thm:est_mms}
%--------------------------------------
Let
\begin{align*}
   \vye(\vp)   &\eqd U^H\vp    \\
   \ve(\vp)    &\eqd \vye-\vy \\
   \fCost(\vp) &\eqd \E\norm{\ve}^2 \eqd \Eb{\ve^H\ve} \\
   \estMS      &\eqd \arg\min_\vp \fCost(\vp)  \\
   R           &\eqd \Eb{UU^H}   \\
   W           &\eqd \Eb{ U\vy}.
\end{align*}

Then
\thmbox{\begin{array}{rcl}
   \estMS                          &=& (\Re{R})^{-1}(\Re{W})  \\
   \fCost(\vp)                     &=& \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \E{\vy^H\vy} \\
   \grad_\vp \fCost(\vp)           &=& 2\Reb{R}\vp - 2\Re{W}  \\
   \fCost(\estMS)                  &=& (\Re{W^H})(\Re{R})^{-1} R (\Re{R})^{-1}(\Re{W}) - 2(\Re{W^H})(\Re{R})^{-1}(\Re{W}) + \E{\vy^H\vy} \\
   \fCost(\estMS)|_{R\mbox{ real}} &=& \E{\vy^H\vy} - (\Re{W^H})R^{-1}(\Re{W}).
\end{array}}
\end{theorem}
\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{align*}
   \fCost(\vp)
     &\eqd \E \norm{\ve}^2
   \\&=    \Eb{\ve^H\ve}
   \\&=    \Eb{ \left(\vye-\vy\right)^H\left(\vye-\vy\right) }
   \\&=    \Eb{ \left(U^H\vp-\vy\right)^H\left(U^H\vp-\vy\right) }
   \\&=    \Eb{ \left(\vp^HU-\vy^H\right)\left(U^H\vp-\vy\right) }
   \\&=    \Eb{ \vp^HUU^H\vp - \vp^HU\vy -\vy^HU^H\vp + \vy^H\vy }
   \\&=    \vp^H\Eb{UU^H}\vp - \vp^H\Eb{U\vy} -\Eb{\vy^HU^H}\vp + \E{\vy^H\vy}
   \\&=    \vp^H\Eb{UU^H}\vp - (\Eb{U\vy}^H\vp)^H -\Eb{U\vy}^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - (W^H\vp)^H -W^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \E{\vy^H\vy}
   \\&=    \vp^H R \vp - 2\Reb{W^H}\vp + \E{\vy^H\vy}
\\
\\
   \grad_\vp \fCost(\vp)
     &= \grad_\vp \left[ \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \E{\vy^H\vy} \right]
   \\&= R\vp + R^T\vp - [(W^H)^\ast]^T - [W^H]^T + 0
   \\&= R\vp + (R^H)^\ast\vp - W - W^\ast
   \\&= R\vp + R^\ast\vp - W - W^\ast
   \\&= (R + R^\ast)\vp - (W + W^\ast)
   \\&= 2(\Re{R})\vp - 2\Re{W}
\\
\\
   \vpo
     &= (\Re{R})^{-1}(\Re{W})
\\
\\
   \fCost(\vpo)
     &=    \vpo^H R \vpo - 2\Reb{W^H}\vpo + \E{\vy^H\vy}
   \\&=    [(\Re{R})^{-1}(\Re{W})]^H R [(\Re{R})^{-1}(\Re{W})] - 2\Reb{W^H}[(\Re{R})^{-1}(\Re{W})] + \E{\vy^H\vy}
   \\&=    (\Re{W^H})(\Re{R})^{-H} R (\Re{R})^{-1}(\Re{W}) - 2\Reb{W^H}(\Re{R})^{-1}(\Re{W}) + \E{\vy^H\vy}
   \\&=    (\Re{W^H})(\Re{R^H})^{-1} R (\Re{R})^{-1}(\Re{W}) - 2\Reb{W^H}(\Re{R})^{-1}(\Re{W}) + \E{\vy^H\vy}
   \\&=    (\Re{W^H})(\Re{R})^{-1} R (\Re{R})^{-1}(\Re{W}) - 2(\Re{W^H})(\Re{R})^{-1}(\Re{W}) + \E{\vy^H\vy}
\\
\\
   \fCost(\vpo)|_{R\mbox{ real}}
     &=    (\Re{W^H})(\Re{R})^{-1} R (\Re{R})^{-1}(\Re{W}) - 2(\Re{W^H})(\Re{R})^{-1}(\Re{W}) + \E{\vy^H\vy}
   \\&=    (\Re{W^H})R^{-1} R R^{-1}(\Re{W}) - 2(\Re{W^H})R^{-1}(\Re{W}) + \E{\vy^H\vy}
   \\&=    (\Re{W^H}) R^{-1}(\Re{W}) - 2(\Re{W^H})R^{-1}(\Re{W}) + \E{\vy^H\vy}
   \\&=    \E{\vy^H\vy} - (\Re{W^H})R^{-1}(\Re{W})
\end{align*}
\end{proof}




\begin{figure}[ht]
\begin{center}
\begin{fsK}
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  \put(-100,  10){\makebox ( 50,  0)[b]{$\vx$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{process} }
  %\put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vp$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn=R^{-1}W$}           }

  \put( 110,  85){\makebox ( 50,  0)[lb]{$y$}               }
  \put( 110, -65){\makebox ( 50,  0)[lb]{$\hat{y}=\vx^T\vp$}               }
  \put( 100,  75){\vector  (  1,  0)   {150}               }
  \put( 100,- 75){\vector  (  1,  0)   {150}               }
  \put( 250,   0){\circle{40} }
  \put( 250,   0){\makebox(0,0)[c]{$+$} }
  \put( 250,  75){\vector  (  0, -1)   { 60}               }
  \put( 250, -75){\vector  (  0,  1)   { 60}               }

  \put( 300,  10){\makebox (100,  0)[b]{$e=\hat{y}-y$}      }
  \put( 270,   0){\vector  (  1,  0)   {80}               }
  \put( 350,   0){\vector  (  0, -1)   {225}               }
  \put( 350,-225){\vector  ( -1,  0)   {250}               }
\end{picture}
\end{fsK}
\end{center}
\caption{
   Adative filter example
   \label{fig:est_adapt}
   }
\end{figure}




%--------------------------------------
\begin{example}[Adaptive filter]
%--------------------------------------
In many adaptive filter and equalization applications,
the autocorrelation matrix $U$ is simply the $m$-element
random data vector $\vx(k)$ at time $k$ such that
\[
   U \eqd \vx(k) \eqd
   \left[\begin{array}{l}
      x(k) \\
      x(k-1) \\
      x(k-2) \\
      \vdots \\
      x(k-m+1)
   \end{array}\right].
\]

This is a special case of the more general case discussed
in Theorem~\ref{thm:est_mms}.
Here, the dimension of $U$ is $m\times1$ (n=1).
As a result,
$\vy$, $\vye$, and $\ve$ are simply scalar quantities (not vectors).
In this special case, we have the following results
\xref{fig:est_adapt}:

\begin{align*}
   \hat{y}(\vp)   &\eqd& \vx^T \vp    \\
   e(\vp)    &\eqd& \hat{y}-y \\
   \fCost(\vp) &\eqd& \E\norm{\ve}^2 \eqd \Eb{e^2} \\
   \estMS      &\eqd& \arg\min_\vp \fCost(\vp)  \\
   R           &\eqd& \Eb{\vx\vx^T}   \\
   W           &\eqd& \Eb{ \vx y }.
\end{align*}

\exbox{\begin{array}{rcl}
   \estMS                          &=& R^{-1}W  \\
   \fCost(\vp)                     &=& \vp^T R \vp -2W^T\vp  + \Eb{\vy^T\vy} \\
   \grad_\vp \fCost(\vp)           &=& 2R\vp - 2W  \\
   \fCost(\estMS)                  &=& W^T R^{-1} R R^{-1}W - 2W^T R^{-1}W + \E{\vy^T\vy} \\
   \fCost(\estMS)|_{R\mbox{ real}} &=&    \E{\vy^T\vy} - W^T R^{-1}W.
\end{array}}
\end{example}


%======================================
\subsubsection{Least squares}
\label{sec:ls}
%======================================
%--------------------------------------
\begin{theorem}[\thmd{Least squares}]
\label{thm:ls}
\index{least squares}
%--------------------------------------
Let
\begin{align*}
   \vye(\vp)   &\eqd U^H\vp                      \\
   \ve(\vp)    &\eqd \vye-\vy                    \\
   \fCost(\vp) &\eqd \norm{\ve}^2 \eqd \ve^H\ve  \\
   \estLS      &\eqd \arg\min_\vp \fCost(\vp)    \\
   R           &\eqd UU^H                        \\
   W           &\eqd U\vy.
\end{align*}

Then
\thmbox{\begin{array}{rcl}
   \estLS                          &=& (\Re{R})^{-1}(\Re{W})  \\
   \fCost(\vp)                     &=& \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \E{\vy^H\vy} \\
   \grad_\vp \fCost(\vp)           &=& 2\Reb{R}\vp - 2\Re{W}  \\
   \fCost(\estLS)                  &=& (\Re{W^H})(\Re{R})^{-1} R (\Re{R})^{-1}(\Re{W}) - 2(\Re{W^H})(\Re{R})^{-1}(\Re{W}) + \E{\vy^H\vy} \\
   \fCost(\estLS)|_{R\mbox{ real}} &=& \E{\vy^H\vy} - (\Re{W^H})R^{-1}(\Re{W}).
\end{array}}
\end{theorem}

\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{align*}
   \fCost(\vp)
     &\eqd \norm{\ve}^2
   \\&=    e^H\ve
   \\&=    \left(\vye-\vy\right)^H\left(\vye-\vy\right)
   \\&=    \left(U^H\vp-\vy\right)^H\left(U^H\vp-\vy\right)
   \\&=    \left(\vp^HU-\vy^H\right)\left(U^H\vp-\vy\right)
   \\&=    \vp^HUU^H\vp - \vp^HU\vy -\vy^HU^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - (W^H\vp)^H -W^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - (W^H\vp)^\ast -W^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \vy^H\vy
   \\&=    \vp^H R \vp - 2\Reb{W^H}\vp + \vy^H\vy
\\
\\
   \grad_\vp \fCost(\vp)
     &= \grad_\vp \left[ \vp^H R \vp - (W^H)^\ast\vp -W^H\vp + \vy^H\vy \right]
   \\&= R\vp + R^T\vp - [(W^H)^\ast]^T - [W^H]^T + 0
   \\&= R\vp + (R^H)^\ast\vp - W - W^\ast
   \\&= R\vp + R^\ast\vp - W - W^\ast
   \\&= (R + R^\ast)\vp - (W + W^\ast)
   \\&= 2(\Re{R})\vp - 2\Re{W}
\\
\\
   \vpo
     &= (\Re{R})^{-1}(\Re{W})
\\
\\
   \fCost(\vpo)
     &=    \vpo^H R \vpo - 2\Reb{W^H}\vpo + \vy^H\vy
   \\&=    [(\Re{R})^{-1}(\Re{W})]^H R [(\Re{R})^{-1}(\Re{W})] - 2\Reb{W^H}[(\Re{R})^{-1}(\Re{W})] + \vy^H\vy
   \\&=    (\Re{W^H})(\Re{R})^{-H} R (\Re{R})^{-1}(\Re{W})     - 2\Reb{W^H}(\Re{R})^{-1}(\Re{W})       + \vy^H\vy
   \\&=    (\Re{W^H})(\Re{R^H})^{-1} R (\Re{R})^{-1}(\Re{W})   - 2\Reb{W^H}(\Re{R})^{-1}(\Re{W})     + \vy^H\vy
   \\&=    (\Re{W^H})(\Re{R})^{-1} R (\Re{R})^{-1}(\Re{W})     - 2(\Re{W^H})(\Re{R})^{-1}(\Re{W})      + \vy^H\vy
\\
\\
   \fCost(\vpo)|_{R\mbox{ real}}
     &=    (\Re{W^H})(\Re{R})^{-1} R (\Re{R})^{-1}(\Re{W}) - 2(\Re{W^H})(\Re{R})^{-1}(\Re{W}) + \vy^H\vy
   \\&=    (\Re{W^H})R^{-1} R R^{-1}(\Re{W}) - 2(\Re{W^H})R^{-1}(\Re{W}) + \vy^H\vy
   \\&=    (\Re{W^H}) R^{-1}(\Re{W}) - 2(\Re{W^H})R^{-1}(\Re{W}) + \vy^H\vy
   \\&=    \vy^H\vy - (\Re{W^H})R^{-1}(\Re{W})
\end{align*}
\end{proof}


%---------------------------------------
\begin{example}[Polynomial approximation]
\index{polynomial approximation}
\index{least squares}
\index{Vandermonde matrix}
%---------------------------------------
\hspace{1pt}\\
Suppose we {\bf know} the locations
$\set{(x_n,y_n)}{n=1,2,3,4,5}$ of 5 data points.
Let $\vx$ and $\vy$ represent the locations of these points such that
\[
   \vx \eqd
   \left[\begin{array}{l}
      x_1  \\
      x_2  \\
      x_3  \\
      x_4  \\
      x_5
   \end{array}\right]
   \qquad\qquad
   \vy \eqd
   \left[\begin{array}{l}
      y_1  \\
      y_2  \\
      y_3  \\
      y_4  \\
      y_5
   \end{array}\right]
\]
Suppose we want to find a second order polynomial
  \[ c x^2 + bx + a \]
that best approximates
these 5 points in the least squares sense.
We define the matrix $U$ (known) and vector $\estn$ (to be computed)
as follows:
\[
   U^H \eqd
   \mcom{
   \left[\begin{array}{lll}
      1  & x_1 & x_1^2  \\
      1  & x_2 & x_2^2  \\
      1  & x_3 & x_3^2  \\
      1  & x_4 & x_4^2  \\
      1  & x_5 & x_5^2
   \end{array}\right]
   }{Vandermonde matrix \footnotemark}
   \addtocounter{footnote}{-1}\citetbl{\citer{horn}{29}}
   \qquad\qquad
   \estn \eqd
   \left[\begin{array}{l}
      a  \\
      b  \\
      c  \\
   \end{array}\right]
\]
Then, using \prefpp{thm:ls}, the best coefficients $\estn$
for the polynomial are
\begin{align*}
  \estn
    &= \left[\begin{array}{l}
          a  \\
          b  \\
          c  \\
       \end{array}\right]
  \\&= R^{-1}W
  \\&= (UU^H)^{-1}\; (U\vy)
  \\&= \left(
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]^H
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]
       \right)^{-1}
       \left(
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]^H
       \left[\begin{array}{l}
          y_1  \\
          y_2  \\
          y_3  \\
          y_4  \\
          y_5
       \end{array}\right]
       \right)
\end{align*}
\end{example}

%======================================
\subsection{Gradient search techniques}
%======================================
One of the biggest advantages of using a gradient search technique is
that they can be implemented \hie{recursively} as shown in the next equation.
The general form of the gradient search parameter estimation techniques is\footnote{\citerp{nelles2001}{90}}
\thmbox{
   \vp_n = \vp_{n-1} - \eta_{n-1} R \;\left[\gradxy{\vp}{\fCost(\vp_n)}\right]
}
where at time $n$

\begin{tabular}{lll}
   $\vp_n$      & is the \hie{state    }   & (vector)  \\
   $\eta_n$     & is the \hie{step size}   & (scalar)  \\
   $\setR$          & is the \hie{direction}   & (matrix)  \\
   $\gradxy{\vp}{\fCost(\vp_n)}$      & is the \hie{gradient } of the cost function $\fCost(\vp_n)$   & (vector)
\end{tabular}

Two major categories of gradient search techniques are
\begin{liste}
   \item steepest descent (includes LMS)
   \item Newton's method (includes RLS and Kalman filters).
\end{liste}

The key difference between the two is that
{\bf \hie{steepest descent} uses only first derivative information},
while
{\bf \hie{Newton's method} uses both first and second derivative information}
making it converge much faster but with significantly higher
complexity.

%======================================
\subsubsection*{First derivative techniques}
\label{sec:1st-deriv}
%======================================
\paragraph{Steepest descent.}
In this algorithm, $R=I$ (identity matrix).
First derivative information is contained in $\grad\fCost$.
Second derivative information, if present, is contained in $\setR$.
Thus, steepest descent algorithms do not use second derivative information.
\thmbox{
  \vp_n = \vp_{n-1} - \eta_{n-1} \;\left[ \gradxy{\vp}{\fCost(\vp_n)} \right]
}
\paragraph{Least Mean Squares (LMS).}\citetbl{\citerp{mik}{526}}
This is a special case of \hie{steepest descent}.
In minimum mean square estimation (Section~\ref{sec:est_mms}),
the cost function $\fCost(\vp)$ is defined as a
\hie{statistical average} of the error vector such that
$\fCost(\vp) = \Eb{\ve^H\ve}$.
In this case the gradient $\grad\fCost$ is difficult to compute.
However, the LMS algorithm greatly simplifies the problem by
instead defining the cost function as a function of the
\hie{instantaneous error} such that
\begin{align*}
   \vy &= y(n)
\\
   \vye &= \hat{y}(n)
\\
   \fCost(\vp)
   &= \norm{e(n)}^2
 \\&= e^2(n)
 \\&= (\hat{y}(n)-y(n))^2
\end{align*}

Computing the gradient of this cost function is then
just a special case of \hie{least squares estimation} (Section~\ref{sec:ls}).
Using LS, we let $U=\vx^T$ and hence
\begin{align*}
   \gradxy{\vp}{\fCost(\vp)}
   &= 2U^TU \vp -2U^T\vy                   && \text{ by \prefp{thm:ls}}
\\ &= 2\vx\vx^T \vp -2\vx y               && \text{ by above definitions}
\\ &= 2\vx\hat{y} -2\vx y                    && \text{ }
\\ &= 2\vx(\hat{y} -y)                      && \text{ }
\\ &= 2\vx e(n)                && \text{ }
\end{align*}

The LMS algorithm uses this instantaneous gradient for $\grad\fCost$,
lets $R=I$, and uses a constant step size $\eta$ to give
\thmbox{
  \vp_n = \vp_{n-1} - 2\eta \vx_n e(n)
}
%--------------------------------------
\subsubsection*{Second derivative techniques}
%--------------------------------------
\paragraph{Newton's Method.}
This algorithm uses the \hie{Hessian} matrix $H$,
which is the second derivative of the cost function $\fCost(\vp)$,
and lets $R=H^{-1}$.
\begin{align*}
   H_n &\eqd& \grad_\vp\grad_\vp \fCost(\vp_n)
\\
   \vp_n &= \vp_{n-1} - \eta_{n-1} H_n^{-1} \left[\grad_\vp\fCost(\vp_n)\right]
\end{align*}


\paragraph{Kalman filtering}\footnote{\citerp{nelles2001}{66}}
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}

\if 0
\paragraph{RLS with forgetting}\citetbl{\citerp{nelles2001}{64}}
This algorithm introduces a forgetting factor $\lambda$
to help the algorithm track non-stationary channels.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+\lambda}  P(k-1)x(k) \\
   P(k) &= \frac{1}{\lambda}(I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}
\fi

\paragraph{Recursive Least Squares (RLS)}\citetbl{\citerp{nelles2001}{66}}
This algorithm is a special case of either the RLS with forgetting
or the Kalman filter.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}  P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1) \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}




%--------------------------------------
\subsection{Direct search}
%--------------------------------------
A direct search algorithm may be used in cases where the cost
function over $\vp$ has several local minimums, making convergence difficult.
Furthermore, direct search algorithms can be very computationally demanding.

