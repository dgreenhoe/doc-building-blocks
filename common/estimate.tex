%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================

%======================================
\chapter{Estimation Overview}
\label{app:est}
\label{chp:est}
%======================================
%======================================
\section{Estimation types}
%======================================
%--------------------------------------
\paragraph{Estimation model.}
%--------------------------------------
Starting in the 1980s and continuing into 2019, James V. Candy has proposed 
the \structd{model-based approach}{\footnotemark} 
to estimation. This approach partitions the task of estimation into three parts:
\footnotetext{
  \citerg{candy1985}{9780070097254},
  \citerg{candy1988}{9780070097513},
  \citeP{candy1992},
  \citerpg{candy2005}{5}{9780471732662},
  \citerpg{candy2019}{7}{9781119457763}.
  ``Dr. Candy received the 
  IEEE Distinguished Technical Achievement Award for the
  \emph{development of model-based signal processing in ocean acoustics}":
  \citerpg{rossing2015}{1234}{9781493907557}
  }
\begin{enumerate}
  \item The \structd{model}:\footnote{
          \citerppc{box1976}{173}{207}{Chapter 6 ``Model Identification"}
          }
        The presumed system architecture,
        ``the form of which is usually suggested by prior knowledge, 
        physical understanding or guesswork."\footnote{\citerp{scargle1979}{5}}
        A very common example is the \structe{additive noise model}.
  \item The \opd{algorithm}: 
        The operation used to calculate the estimate.
        An \ope{algorithm} may also be called a \opd{processor},
        \opd{filter}, or \opd{method}.
  \item The \fnctd{criterion function}: 
        A \fncte{function} which measures the performance
        of the algorithm.
        This function may also be called a \fnctd{cost function}. 
\end{enumerate}

%--------------------------------------
\paragraph{Estimation types.}
%--------------------------------------
Let $\rvx(t;\theta)$ be a waveform with parameter $\theta$.
There are three basic types of estimation of $\rvx$:

\begin{enume}
   \item \ope{detection}:
      \begin{liste}
         \item The waveform $\rvx(t;\theta_n)$ is known except for the value of parameter $\theta_n$.
         \item The parameter $\theta_n$ is one of a finite set of values.
         \item Estimate $\theta_n$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{parametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t;\theta)$ is known except for the value of parameter $\theta$.
         \item The parameter $\theta$ is one of an infinite set of values.
         \item Estimate $\theta$ and thereby also estimate $\rvx(t;\theta)$.
      \end{liste}
   \item \prope{nonparametric} estimation:
      \begin{liste}
         \item The waveform $\rvx(t)$ is unknown and assumed without any parameter $\theta$.
         \item Estimate $\rvx(t)$.
      \end{liste}
\end{enume}

%--------------------------------------
\paragraph{Estimation criterion.}
%--------------------------------------
Optimization requires a criterion against which the quality of an
estimate is measured.\footnote{\citergc{srv}{chapters 3, 5}{013125295X}.}
The most demanding and general criterion is the \prope{Bayesian} criterion.
The Bayesian criterion requires knowledge of the probability
distribution functions and the definition of a \fncte{cost function}.
Other criterion are special cases of the Bayesian criterion
such that the cost function is defined in a special way,
no cost function is defined, and/or the distribution is not known
\xref{fig:est-tech}.

%--------------------------------------
\paragraph{Estimation techniques.}
\label{ref:sec:parameter-est}
%--------------------------------------
Estimation techniques can be classified into
five groups \xref{fig:est-tech}:\footnote{%
  \citerpgc{nelles2001}{26}{3540673695}{``Fig 2.2 Overview of linear and nonlinear optimization techniques"},
  \citerpgc{nelles2001}{33}{3540673695}{``Fig 2.5 The Bayes method is the most general approach but\ldots"},
  \citerpgc{nelles2001}{63}{3540673695}{``Table 3.3 Relationship between linear recursive and nonlinear optimization techniques"},
  \citerpg{nelles2001}{66}{3540673695}
  }
\begin{enume}
   \item sequential decoding
   \item norm minimization
   \item gradient search
   \item inner product analysis
   \item direct search
\end{enume}

Sequential decoding is a non-linear estimation family.
Perhaps the most famous of these is the Veterbi algorithm which
uses a trellis to calculate the estimate.
The Verterbi algorithm has been shown to yield an optimal estimate
in the maximal likelihood (ML) sense.
Norm minimization and gradient search algorithms are all linear algorithms.
While this restriction to linear operations often simplifies calculations,
it often yields an estimate that is not optimal in the ML sense.

%=======================================
\section{Estimation criterion}
\label{sec:est_criterion}
%=======================================
\begin{figure}
\centering%
\includegraphics{graphics/latestimation.pdf}
\caption{
   Estimation criterion
   \label{fig:est-criterion}
   }
\end{figure}

%--------------------------------------
\begin{definition}
\index{MAP}
\index{ML}
\index{maximum a-posteriori}
\index{maximum likelihood}
\label{def:MAP}
\label{def:ML}
\label{def:estB}
\label{def:estMS}
\label{def:estMM}
\label{def:estMAP}
\label{def:estML}
%--------------------------------------
Let\\
$\begin{array}{FlM}
    (A).& \rvx(t;\theta)            & be a random process with unknown parameter $\theta$
  \\(B).& \rvy(t)                   & an observed random process which is statistically dependent on $\rvx(t;\theta)$
  \\(C).& \fCost(\theta,\pdfp(x,y)) & be a cost function.
\end{array}$
\\
Then the following \fnctd{estimate}s are defined as follows:
\defbox{\begin{array}{FMMlc>{\ds}l}
     (1).&\fnctd{Bayesian estimate}                         &                          & \estB   &\eqd& \argmin_{\theta} C(\theta,\pdfp(x,y))
   \\(2).&\fnctd{Mean square estimate}                      &(``\fnctd{MS  estimate}") & \estMS  &\eqd& \argmin_{\theta} \E\norm{C(\theta,\pdfp(x,y))}^2
   \\(3).&\fnctd{mini-max estimate}                         &(``\fnctd{MM  estimate}") & \estMM  &\eqd& \argmin_{\theta}\max_{\pdfp} C(\theta,\pdfp(x,y))
   \\(4).&\mc{2}{M}{\begin{tabular}[t]{@{}l}\fnctd{maximum a-posteriori probability estimate}\\
                                         (``\fnctd{MAP estimate}")
                    \end{tabular}}
         & \estMAP &\eqd& \argmax_{\theta} \psP\setn{\rvx(t;\theta)|\rvy(t)}
   \\(5).&\fnctd{maximum likelihood estimate}               &(``\fnctd{ML  estimate}") & \estML  &\eqd& \argmax_{\theta} \psP\setn{\rvy(t)|\rvx(t;\theta)}
\end{array}}
\end{definition}

%--------------------------------------
\begin{theorem}
\label{thm:map=ml}
%--------------------------------------
Let $\rvx(t;\theta)$ be a random process with unknown parameter $\theta$.
\thmbox{
  \brb{\text{$\psP\setn{\theta}=$\prope{constant}}}
  \quad\implies\quad
  \brb{\estMAP = \estML}
  }
\end{theorem}
\begin{proof}
\begin{align*}
   \estMAP
     &\eqd \argmax_{\theta} \psP\setn{\rvx(t;\theta)|\rvy(t)}
     &&    \text{by definition of $\estMAP$}
     &&    \text{\xref{def:estMAP}}
   \\&\eqd \argmax_{\theta} \frac{\psP\setn{\rvx(t;\theta) \land \rvy(t)}}
                               {\psP\setn{r(t)}}
     && \text{by definition of \fncte{conditional probability}}
     && \text{\xref{def:conprob}}
   \\&\eqd \argmax_{\theta} \frac{\psP\setn{r(t) | \rvx(t;\theta) }\psP\setn{\rvx(t;\theta) }}
                               {\psP\setn{\rvy(t)}}
     && \text{by definition of \fncte{conditional probability}}
     && \text{\xref{def:conprob}}
   \\&=    \argmax_{\theta} \psP\setn{\rvy(t) | \rvx(t;\theta) }\psP\setn{\rvx(t;\theta)}
     &&\mathrlap{\text{because $\rvy(t)$ is independent of $\theta$}}
   \\&=    \argmax_{\theta} \psP\setn{\rvy(t) | \rvx(t;\theta) }
   \\&\eqd \estML
     &&  \text{by definition of $\estML$}
     &&  \text{\xref{def:estML}}
\end{align*}
\end{proof}

%=======================================
\section{Measures of estimator quality}
\label{sec:quality}
%=======================================
%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{silverman1986}{35}{9780412246203}{\textsection ``1.3.2 Measures of discrepancy\ldots"},
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"},
  \citerp{bendat1966}{183}{\textsection ``5.3 Statistical Errors for Parameter Estimates"}
  }
\label{def:mse}
%---------------------------------------
\defbox{
  \begin{array}{M}
    The \fnctd{mean square error} $\mse(\estT)$ of an estimate $\estT$\\ 
    of a parameter $\theta$ is defined as
  \end{array}
  \qquad
  \begin{array}{rc>{\ds}l}
    \mse(\estT) &\eqd& \pE\brs{\brp{\estT-\theta}^2}
  \end{array}
  }
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"}
  }
\label{def:nre}
%---------------------------------------
\defbox{
  \begin{array}{M}
    The \fnctd{normalized rms error} $\nre(\estT)$\\ 
    of an estimate $\estT$\\ 
    of a parameter $\theta$ is defined as
  \end{array}
  \qquad
  \begin{array}{rc>{\ds}l}
    \nre(\estT) &\eqd& \frac{\sqrt{\mse(\estT)}}{\theta} 
                 \eqd \frac{\sqrt{\pE\brs{\brp{\estT-\theta}^2}}}{\theta}
  \end{array}
  }
\end{definition}

%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{silverman1986}{35}{9780412246203}{\textsection ``1.3.2 Measures of discrepancy\ldots"},
  \citePpc{rosenblatt1956}{835}{``integrated mean square error"}
  }
\label{def:mise}
%---------------------------------------
\defbox{
  \begin{array}{M}
    The \fnctd{mean integrated square error} $\mise(\estT)$\\
    of an estimate $\estT$ of a parameter $\theta$ is defined as
  \end{array}
  \qquad
  \begin{array}{rc>{\ds}l}
    \mise(\estT) &\eqd& \pE\int_{\theta\in\R}\brs{\brp{\estT-\theta}^2}
  \end{array}
  }
\end{definition}

The \fncte{mean square error} of $\estT$ can be expressed as the sum of two components:
the variance of $\estT$ and the bias of $\estT$ squared (next Theorem).
For an example of \pref{thm:mse} in action, see the proof for the $\mse(\meanest)$ of the 
\fncte{arithmetic mean estimate} as provided in \prefpp{thm:mse_mean}.
%---------------------------------------
\begin{theorem}
\footnote{
  \citerpg{choi1978}{76}{9780135016190},
  \citerpgc{kay1988}{45}{8131733564}{\textsection\scshape``3.3 Estimation Theory"},
  \citerpgc{stuart1991}{629}{9780340560235}{``Minium mean-square-error estimation"},
  \citerpgc{clarkson1993}{51}{0849386098}{\textsection ``2.6 Estimation of Moments"},
  \citergc{bendat2010}{1118210824}{\textsection ``1.4.3 Error Analysis Criteria"},
  \citerp{bendat1966}{183}{\textsection ``5.3 Statistical Errors for Parameter Estimates"},
  \citerpgc{bendat1980}{39}{0471058874}{\textsection ``2.4.1 Bias versus Random Errors"}
  }
\label{thm:mse}
%---------------------------------------
Let $\mse(\estT)$ be the \fncte{mean square error} \xref{def:mse} 
and $\nre(\estT)$    the \fncte{normalized rms error} \xref{def:nre} of an estimator $\estT$.
\thmbox{\begin{array}{rc>{\ds}l | rc>{\ds}l}
  \mse(\estT) &=&
      \mcom{\pE\brs{\brp{\estT-\pE\estT}^2}}{variance of $\estT$}
    + \mcom{\brs{\pE\estT - \theta}^2}{bias of $\estT$ squared}
  &
  \nre(\estT) &=&
    \frac{\sqrt{
    \pE\brs{\brp{\estT-\pE\estT}^2} + \brs{\pE\estT - \theta}^2
    }}{\theta}
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \mse(\estT)
    &\eqd \pE\brs{\brp{\estT-\theta}^2}
    && \text{by definition of $\mse$}
    && \text{\xref{def:mse}}
  \\&= \mathrlap{\pE\brs{\brp{\estT\mcom{-\pE\estT+\pE\estT}{$0$}-\theta}^2}
     \qquad\text{by \prope{additive identity} property of $\fieldC$}}
  \\&= \pE\brs{
         \brp{\estT-\pE\estT}^2
        +\mcom{\brp{\pE\estT-\theta}^2}{constant}
        -2\brp{\estT-\pE\estT}\brp{\pE\estT-\theta}
       }
    && \text{by \thme{Binomial Theorem}}
    && \text{\ifxref{polynom}{thm:binomial}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2	
        -2\pE\brs{
         \estT\pE\estT
        -\estT\theta
        -\pE\estT\estT
        +\pE\estT\theta
        }
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
        -2\mcom{\brs{
         \pE\estT\pE\estT
        -\pE\estT\pE\theta
        -\pE\estT\pE\estT
        +\pE\estT\pE\theta
        }}{$0$}
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&= \pE\brp{\estT-\pE\estT}^2
        +\brp{\pE\estT-\theta}^2
\end{align*}
\end{proof}

%--------------------------------------
\begin{definition}
\footnote{
  \citerpg{choi1978}{76}{9780135016190},
  \citerpgc{shao2003}{161}{9780387953823}{\textsection ``The UMVUE"},
  \citerpgc{bolstad2007}{164}{9780470141151}{\textsection ``Minimum Variance Unbiased Estimator"},
  }
%--------------------------------------
\defboxt{
  An estimate $\estT$ of a parameter $\theta$ is a
  \propd{minimum variance unbiased estimator} (\propd{MVUE}) if 
  \\\indentx$\begin{array}{FMD}
    (1). & $\pE\estT=0$ (\prope{unbiased}) & and
  \\(2). & no other unbiased estimator $\estPhi$ has smaller variance $\var(\estPhi)$
  \end{array}$
  }
\end{definition}

%======================================
%\section{Estimation techniques}
%======================================
\begin{figure}
\centering%
\includegraphics{graphics/estimation_techniques.pdf}
\caption{Estimation Algorithms\label{fig:est-tech}}
\end{figure}



