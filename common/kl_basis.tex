%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%=======================================
\chapter{Projection Statistics for Additive Noise Systems}
%=======================================
%=======================================
\section{Projection Statistics}
%=======================================
\prefpp{thm:sstat} (next) shows that the finite set
$\setY\eqd\set{\fdoty_n}{n=1,2,\ldots,\xN}$ (a finite number of values) provides just as
good an estimate as having the entire $\rvy(t)$ waveform
(an uncountably infinite number of values)
with respect to the following cases:
\begin{enume}
   \item the conditional probability of $\rvx(t;\theta)$ given $\rvy(t)$
   \item the \fncte{MAP estimate} of the sequence
   \item the \fncte{ML estimate}  of the sequence.
\end{enume}
That is, even with a drastic reduction in the number of statistics
from uncountably infinite to finite $\xN$,
no quality is lost with respect to the estimators listed above.
This amazing result is very useful in practical system implementation
and also for proving other theoretical results
(notably estimation and detection theorems).

But first, some definitions (next) that are used repeatedly in this chapter.
%---------------------------------------
\begin{definition}
\label{def:sstat}
%---------------------------------------
Let $\Psi\eqd\set{\fpsi_n}{n=1,2,\ldots,\xN}$ be an \structe{orthonormal basis} 
for a parameterized function $\rvx(t;\theta)$ with parameter $\theta$.
Let $\fy(t)$ be $\fx(t;\theta)$ plus a \fncte{random process} $\rvv(t)$ such that 
\\\indentx$\ds
  \rvy(t)\eqd\rvx(t;\theta)+\rvv(t)
$\\
Let $\fdoty_n$, $\fdotx_n$, and $\fdotv_n$ be \ope{projections}\ifsxref{operator}{def:opP}
onto the \fncte{basis vector} $\fpsi_n(t)$ such that 
\defbox{\begin{array}{r c>{\ds}l c>{\ds}l c>{\ds}l D}
    \fdotx_n(\theta) &\eqd& \opP_n\fx(t) &\eqd& \inprod{\fx(t;\theta)}{\fpsi_n(t)} &\eqd& \int_{t\in\R}\fx(t;\theta)\fpsi_n(t)\dt
  \\\fdotv_n         &\eqd& \opP_n\fv(t) &\eqd& \inprod{\fv(t)}       {\fpsi_n(t)} &\eqd& \int_{t\in\R}\fv(t)       \fpsi_n(t)\dt
  \\\fdoty_n         &\eqd& \opP_n\fy(t) &\eqd& \inprod{\fy(t)}       {\fpsi_n(t)} &\eqd& \int_{t\in\R}\fy(t)       \fpsi_n(t)\dt
\end{array}}
\\
Let the set $\setY$ be defined as $\setY\eqd\set{\fdoty_n}{1,2,\ldots,\xN}$ 
Let $\estMAP$ be the \fncte{MAP estimate} 
and $\estML$  be the \fncte{ML estimate} \xref{def:estML} of $\theta$.
\end{definition}

%---------------------------------------
\begin{lemma}
\label{lem:fdotv}
%---------------------------------------
Let $\Psi$, $\rvv(t)$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\lembox{
  \brb{\begin{array}{rclM}
    \pE\fv(t) &=& 0 & (\prope{zero-mean})
  \end{array}}
  \brb{\begin{array}{rclM}
  \pE\fdotv_{n} &=& 0 & (\prope{zero-mean})
  \end{array}}
  }
\end{lemma}
\begin{proof}
\begin{align*}
  \pE\fdotv_{n}
    &= \pE\inprod{\fv(t)}{\fpsi_n(t)}
    && \text{by definition of $\fdotv_n$}
    && \text{\xref{def:sstat}}
  \\&= \inprod{\pE\fv(t)}{\fpsi_n(t)}
    && \text{by \prope{linearity} of $\inprodn$}
  \\&= \inprod{0}{\fpsi_n(t)}
    && \text{by \prope{zero-mean} hypothesis}
  \\&= 0
\end{align*}
\end{proof}

%=======================================
\section{Sufficient Statistics}
%=======================================
%---------------------------------------
\begin{theorem}[\thmd{Sufficient Statistic Theorem}]
\footnote{
  \citePpc{fisher1922}{316}{``Criterion of Sufficiency"}
  }
\label{thm:sstat}
\index{optimal receiver}
\index{MAP} \index{maximum a-posteriori probability estimation}
\index{ML}  \index{maximum likelihood estimation}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
Let $\estMAP$ be the \fncte{MAP estimate} 
and $\estML$  be the \fncte{ML estimate} \xref{def:estML} of $\theta$.
\thmbox{
  \brb{\begin{array}{FMD}
     (A). & $\rvv(t)$ is \prope{zero-mean}                       & and 
   \\(B). & $\rvv(t)$ is \prope{white}                           & and
   \\(C). & $\rvv(t)$ is \prope{Gaussian}                        & 
  \end{array}}
  \implies
  \mcom{\brb{\begin{array}{F rc>{\ds}l D}
     (1). & \mc{3}{l}{\psP\set{ \rvx(t;\theta)}{\rvy(t)} = \psP\set{\rvx(t;\theta)}{\setY}}        & and
   \\(2). & \estMAP                           &=& \argmax_{\estT} \psP\set{\rvx(t;\theta)}{\setY} & and
   \\(3). & \estML                            &=& \argmax_{\estT} \psP\set{\setY}{\rvx(t;\theta)} &
  \end{array}}}{the $\xN$ element set $\setY$ is a \prope{sufficient statistic} for estimating $\fx(t;\theta)$}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item definition: Let $\ds\rvv'(t) \eqd \rvv(t) - \sum_{n=1}^\xN \fdotv_n \fpsi_n(t)$.
        \label{idef:sstat_vprime}

  \item lemma: The relationship between $\setY$ and $\rvv'(t)$ is given by
        \label{ilem:sstat_Yvprime}
  \begin{align*}
     &\boxed{\rvy(t)}
     \\&= \sum_{n=1}^\xN \inprod{\rvy(t)}{\fpsi_n(t)}\fpsi_n(t) +
          \brs{\rvy(t)- \sum_{n=1}^\xN \inprod{\rvy(t)}{\fpsi_n(t)}\fpsi_n(t) }
       && \begin{array}{@{}M}%
            by \prope{additive identity} property\\
            of $\fieldC$%
          \end{array}
     \\&\eqd \sum_{n=1}^\xN \inprod{\rvy(t)}{\fpsi_n(t)}\fpsi_n(t) +
          \brs{\rvy(t)- \sum_{n=1}^\xN \inprod{\rvx(t)+\rvv(t)}{\fpsi_n(t)}\fpsi_n(t) }
       && \text{by definition of $\fy(t)$}
     \\&= \sum_{n=1}^\xN \fdoty_n\fpsi_n(t) +
          \mcom{\rvx(t)+\rvv(t)}{$\fy(t)$}
             - \mcom{\sum_{n=1}^\xN \inprod{\rvx(t)}{\fpsi_n(t)}\fpsi_n(t)}{$\fx(t)$}
             - \mcom{\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)}{$\rvv(t)-\rvv'(t)$}
       && \begin{array}{@{}M}
            by definition of $\fdoty_n$ and\\
            \prope{additive} property of $\inprodn$\\ 
            \xref{def:inprod}
          \end{array}
     \\&= \sum_{n=1}^\xN \fdoty_n\fpsi_n(t) +
          \rvx(t)+\rvv(t) - \rvx(t) - \brs{\rvv(t) - \rvv'(t)}
     \\&= \boxed{\sum_{n=1}^\xN \fdoty_n\fpsi_n(t) + \rvv'(t)}
  \end{align*}

  \item lemma: $\pE\brs{\fdotv_n\rvv(t)} = N_o\fpsi_n(t)$. Proof: \label{ilem:sstat_vdotv}
    \begin{align*}
      &\pE\brs{\fdotv_n\rvv(t)} 
      \\&\eqd \pE\brs{\brp{\int_{t\in\R}\rvv(u)\fpsi_n(u)\du} \rvv(t)}
        && \text{by definition of $\fdotv_n(t)$}
        && \text{\xref{def:sstat}}
      \\&= \pE\brs{\int_{t\in\R}\rvv(u)\rvv(t)\fpsi_n(u)\du}
        && \text{by \prope{linearity} of $\int\du$ operator}
      \\&= \int_{t\in\R}\pE\brs{\rvv(u)\rvv(t)}\fpsi_n(u)\du
        && \text{by \prope{linearity} of $\pE$}
        && \text{\xref{thm:pE_linop}}
      \\&= \int_{t\in\R}N_o\delta(u-t)\fpsi_n(u)\du
        && \text{by \prope{white} hypothesis}
      \\&= N_o\fpsi_n(t)
        && \text{by property of \fncte{Dirac delta} $\delta(t)$}
    \end{align*}

  \item lemma: $\setY$ and $\rvv'(t)$ are \prope{uncorrelated}:\label{ilem:sstat_uncorrelated} Proof:
  \begin{align*}
     &\pE\brs{\fdoty_n \rvv'(t)}
     \\&\eqd \pE\brs{\inprod{\rvy(t)}{\fpsi_n(t)}\brp{ \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)}}
       && \text{by definitions of $\fdoty_n$ and $\rvv'(t)$}
     \\&\eqd \pE\brs{\inprod{\rvx(t)+\rvv(t)}{\fpsi_n(t)}\left( \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)\right)}
       && \text{by definition of $\fy(t)$}
     \\&= \pE\brs{\Bigg(\inprod{\rvx(t)}{\fpsi_n(t)}+\inprod{\rvv(t)}{\fpsi_n(t)}\Bigg)
              \brp{ \rvv(t)-\sum_{n=1}^\xN \inprod{\rvv(t)}{\fpsi_n(t)}\fpsi_n(t)}}
       && \begin{array}{@{}M}%
            by \prope{additive} property of\\ 
            $\inprodn$ \xref{def:inprod}%
          \end{array}
     \\&= \pE\brs{\Bigg(\fdotx_n+\fdotv_n\Bigg)
              \left( \rvv(t)-\sum_{n=1}^\xN \fdotv_n\fpsi_n(t)\right)}
       && \begin{array}{@{}M}%
            by definitions of $\fdotx_n$ and $\fdotv_n$\\
            \xref{def:sstat}
          \end{array}
     \\&= \pE\brs{\fdotx_n \rvv(t) - \fdotx_n \sum_{n=1}^\xN \fdotv_n\fpsi_n(t)
              +\fdotv_n \rvv(t) - \fdotv_n \sum_{n=1}^\xN \fdotv_n\fpsi_n(t) }
     \\&= \pE\brs{\fdotx_n \rvv(t)}
          - \pE\brs{\fdotx_n \sum_{n=1}^\xN \fdotv_n\fpsi_n(t)}
          + \pE\brs{\fdotv_n\rvv(t)}
          - \pE\brs{\sum_{m=1}^\xN \fdotv_n \fdotv_m\fpsi_m(t)}
       && \begin{array}{@{}M}%
            by \prope{linearity} of $\pE$\\%
            \xref{thm:pE_linop}%
          \end{array}
     \\&= \fdotx_n \cancelto{0}{\pE{\rvv(t)}}
          - \fdotx_n \sum_{n=1}^\xN \cancelto{0}{\pE\brs{\fdotv_n}}\fpsi_n(t)
          + \pE\brs{\fdotv_n\rvv(t)}
          - \sum_{m=1}^\xN \pE\brs{\fdotv_n \fdotv_m}\fpsi_m(t)
       && \begin{array}{@{}M}%
            by \prope{linearity} of $\pE$\\%
            \xref{thm:pE_linop}%
          \end{array}
     \\&= 0 - 0
          + \pE\brs{\fdotv_n\rvv(t)} 
          - \sum_{m=1}^\xN N_o\kdelta_{mn} \fpsi_m(t)
       && \text{by \prope{white} hypothesis}
     \\&= N_o\fpsi_n(t) - N_o\fpsi_n(t)
       && \text{by \pref{ilem:sstat_vdotv}}
     \\&= 0
     \\&\implies\text{\propb{uncorrelated}}
  \end{align*}
  
  \item lemma: $\setY$ and $\rvv'(t)$ are \prope{independent}. Proof:
        By \pref{ilem:sstat_uncorrelated}, $\fdoty_n$ and $\rvv'(t)$ are \prope{uncorrelated}.
        By hypothesis, they are \prope{Gaussian}, and thus are also \propb{independent}.
        \label{ilem:sstat_independent}
  
  \item Proof that $\psP\set{\rvx(t;\theta)}{\rvy(t)}=\psP\set{\rvx(t;\theta)}{\fdoty_1,\;\fdoty_2,\ldots,\fdoty_{\xN}}$:
        \label{item:sstat_P}
  
  \begin{align*}
     \psP\set{\rvx(t;\theta)}{\rvy(t)}
       &= \psP\set{\rvx(t;\theta)}{\sum_{n=1}^\xN\fdoty_n \fpsi_n(t) + \rvv'(t)}
     \\&= \psP\set{\rvx(t;\theta)}{\setY, \rvv'(t)}
       && \begin{array}{M}
            because $\setY$ and $\rvv'(t)$ can be\\ 
            extracted by $\inprod{\cdots}{\fpsi_n(t)}$
          \end{array}
     \\&= \frac{\psP\set{\setY, \rvv'(t)}{\rvx(t;\theta)}  P\setn{\rvx(t;\theta)} }
               {\psP\setn{\setY,\rvv'(t)}}
     \\&= \frac{\psP\set{ \setY}{\rvx(t;\theta)}\psP\set{ \rvv'(t)}{\rvx(t;\theta)}\psP\setn{\rvx(t;\theta)}}
               {\psP\setn{\setY}\psP\setn{\rvv'(t)}}
       && \text{by \prope{independence} of $\setY$ and $\rvv'(t)$ \xref{ilem:sstat_independent}}
     \\&= \frac{\psP\set{ \setY}{\rvx(t;\theta)}\psP\setn{ \rvv'(t)}\psP\setn{\rvx(t;\theta)}}
               {\psP\setn{\setY}\psP\setn{\rvv'(t)}}
       && \text{by \prope{independence} of $\fx$ and $\rvv$}
     \\&= \frac{\psP\set{\setY}{\rvx(t;\theta)} \psP\setn{\rvx(t;\theta)}}
               {\psP\setn{\setY}}
     \\&= \frac{\psP\setn{ \setY,\rvx(t;\theta)}}
               {\psP\setn{\setY}}
     \\&= \psP\set{\rvx(t;\theta)}{\setY}
       && \begin{array}{@{}M}%
            by definition of \fncte{conditional probability}\\ 
            \xref{def:conprob}
          \end{array}
  \end{align*}
  
  \item Proof that $\setY$ is a \prope{sufficient statistic} for the \vale{MAP estimate}:
  \begin{align*}
     \estMAP
       &\eqd \argmax_{\estT} \psP\set{\rvx(t;\theta)}{\rvy(t)}
       &&    \text{by definition of \vale{MAP estimate} \xref{def:estMAP}}
     \\&=    \argmax_{\estT} \psP\set{\rvx(t;\theta)}{\setY}
       &&    \text{by \pref{item:sstat_P}}
  \end{align*}
  
  \item Proof that $\setY$ is a \prope{sufficient statistic} for the \fncte{ML estimate}:
  \begin{align*}
     \estML
       &\eqd \argmax_{\estT} \psP\set{\rvy(t)}{\rvx(t;\theta)}
       &&    \text{by definition of \fncte{ML estimate} \xref{def:estML}}
     \\&=    \argmax_{\estT} \psP\set{\sum_{n=1}^\xN\fdoty_n\fpsi_n(t)+\rvv'(t)}{\rvx(t;\theta)}
     \\&=    \argmax_{\estT} \psP\set{\setY,\rvv'(t)}{\rvx(t;\theta)}
       &&    \text{because $\setY$ and $\rvv'(t)$ can be extracted by  $\inprod{\cdots}{\fpsi_n(t)}$}
     \\&=    \argmax_{\estT} \psP\set{\setY}{\rvx(t;\theta)}\psP\setn{\rvv'(t)}{\rvx(t;\theta)}
       &&    \text{by \prope{independence} of $\setY$ and $\rvv'(t)$ 
                   \xref{ilem:sstat_independent}}
     \\&=    \argmax_{\estT} \psP\set{\setY}{\rvx(t;\theta)}\psP\setn{\rvv'(t)}
       &&    \text{by \prope{independence} of $\rvx(t)$ and $\rvv'(t)$}
     \\&=    \argmax_{\estT} \psP\set{\setY}{\rvx(t;\theta)}
       &&    \text{by \prope{independence} of $\rvv'(t)$ and $\theta$}
  \end{align*}
\end{enumerate}
\end{proof}

%======================================
\section{Additive noise}
%======================================
%Depending on the nature of the channel (additive, white, and/or \prope{Gaussian})
%we can know certain characteristics of the noise and received statistics.
%These are described in the next four theorems.

%---------------------------------------
\begin{theorem}[\thmd{Additive noise projection statistics}]
\label{thm:an_stats}
\index{projection statistics!Additive noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \brb{\begin{array}{FrclDD}
    (A). & \fy(t)                    &\eqd     & \fx(t;\theta) + \fv(t) & (\propb{additive})              & and
  \\(B). & \pE\brs{\fv(t)}           &=        & 0                      & (\prope{zero-mean})             & and
  \\(C). & \fx(t)                    &\subseteq& \linspan\Psi           & ($\Psi$ \prope{spans} $\fx(t)$) & and
  \\(C). & \inprod{\fpsi_n}{\fpsi_m} &=        & \kdelta_{mn}           & (\prope{orthonormal})
  \end{array}}
  \implies
  \brb{\begin{array}{lrcl}
     \pE(\fdoty_n|\theta)       &= \fdotx_n(\theta) + \pE \fdotv_n
  \end{array}}
}
\end{theorem}
\begin{proof}
\begin{align*}
   \pE(\fdoty_n |\theta)
     &\eqd \pE\brp{\inprod{\rvy(t)}{\fpsi_n(t)}  |\theta}
     && \text{by definition of $\fdoty_n$}
     && \text{\xref{def:sstat}}
   \\&= \pE{\inprod{\rvx(t;\theta)+\fv(t)}{\fpsi_n(t)}}
     && \text{by \prope{additive} hypothesis}
     && \text{hypothesis (A)}
   \\&= \pE{\inprod{\rvx(t;\theta)}{\fpsi_n(t)}} +   {\inprod{\fv(t)}{\fpsi_n(t)}}
     && \text{by \prope{additive} property of $\inprodn$}
     && \text{\xref{def:inprod}}
   \\&= \pE\brp{\inprod{\sum_{k=1}^\xN \fdotx_k(\theta) \fpsi_k(t)}{\fpsi_n(t)} + \fdotv_n}
     && \text{by definition of $\fdotv_n$}
     && \text{\xref{def:sstat}}
   \\&= \pE\brp{\sum_{k=1}^\xN \fdotx_k(\theta) \inprod{\fpsi_k(t)}{\fpsi_n(t)} + \fdotv_n}
     && \text{by \prope{additive} property of $\inprodn$}
     && \text{\xref{def:inprod}}
   \\&= \pE\brp{\sum_{k=1}^\xN \fdotx_k(\theta) \kdelta_{k-n}(t) + \fdotv_n}
     && \text{by \prope{orthonormal} hypothesis}
     && \text{(C)}
   \\&= \pE\brp{\fdotx_n(\theta) + \fdotv_n}
     && \text{by definition of $\kdelta$}
     && \text{\ifsxref{vsinprod}{def:kdelta}}
   \\&= \pE{\fdotx_n(\theta) } + \cancelto{0}{\pE{\fdotv_n}}
     && \text{by \prope{linearity} of $\pE$}
     && \text{\xref{thm:pE_linop}}
   \\&= \fdotx_n(\theta)
\end{align*}
\end{proof}

%======================================
%\subsection{Additive gaussian noise channel}
%\label{sec:opCagn}
%======================================
\begin{figure}[ht] \color{figcolor}
\centering%
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotv_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdoty_n|\theta_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdoty_n|\theta_2)$} }
\end{picture}
\caption{
  Additive \prope{Gaussian} noise channel Statistics
   %\label{fig:awgn_stats}
   }
\end{figure}

%---------------------------------------
\begin{theorem}[\thmd{Additive Gaussian noise projection statistics}]
\label{thm:agn_stats}
\index{projection statistics!Additive \prope{Gaussian} noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \mcom{\brb{\begin{array}{FrclDD}
    (A). & \fy(t)                    &\eqd     & \fx(t) + \fv(t) & (\propb{additive})              & and
  \\(B). & \fv(t)                    &\sim     & \pN{0}{\sigma^2}& (\propb{Gaussian})              & and
  \\(C). & \fx(t)                    &\subseteq& \linspan\Psi    & ($\Psi$ \prope{spans} $\fx(t)$) & and
  \\(D). & \inprod{\fpsi_n}{\fpsi_m} &=        & \kdelta_{mn}    & (\prope{orthonormal})
  \end{array}}}{\prope{additive Gaussian} system}
\implies
\brb{\begin{array}{Frcl}
     (1). & \pE\fdotv_n           &=   & 0                               %& (\prope{zero-mean})
   \\(2). & \pE(\fdoty_n|\theta)  &=   & \fdotx_n(\theta)                %& 
   \\(3). & \fdotv_n              &\sim& \pN{0}{\sigma^2}                %& (\prope{Gaussian})
   \\(4). & \fdoty_n|\theta       &\sim& \pN{\fdotx_n(\theta)}{\sigma^2} %& (\prope{Gaussian})
\end{array}}
}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Proof for (1): By hypothesis (3) and \prefp{lem:fdotv}.

  \item Proof for (2):
    \begin{align*}
       \pE(\fdoty_n |\theta)
         &\eqd \pE\brs{\inprod{\rvy(t)}{\fpsi_n(t)}  |\theta}
         && \text{by definition of $\fdoty_n$}
         && \text{\xref{def:sstat}}
       \\&= \pE\brs{\inprod{\rvx(t;\theta)+\fv(t)}{\fpsi_n(t)}}
         && \text{by \prope{additive} hypothesis}
         && \text{hypothesis (A)}
       \\&= \pE\brs{\inprod{\rvx(t;\theta)}{\fpsi_n(t)}} +   \pE\brs{\inprod{\fv(t)}{\fpsi_n(t)}}
         && \text{by \prope{additive} property of $\inprodn$}
         && \text{\xref{def:inprod}}
       \\&= \pE\inprod{\sum_{k=1}^\xN \fdotx_k(\theta) \fpsi_k(t)}{\fpsi_n(t)} + \pE\fdotv_n
       \\&=    \sum_{k=1}^\xN \pE\brs{\fdotx_k(\theta)}\inprod{\fpsi_k(t)}{\fpsi_n(t)} + \pE\fdotv_n
       \\&=    \sum_{k=1}^\xN \pE\brs{\fdotx_k(\theta)}\kdelta_{k-n}(t) + \pE\fdotv_n
         && \text{by \prope{orthonormal} hypothesis}
         && \text{(D)}
       \\&=  \pE\fdotx_n(\theta)  + \pE\fdotv_n
         && \text{by definition of $\kdelta$}
         && \text{\ifsxref{vsinprod}{def:kdelta}}
       \\&= \fdotx_n(\theta) + 0
         && \text{by \prefp{lem:fdotv}}
    \end{align*}

  \item Proof for (3) and (4): 
        The distributions follow because they are linear operations on Gaussian processes.
\end{enumerate}
\end{proof}

%======================================
%\subsection{Additive white noise channel}
%\label{sec:opCawn}
%======================================
%---------------------------------------
\begin{theorem}[\thmd{Additive white noise projection statistics}]
\label{thm:awn_stats}
\index{projection statistics!Additive white noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \mcom{\brbr{\begin{array}{FrclD}
    (A). & \fy(t)                    &\eqd     & \fx(t) + \fv(t)     & and %& (\propb{additive})              & and
  \\(B). & \cov{\fv(t)}{\fv(u)}      &=        & \sigma^2\delta(t-u) & and %& (\propb{white})                 & and
  \\(C). & \pE\brs{\fv(t)}           &=        & 0                   & and %  (\prope{zero-mean})             & and
 %\\(B). & \fv(t)                    &\sim     & \pN{0}{\sigma^2}    & and %& (\propb{Gaussian})              & and
  \\(E). & \fx(t)                    &\subseteq& \linspan\Psi        & and %& ($\Psi$ \prope{spans} $\fx(t)$) & and
  \\(E). & \inprod{\fpsi_n}{\fpsi_m} &=        & \kdelta_{mn}        &     %& (\prope{orthonormal})
  \end{array}}}{\prope{additive white} system}
  \implies
  \brbl{\begin{array}{FlclDD}
       (1). & \pE\fdotv_n                            &=& 0                     & (\prope{zero-mean})    & and 
     \\(2). & \pE(\fdoty_n|\theta)                   &=& \fdotx_n(\theta)      &                        & and 
     \\(3). & \cov{\fdotv_n}{\fdotv_m}               &=& \sigma^2 \kdelta_{nm} & (\prope{uncorrelated}) & and 
     \\(4). & \cov{\fdoty_n|\theta}{\fdoty_m|\theta }&=& \sigma^2 \kdelta_{nm} & (\prope{uncorrelated}) &
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Because the noise is \prope{additive} (hypothesis A)\ldots
    \begin{align*}
         \pE\fdotv_{n}            &= 0                            && \text{by \prope{additive} property and \prefp{thm:an_stats}}
       \\(\fdoty_n |\theta)       &= \fdotx_n(\theta)  + \fdotv_n && \text{by \prope{additive} property and \prefp{thm:an_stats}}
       \\\pE({\fdoty_n} | \theta) &= \fdotx_n(\theta)             && \text{by \prope{additive} property and \prefp{thm:an_stats}}
    \end{align*}

  \item Proof for (3):\label{item:awn_stats_covvv}
    \begin{align*}
     \cov{\fdotv_m}{\fdotv_n}
        &= \cov{\inprod{\fv(t)}{\fpsi_m(t)}}{\inprod{\fv(t)}{\fpsi_n(t)}}
        && \text{by def. of $\fdotv_n$}
        && \text{\xref{def:sstat}}
      \\&= \cov{\brp{\int_{t\in\R} \fv(t) \fpsi_m(t)\dt}}
               {\brp{\int_{u\in\R} \fv(u) \fpsi_n(u)\du}}
        && \text{by def. of $\inprodn$}
        && \text{\xref{def:sstat}}
      \\&= \pE\brs{\brp{\int_{t\in\R} \fv(t) \fpsi_m(t)\dt} 
                   \brp{\int_{u\in\R} \fv(u) \fpsi_n(u)\du}}
        && \text{by def. of $\pCov$}
      \\&= \pE\brs{\int_{t\in\R}\int_{u\in\R} \fv(t)\fv(u) \fpsi_m(t) \fpsi_n(u) \dt\du} 
      \\&= \int_{t\in\R}\int_{u\in\R} \pE\brs{\fv(t)\fv(u)} \fpsi_m(t) \fpsi_n(u) \dt\du
      \\&= \int_{t\in\R}\int_{u\in\R} \sigma^2\delta(t-u) \fpsi_m(t) \fpsi_n(u) \dt\du
        && \text{by \prope{white} hyp.}
        && \text{(B)}
      \\&= \sigma^2 \int_{t\in\R}  \fpsi_m(t) \fpsi_n(u) \dt
      \\&= \sigma^2 \inprod{\fpsi_m(t)}{\fpsi_n(u)}
        && \text{by def. of $\inprodn$}
        && \text{\xref{def:sstat}}
      \\&= \brbl{\begin{array}{lM}
               \sigma^2 & for $n=m$ \\
               0        & for $n\ne m$.
            \end{array}}
        && \text{by \prope{orthonormal} prop.}
        && \text{\xref{def:sstat}}
    \end{align*}
  
  \item Proof for (4):
    \begin{align*}
      \cov{\fdoty_n|\theta}{\fdoty_m|\theta }
         &= \pE\brs{\fdoty_n \fdoty_m |\theta} - [\pE\fdoty_n|\theta][\pE\fdoty_m|\theta ]
       \\&= \pE\brs{(\fdotx_n(\theta) +\fdotv_n)(\fdotx_m(\theta) +\fdotv_m)} - \fdotx_n(\theta) \fdotx_m(\theta)
       \\&= \pE\brs{\fdotx_n(\theta) \fdotx_m(\theta) +\fdotx_n(\theta) \fdotv_m+ \fdotv_n\fdotx_m(\theta) +\fdotv_n\fdotv_m } - \fdotx_n(\theta) \fdotx_m(\theta)
       \\&= \fdotx_n(\theta) \fdotx_m(\theta) + \fdotx_n(\theta) \pE\brs{\fdotv_m}+ \pE\brs{\fdotv_n}\fdotx_m(\theta) +\pE\brs{\fdotv_n\fdotv_m}  - \fdotx_n(\theta) \fdotx_m(\theta)
       \\&= 0 + \fdotx_n(\theta) \cdot0 + 0\cdot\fdotx_m(\theta) + \cov{\fdotv_n}{\fdotv_m}+[\pE\fdotv_n][\pE\fdotv_m]
       \\&= \sigma^2 \kdelta_{nm} + 0\cdot0
         && \text{by \pref{item:awn_stats_covvv}}
       \\&= \brbl{\begin{array}{lM}
                \sigma^2 & for $n=m$ \\
                0        & for $n\ne m$.
             \end{array}}
    \end{align*}
\end{enumerate}
\end{proof}

%======================================
%\subsection{Additive white gaussian noise channel}
%\label{sec:opCawgn}
%======================================
\begin{figure}[ht] \color{figcolor}
\centering%
\setlength{\unitlength}{0.2mm}
\begin{picture}(300,160)(-200,-30)
  %\graphpaper[10](-100,0)(300,150)
  \thicklines
  \put(-200,   0 ){\line(1,0){400} }
  \put(   0, -10 ){\line(0,1){150} }

  \qbezier[30](100,0)(100, 60)(100,120)
  \qbezier[30](-100,0)(-100, 60)(-100,120)

  \qbezier(- 60,  60)(-100, 180)(-140,  60)
  \qbezier(-  0,   0)(- 40,   0)(- 60,  60)
  \qbezier(-140,  60)(-160,   0)(-200,   0)

  \qbezier( -40,  60)(   0, 180)(  40,  60)
  \qbezier(-100,   0)( -60,   0)( -40,  60)
  \qbezier(  40,  60)(  60,   0)( 100,   0)

  \qbezier(  60,  60)( 100, 180)( 140,  60)
  \qbezier(   0,   0)(  40,   0)(  60,  60)
  \qbezier( 140,  60)( 160,   0)( 200,   0)

  \put(   0, -20 ){\makebox(0,0)[t]{$0$} }
  \put(  30, 100 ){\makebox(0,0)[bl]{$\fdotv_n$} }
  \put(-100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_1)$} }
  \put(-130, 100 ){\makebox(0,0)[br]{$(\fdoty_n|\theta_1)$} }
  \put( 100, -20 ){\makebox(0,0)[t]{$\fdotx_n(\theta_2)$} }
  \put( 130, 100 ){\makebox(0,0)[bl]{$(\fdoty_n|\theta_2)$} }
\end{picture}
\caption{
  Additive white \prope{Gaussian} noise channel statistics
   \label{fig:awgn_stats}
   }
\end{figure}

%---------------------------------------
\begin{theorem}[\thmd{AWGN projection statistics}]
\label{thm:awgn_stats}
\label{thm:ms_stats}
\index{projection statistics!Additive white \prope{Gaussian} noise channel}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
\thmbox{
  \mcom{\brbr{\begin{array}{FrclD}
    (A). & \fy(t)                    &\eqd     & \fx(t) + \fv(t)     & and %& (\propb{additive})              & and
  \\(B). & \cov{\fv(t)}{\fv(u)}      &=        & \sigma^2\delta(t-u) & and %& (\propb{white})                 & and
  \\(C). & \fv(t)                    &\sim     & \pN{0}{\sigma^2}    & and %& (\propb{Gaussian})              & and
  \\(D). & \fx(t)                    &\subseteq& \linspan\Psi        & and %& ($\Psi$ \prope{spans} $\fx(t)$) & and
  \\(E). & \inprod{\fpsi_n}{\fpsi_m} &=        & \kdelta_{mn}        &     %& (\prope{orthonormal})
  \end{array}}}{\prope{additive white Gaussian} system}
  \implies
  \brbl{\begin{array}{FrclD}
       (1). & \fdotv_n                            &\sim& \pN{0}{\sigma^2}                     & (\prope{Gaussian})
     \\(2). & \fdoty_n|\theta                     &\sim& \pN{\fdotx_n(\theta)}{\sigma^2}      & (\prope{Gaussian})
     \\(3). & \cov{\fdotv_n}{\fdotv_m}            &=   & \sigma^2 \kdelta_{nm}                & (\prope{uncorrelated})
     \\(4). & \cov{\fdoty_n}{\fdoty_m }           &=   & \sigma^2 \kdelta_{nm}                & (\prope{uncorrelated})
     \\(5). & \psp\{\fdotv_n=a \land \fdotv_m=b\} &=   & \psp\{\fdotv_n=a\}\psp\{\fdotv_m=b\} & (\prope{independent})
     \\(6). & \psp\{\fdoty_n=a \land \fdoty_m=b\} &=   & \psp\{\fdoty_n=a\}\psp\{\fdoty_m=b\} & (\prope{independent})
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Proof for (1) and (2) follow because the operations are \prope{linear} on processes are \prope{Gaussian} (hypothesis C).

  \item Proof for (3) and (4):
    \begin{align*}
       \pE{\fdotv_{n}}           &= 0                      && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
     \\\cov{\fdotv_m}{\fdotv_n}  &= \sigma^2 \kdelta_{mn}  && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
     \\\fdoty_n                  &= \fdotx_n  + \fdotv_n   && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
     \\\pE{\fdoty_n}             &= \fdotx_n               && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
     \\\cov{\fdoty_n}{\fdoty_m } &= \sigma^2 \kdelta_{mn}  && \text{by \prope{AWN} properties and \prefp{thm:awn_stats}}
    \end{align*}

  \item Proof for (5) and (6): Because the processes are \prope{Gaussian},
        \prope{uncorrelated} implies \prope{independent}.
\end{enumerate}
\end{proof}

%======================================
\section{Optimal symbol estimation}
\label{sec:awgn_est}
\index{maximum likelihood estimation}
%======================================
The AWGN projection statistics provided by
\prefpp{thm:awgn_stats} help generate the optimal
ML-estimates for a number of communication systems.
These ML-estimates can be expressed in either of two standard forms:
\begin{liste}
  \item {\bf Spectral decompostion}:
     The optimal estimate is expressed in terms of \hie{projections}
     of signals onto orthonormal basis functions.
  \item {\bf Matched signal}:
     The optimal estimate is expressed in terms of the (noisy)
     received signal correlated with (``matched" with)
     the (noiseless) transmitted signal.
\end{liste}
\prefpp{thm:estML_general} (next) expresses the general
optimal \fncte{ML estimate} in both of these forms.

Parameter detection is a special case of parameter estimation.
In parameter detection, the estimate is a member of an finite set.
In parameter estimation, the estimate is a member of an infinite set
\xref{sec:awgn_est}.

%---------------------------------------
\begin{theorem}[\thmd{General ML estimation}]
\index{maximum likelihood estimation!general}
\label{thm:estML_general}
\label{thm:ml_est_det}
%\citepp{srv}{157}{158}
%---------------------------------------
Let $\Psi$, $\rvy(t)$, $\rvx(t)$, $\rvv(t)$, $\fdoty_n$, $\fdotx_n$, $\fdotv_n$, and $\setY$ be defined as in \prefpp{def:sstat}.
Let $\estML$  be the \fncte{ML estimate} \xref{def:estML} of $\theta$.
\thmbox{\begin{array}{rc>{\ds}l@{\qquad}D}
   \estML
     &=& \argmin_{\estT} \brs{ \sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 }
       & (spectral decomposition)
   \\&=& \argmax_{\estT}
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\theta)}-\norm{\rvx(t;\theta)}^2 \right]
       & (matched signal)
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML
     &= \argmax_{\estT} \psP\set{\rvy(t)}{\rvx(t;\theta)}
   \\&= \argmax_{\estT} \psP\set{\fdoty_1,\fdoty_2,\ldots,\fdoty_n}{\rvx(t;\theta)}
     && \text{by \prefpp{thm:sstat}}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN \psP\set{\fdoty_n}{\rvx(t;\theta)}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN \pdfpb{\fdoty_n|\rvx(t;\theta)}
   \\&= \argmax_{\estT} \prod_{n=1}^\xN
         \frac{1}{\sqrt{2\pi\sigma^2}}
         \exp{\frac{[\fdoty_n-\fdotx_n(\estT)]^2}{-2\sigma^2} }
     && \text{by \prefpp{thm:awgn_stats}}
   \\&= \argmax_{\estT}
         \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
         \exp{\frac{-1}{2\sigma^2} \sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 }
   \\&= \argmax_{\estT}
         \left[ -\sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
\\ \\
   \\&= \argmax_{\estT}
         \left[ -\lim_{N\to\infty}\sum_{n=1}^\xN [\fdoty_n-\fdotx_n(\estT)]^2 \right]
     && \text{by \prefpp{thm:sstat}}
   \\&= \argmax_{\estT}
         \left[ -\norm{\rvy(t)-\rvx(t;\theta)}^2 \right]
     && \text{by \thme{Plancheral's formula}}
     && \text{\xref{thm:plancherel}}
   \\&= \argmax_{\estT}
         \left[ -\norm{\rvy(t)}^2 +2\Real\inprod{\rvy(t)}{\rvx(t;\theta)}-\norm{\rvx(t;\theta)}^2 \right]
   \\&= \argmax_{\estT}
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\theta)}-\norm{\rvx(t;\theta)}^2 \right]
     && \text{because $\rvy(t)$ \prope{independent} of $\estT$}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{ML amplitude estimation}]
\label{thm:estML_amplitude}
\footnote{
  \citerppg{srv}{158}{159}{013125295X}
  }
\index{maximum likelihood estimation!amplitude}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
\begin{align*}
     \rvy(t)     &= \rvx(t;a) + \fv(t)
   \\\rvx(t;a)   &\eqd  a  \sym(t).
\end{align*}
Then
\thmbox{\begin{array}{rclM}
  \estML[a]    &=&  \frac{1}{\norm{\lambda(t)}^2} \inprod{\rvy(t)}{\lambda(t)}
               &    (optimal ML-estimate of $a$)
             \\&=&  \frac{1}{\norm{\sym(t)}^2} \sum_{n=1}^\xN \fdoty_n \fdotlam_n
               & %\text{(optimal ML-estimate of $a$)}
\\\pE\estML[a] &=& a
               &   ($\estML[a]$ is \propb{unbiased})
\\\var\estML[a]&=& \frac{\sigma^2}{\norm{\sym(t)}^2}
               &  (variance of estimate $\estML[a]$)
\\\var\estML[a]&=& \mbox{CR lower bound}
               &   ($\estML[a]$ is an {\bf \prope{efficient} estimate})
\end{array}}
\end{theorem}

\begin{proof}\\
\begin{enumerate}
\item \fncte{ML estimate} in ``matched signal" form:
\begin{align*}
   \estML[a]
     &= \argmax_a
         \left[ 2\inprod{\rvy(t)}{\rvx(t;\theta)}-\norm{\rvx(t;\phi)}^2 \right]
     && \text{by \prefpp{thm:estML_general}}
   \\&= \argmax_a
         \left[ 2\inprod{\rvy(t)}{a\lambda(t)}-\norm{a\lambda(t)}^2 \right]
     && \text{by hypothesis}
   \\&= \arg_a
         \left[ \pderiv{}{a}2a\inprod{\rvy(t)}{\lambda(t)}-\pderiv{}{a}a^2\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ 2\inprod{\rvy(t)}{\lambda(t)}-2a\norm{\lambda(t)}^2 =0\right]
   \\&= \arg_a
         \left[ \inprod{\rvy(t)}{\lambda(t)}=a\norm{\lambda(t)}^2 \right]
   \\&= \frac{1}{\norm{\lambda(t)}^2} \inprod{\rvy(t)}{\lambda(t)}
\end{align*}

\item \fncte{ML estimate} in ``spectral decomposition" form:
\begin{align*}
   \estML[a]
     &= \argmin_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}^2 }
     && \text{by \prefpp{thm:estML_general}}
   \\&= \arg_a
         \brp{ \pderiv{}{ a }\sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}^2=0 }
   \\&= \arg_a
         \brp{ 2\sum_{n=1}^\xN \brs{\fdoty_n - \fdotx_n( a )}\pderiv{}{ a }\fdotx_n( a )=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n - \inprod{ a \lambda(t)}{\fpsi_n(t)}}\pderiv{}{ a }\inprod{ a \lambda(t)}{\fpsi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \inprod{\lambda(t)}{\fpsi_n(t)}}\pderiv{}{ a }( a \inprod{\lambda(t)}{\fpsi_n(t)})=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \fdotlam_n } \inprod{\lambda(t)}{\fpsi_n(t)}=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \brs{\fdoty_n -  a \fdotlam_n } \fdotlam_n=0 }
   \\&= \arg_a
         \brp{ \sum_{n=1}^\xN \fdoty_n\fdotlam_n = \sum_{n=1}^\xN  a \fdotlam_n^2 }
   \\&= \brp{\frac{1}{\sum_{n=1}^\xN \fdotlam_n^2}}
         \sum_{n=1}^\xN \fdoty_n\fdotlam_n
   \\&= \frac{1}{\norm{\lambda(t)}^2}
         \sum_{n=1}^\xN \fdoty_n\fdotlam_n
\end{align*}

\item Prove that the estimate $\estML[a]$ is \propb{unbiased}:

\begin{align*}
   \pE\estML[a]
     &= \pE\frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} \rvy(t)\sym(t) \dt
     && \text{by previous result}
   \\&= \pE\frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} [ a \sym(t)+\fv(t)]\sym(t) \dt
     && \text{by hypothesis}
   \\&= \frac{1}{\norm{\sym(t)}^2} \int_{t\in\R} \pE[ a \sym(t)+\fv(t)]\sym(t) \dt
     && \text{by linearity of $\int\cdot\dt$ and $\pE$}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \int_{t\in\R} \sym^2(t) \dt
     && \text{by $\pE$ operation}
   \\&= \frac{1}{\norm{\sym(t)}^2}  a  \norm{\sym(t)}^2
     && \text{by definition of $\norm{\cdot}^2$}
   \\&=   a
\end{align*}

\item Compute the variance of $\estML[a]$:
\begin{align*}
  \pE \estML[a]^2
    &= \pE \left[ \frac{1}{\norm{\lambda(t)}^2} \int_{t\in\R} \rvy(t)\lambda(t) \dt\right]^2
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \rvy(t)\lambda(t) \dt \int_v \rvy(v)\lambda(v) \dv
        \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v [a\lambda(t) + \fv(t)][a\lambda(v) + \fv(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \pE \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v
        [a^2\lambda(t)\lambda(v) + a\lambda(t)\fv(v) + a\lambda(v)\fv(t) + \fv(t)\fv(v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \left[  \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v
        [a^2\lambda(t)\lambda(v) + 0 + 0 + \sigma^2\delta(t-v)]
        \lambda(t) \lambda(v)
        \dv\dt \right]
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v a^2\lambda^2(t)\lambda^2(v) \dv\dt +
        \frac{1}{\norm{\lambda(t)}^4}
        \int_{t\in\R} \int_v \sigma^2\delta(t-v) \lambda(t) \lambda(v) \dv\dt
  \\&= \frac{1}{\norm{\lambda(t)}^4}
        a^2 \int_{t\in\R} \lambda^2(t) \dt \int_v \lambda^2(v) \dv +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2\int_{t\in\R} \lambda^2(t) \dt
  \\&= a^2 \frac{1}{\norm{\lambda(t)}^4}
        \norm{\lambda(t)}^2 \norm{\lambda(v)}^2 +
        \frac{1}{\norm{\lambda(t)}^4}
        \sigma^2 \norm{\lambda(t)}^2
  \\&= a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}
\\
\\
  \var\estML[a]
    &= \pE \estML[a]^2 - (\pE \estML[a])^2
  \\&= \left.\left.\left(a^2 + \frac{\sigma^2}{\norm{\lambda(t)}^2}\right) - \right( a^2 \right)
  \\&= \frac{\sigma^2}{\norm{\lambda(t)}^2}
\end{align*}

\item Compute the \ineq{Cram/'er-Rao Bound}:
\begin{align*}
   \pdfpb{\rvy(t)|\fx(t; a)}
     &=  \pdfpb{\fdoty_1, \fdoty_2,\ldots,\fdoty_{\xN}|\fx(t; a)}
   \\&=  \prod_{n=1}^\xN \frac{1}{\sqrt{2\pi\sigma^2}}
          \exp{\frac{(\fdoty_n- a\fdotlam_n)^2}{-2\sigma^2}}
   \\&=  \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
\\
\\
   \pderiv{}{a}\ln\pdfpb{\rvy(t)|\fx(t; a)}
     &=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}\ln
          \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^\xN
       +  \pderiv{}{a}\ln
          \exp{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \pderiv{}{a}
          \brs{\frac{1}{-2\sigma^2} \sum_{n=1}^\xN (\fdoty_n- a\fdotlam_n)^2}
   \\&=  \frac{1}{-2\sigma^2} \sum_{n=1}^\xN 2(\fdoty_n- a\fdotlam_n)(- \fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdoty_n- a\fdotlam_n)
\\
\\
   \pderiv{^2}{a^2}\ln\pdfpb{\rvy(t)|\fx(t; a)}
     &=  \pderiv{}{a}\pderiv{}{a}\ln\pdfpb{\rvy(t)|\fx(t; a)}
   \\&=  \pderiv{}{a}
          \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(\fdoty_n- a\fdotlam_n)
   \\&=  \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n(-\fdotlam_n)
   \\&=  \frac{-1}{\sigma^2} \sum_{n=1}^\xN \fdotlam_n^2
   \\&=  \frac{-\norm{\lambda(t)}^2}{\sigma^2}
\\
\\
   \var\estML[a]
     &\eqd \pE\brs{\estML[a]-\pE\estML[a]}^2
   \\&=    \pE\brs{\estML[a]- a}^2
   \\&\ge  \frac{-1}{\pE\brp{\pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t)|\fx(t; a)}}}
   \\&=    \frac{-1}{\pE\brp{\frac{-\norm{\lambda(t)}^2}{\sigma^2}}}
   \\&=    \frac{\sigma^2}{\norm{\lambda(t)}^2}
     \qquad\text{(Cram/'er-Rao lower bound of the variance)}
\end{align*}

\item Proof that $\estML[a]$ is an \prope{efficient} estimate:

An estimate is \prope{efficient} if
$\var\estML[a]=\mbox{CR lower bound}$.
We have already proven this, so $\estML[a]$ is an \prope{efficient} estimate.

Also, even without explicitly computing the variance of $\estML[a]$,
the variance equals the \vale{Cram/'er-Rao lower bound}
(and hence $\estML[a]$ is an \prope{efficient} estimate)
if and only if
\\\indentx$\ds\estML[a] -  a =
   \brp{\frac{-1}{\pE\brs{
              \pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t)|\fx(t; a)}
           }}}
   \brp{\pderiv{}{ a} \ln \pdfpb{\rvy(t)|\fx(t; a)}}
  $
\begin{align*}
   \brp{\frac{-1}{\pE\brp{\pderiv{^2}{ a^2} \ln \pdfpb{\rvy(t)|\fx(t; a)}}}}
   \brp{\pderiv{}{ a} \ln \pdfpb{\rvy(t)|\fx(t; a)}}
     &= \left(\frac{\sigma^2}{\norm{\lambda(t)}^2}\right)
         \left(
           \frac{1}{\sigma^2} \sum_{n=1}^\xN \fdotlam( \fdoty - a \fdotlam)
         \right)
   \\&= \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam \fdoty -
         \frac{1}{\norm{\lambda(t)}^2} \sum_{n=1}^\xN \fdotlam^2
   \\&= \estML[a] - a
\end{align*}
\end{enumerate}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{ML phase estimation}]
\label{thm:estML_phase}
\footnote{
  \citerppg{srv}{159}{160}{013125295X}
  }
\index{maximum likelihood estimation!phase}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
$\begin{array}[t]{rcl}
     \rvy(t)      &=& \fx(t;\phi) + \fv(t)
   \\\rvx(t;\phi) &=& A\cos(2\pi f_ct +  \phi).
\end{array}$
\\
Then the optimal ML-estimate of parameter $ \phi $ is
\thmbox{
   \estML[\phi]
      =   -\atan\brp{
           \frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           }
   }
\end{theorem}
\begin{proof}
\begin{align*}
   \estML[\phi]
     &= \argmax_\phi
         \brs{ 2\inprod{\rvy(t)}{\rvx(t;\theta)}-\norm{\rvx(t;\phi)}^2 }
     && \text{by \prefpp{thm:estML_general}}
   \\&= \argmax_\phi
         \brs{ 2\inprod{\rvy(t)}{\rvx(t;\phi)} }
     && \text{because $\norm{\rvx(t;\phi)}$ does not depend on $\phi$}
   \\&= \arg_\phi
         \brs{ \pderiv{}{\phi} \inprod{\rvy(t)}{\rvx(t;\phi)} = 0 }
   \\&= \arg_\phi
         \brs{ \inprod{\rvy(t)}{\pderiv{}{\phi} \rvx(t;\phi)} = 0 }
     && \text{because $\inprod{\cdot}{\cdot}$ is a linear operator}
   \\&= \arg_\phi
         \brs{\inprod{\rvy(t)}{\pderiv{}{\phi} A\cos(2\pi f_ct+\phi)} = 0 }
   \\&= \arg_\phi
         \brs{\inprod{\rvy(t)}{-A\sin(2\pi f_ct+\phi)} = 0 }
   \\&= \arg_\phi
         \left[ -A\inprod{\rvy(t)}{\cos(2\pi f_ct)\sin\phi+\sin(2\pi f_ct)\cos\phi} = 0 \right]
   \\&= \arg_\phi \brs{
           \sin\phi\inprod{\rvy(t)}{\cos(2\pi f_ct)} =
          -\cos\phi\inprod{\rvy(t)}{\sin(2\pi f_ct)}
           }
   \\&= \arg_\phi \brs{
           \frac{\sin\phi}{\cos\phi} =
          -\frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           }
   \\&= \arg_\phi \brs{
           \tan\phi =
          -\frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           }
   \\&=  -\atan\brp{
           \frac{\inprod{\rvy(t)}{\sin(2\pi f_ct)}}
                {\inprod{\rvy(t)}{\cos(2\pi f_ct)}}
           }
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{ML estimation of a function of a parameter}]
\footnote{
  \citerppg{srv}{142}{143}{013125295X}
  }
\label{thm:estML-CR}
%---------------------------------------
Let $\opSys$ be an additive white gaussian noise system
such that
$\begin{array}[t]{rcl}
     \rvy(t)       &=& \fx(t;\theta) + \fv(t)
   \\\fx(t;\theta) &=& \fg(\theta)
\end{array}$\\
and $\fg$ is \prope{one-to-one and onto} (\prope{invertible}).
\\
\thmbox{\begin{array}{M>{\ds}rc>{\ds}l}
  Then the optimal ML-estimate of parameter $\theta$ is
   & \estML &=& \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right).
  \\
  If an \fncte{ML estimate} $\estML$ is unbiased ($\pE \estML = \theta$) then
    & \var\estML &\ge&
      \frac{\sigma^2}{\xN}
      \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}.
  \\
  If $\fg(\theta) = \theta$ then $\estML$ is an \propb{efficient} estimate such that
   & \var\estML &=& \frac{\sigma^2}{\xN}.
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
   \estML
     &= \argmin_{\theta}
         \brs{\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }
     && \text{by \prefp{thm:estML_general}}
   \\&= \arg_{\theta}\brs{
            \pderiv{}{\theta}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 = 0
         }
     && \text{because form is \prope{quadratic}}
   \\&= \arg_{\theta}\left[
             2\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]\pderiv{}{\theta}\fg(\theta) = 0
         \right]
   \\&= \arg_{\theta}\left[
             2\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)] = 0
         \right]
   \\&= \arg_{\theta}\left[
             \sum_{n=1}^\xN \fdoty_n = \xN \fg(\theta)
         \right]
   \\&= \arg_{\theta}\left[
             \fg(\theta) = \frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n
         \right]
   \\&= \arg_{\theta}\brs{
              \theta  = \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right)
         }
   \\&= \fg^{-1}\left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n\right)
\end{align*}


If $\estML$ is unbiased ($\pE\estML=\theta$), we can use
the \vale{Cram/'er-Rao bound} to find a lower bound on the variance:

\begin{align*}
   \var\estML
     &\eqd \pE\brs{\estML-\pE\estML}^2
   \\&= \pE\brs{\estML-\theta}^2
   \\&\ge \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t)|\fx(t;\theta)}
           }}
     && \text{by \ineqe{Cram/'er-Rao Inequality}}
   \\&=   \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln
              \pdfpb{\fdoty_1, \fdoty_2,\ldots,\fdoty_{\xN}|\fx(t;\theta)}
           }}
     && \text{by \thme{Sufficient Statistic Theorem}}
     && \text{\xref{thm:sstat}}
   \\&=   \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN
              \exp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }\right]
           }}
     && \begin{array}{@{}M}
         by \prope{AWGN} hypothesis\\
         and \prefp{thm:awgn_stats}
        \end{array}
   \\&=   \frac{-1}{\ds\pE\brp{
              \pderiv{^2}{\theta^2} \ln\left[
              \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^\xN \right]
              +
              \pderiv{^2}{\theta^2} \ln\left[
              \exp{\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 }\right]
           }}
  \\&=   \frac{-1}{\ds\pE\brp{
             \pderiv{^2}{\theta^2}
             \left(\frac{-1}{2\sigma^2}\sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2 \right)
          }}
  \\&=   \frac{2\sigma^2}{\ds\pE\brp{
             \pderiv{}{\theta} \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]^2
          }}
  \\&=   \frac{2\sigma^2}{\ds\pE\brp{
             -2\pderiv{}{\theta}
             \pderiv{\fg(\theta)}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
          }}
    && \text{by \thme{Chain Rule}}
  \\&=   \frac{-\sigma^2}{\ds\pE\brp{
             \pderiv{\fg^2(\theta)}{\theta^2}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
             +
             \pderiv{\fg(\theta)}{\theta}
             \pderiv{}{\theta}
             \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
          }}
     && \text{by \thme{Product Rule}}
   \\&=   \frac{-\sigma^2}{\ds\pE\brp{
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN [\fdoty_n-\fg(\theta)]
              -N
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }}
   \\&=   \frac{-\sigma^2}{\ds
              \pderiv{\fg^2(\theta)}{\theta^2}
              \sum_{n=1}^\xN \pE[\fdoty_n-\fg(\theta)]
              -\xN
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
   \\&=   \frac{-\sigma^2}{
              -\xN
              \pderiv{\fg(\theta)}{\theta}
              \pderiv{\fg(\theta)}{\theta}
           }
     && \text{because derivative of constant = 0}
   \\&=   \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2}
\end{align*}

The inequality becomes equality (an \prope{efficient} estimate)
if and only if
\[ \estML - \theta =
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t)|\fx(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\rvy(t)|\fx(t;\theta)} \right).
\]

\begin{align*}
   \left( \frac{-1}{\pE\left(
              \pderiv{^2}{\theta^2} \ln \pdfpb{\rvy(t)|\fx(t;\theta)}
           \right)} \right)
   \left(\pderiv{}{\theta} \ln \pdfpb{\rvy(t)|\fx(t;\theta)} \right)
     &= \left(
         \frac{\sigma^2}{\xN}
           \frac{1}{\left[ \pderiv{\fg(\theta)}{\theta} \right]^2} \right)
         \left(\frac{-1}{2\sigma^2}(2)\pderiv{\fg(\theta)}{\theta}
         \sum_{n=1}^\xN [\fdoty_n - \fg(\theta) ]\right)
   \\&= -\frac{1}{\xN}
         \frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\sum_{n=1}^\xN [\fdoty_n - \fg(\theta) ] \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\frac{1}{\xN}\sum_{n=1}^\xN \fdoty_n - \fg(\theta) \right)
   \\&= -\frac{1}{\pderiv{\fg(\theta)}{\theta}}
         \left(\estML - \fg(\theta) \right)
   \\&= -(\estML - \theta)
\end{align*}
\end{proof}

%======================================
\section{Colored noise}
\index{noise!colored}
%======================================
This chapter presented several theorems whose results depended on the
noise being white.
However if the noise is {\bf colored}, then these results are
invalid.
But there is still hope for colored noise.
Processing colored signals can be accomplished using two techniques:
\begin{enume}
   \item Karhunen-Lo\`{e}ve basis functions
      \footnote{{\bf Karhunen-Lo\`{e}ve}: \prefpp{sec:KL}}
   \item whitening filter
\footnote{
   \begin{tabular}[t]{ll}
      \ope{Continuous data whitening}: & \prefp{sec:whiten}  \\
      \ope{Discrete data whitening}:   & \prefp{sec:d-whiten}
   \end{tabular}
   }
\end{enume}

\paragraph{Karhunen-Lo\`{e}ve.}
If the noise is white, the set $\{\inprod{\rvy(t)}{\fpsi_n(t)}\}$
is a \prope{sufficient statistic} regardless of which
set $\{\fpsi_n(t)\}$ of orthonormal basis functions are used.
If the noise is colored, and if $\{\fpsi_n(t)\}$ satisfy the
Karhunen-Lo\`{e}ve criterion
   \[ \int_{t_2}\Rxx(t_1,t_2)\fpsi_n(t_2)\dd{t_2} = \lambda_n \fpsi_n(t_1) \]
then $\{\inprod{\rvy(t)}{\fpsi_n(t)}\}$ is still a sufficient statistic.

\paragraph{Whitening filter.}
The whitening filter makes the received signal $\rvy(t)$ statistically white
(uncorrelated in time). In this case,
any orthonormal basis set can be used to generate sufficient statistics.




%======================================
\section{Signal matching}
\index{matched filter}
%======================================
\paragraph{Detection methods.}
There are basically two types of detection methods:
\begin{enume}
   \item signal matching
   \item orthonormal decomposition.
\end{enume}

Let $\setS$ be the set of transmitted waveforms and
$\setY$ be a set of orthonormal basis functions that span $\setS$.
\hie{Signal matching} computes the innerproducts of a
received signal $\rvy(t)$ with each signal from $\setS$.
\hie{Orthonormal decomposition} computes the innerproducts of
$\rvy(t)$ with each signal from the set $\setY$.

In the case where $\seto{\setS}$ is large, often $\seto{\setY}<<\seto{\setS}$
making orthonormal decomposition much easier to implement.
For example, in a QAM-64 modulation system,
signal matching requires $\seto{\setS}=64$ innerproduct calculations,
while orthonormal decomposition only requires $\seto{\setY}=2$
innerproduct calculations because all 64 signals in $\setS$ can be spanned
by just 2 orthonormal basis functions.

\paragraph{Maximizing SNR.}
\prefpp{thm:sstat} shows that the innerproducts of $\rvy(t)$ with
basis functions of $\setY$ is \prope{sufficient} for optimal detection.
\prefpp{thm:mf_maxSNR} (next) shows that a receiver can
maximize the SNR of a received signal when signal matching is used.

%--------------------------------------
\begin{theorem}
\label{thm:mf_maxSNR}
%--------------------------------------
Let $\rvx(t)$ be a transmitted signal, $\fv(t)$ noise, and $\rvy(t)$ the received signal
in an AWGN channel.
Let the \hie{signal to noise ratio} SNR be defined as
\\\indentx$\ds
      \snr[\rvy(t)] \eqd \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                            {\pE\brs{\abs{\inprod{\fv(t)}{\rvx(t)}}^2}}.
          $
\thmboxt{
  $\ds\snr[\rvy(t)] \le \frac{2\norm{\rvx(t)}^2}{N_o }$
  \qquad
  and is maximized (equality) when $\rvx(t)=a\rvx(t)$, where $a\in\R$.
  }
\end{theorem}

\begin{proof}
\begin{align*}
   \snr[\rvy(t)]
     &\eqd \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\pE\brs{\abs{\inprod{\fv(t)}{\rvx(t)}}^2}}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{f(t)}}^2}
                {\pE\brs{\left[\int_{t\in\R} \fv(t)\rvx^\ast(t)\;dt\right]
                      \left[\int_{\estT} n(\estT)f^\ast(\estT)\;du\right]^\ast}
                }
   \\&=    \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\pE\brs{\int_{t\in\R} \int_{\estT} \fv(t)n^\ast(\estT)\rvx^\ast(t)\rvx(\estT)\;dtdu}}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{f(t)}}^2}
                {\int_{t\in\R} \int_{\estT} \pE\brs{\fv(t)n^\ast(\estT)}\rvx^\ast(t)\rvx(\estT)\;dtdu}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\int_{t\in\R} \int_{\estT} \frac{1}{2}N_o\delta(t-\estT) \rvx^\ast(t)\rvx(\estT)\;dtdu}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\frac{1}{2}N_o \int_{t\in\R} \rvx^\ast(t)\rvx(t)\dt}
   \\&=    \frac{\abs{\inprod{\rvx(t)}{\rvx(t)}}^2}
                {\frac{1}{2}N_o \norm{\rvx(t)}^2}
   \\&\le  \frac{\abs{\norm{\rvx(t)}\;\norm{\rvx(t)}}^2}
                {\frac{1}{2}N_o \norm{\rvx(t)}^2}
     &&    \text{by \thme{Cauchy-Schwarz Inequality}}
     &&    \text{\ifsxref{vsinprod}{thm:cs}}
   \\&=    \frac{2\norm{\rvx(t)}^2}
                {N_o }
\end{align*}
The Cauchy-Schwarz Inequality becomes an equality
($\snr$ is maximized) when $\rvx(t)=a\rvx(t)$.
\end{proof}

\paragraph{Implementation.}
The innerproduct operations can be implemented using either
  \begin{dingautolist}{"C0}
     \item a correlator or
     \item a matched filter.
  \end{dingautolist}

A correlator is simply an integrator of the form
   $\ds\inprod{\rvy(t)}{f(t)} = \int_0^T \rvy(t)f(t)\dt.$

A matched filter introduces a function $\fh(t)$ such that
$\fh(t) =\rvx(T-t)$ (which implies $\rvx(t)=h(T-t)$) giving
  \[
    \mcom{\inprod{\rvy(t)}{\rvx(t)} = \int_0^T \rvy(t)\rvx(t)\dt }
         {correlator}
    =
    \mcom{\brlr{\int_0^\infty \rvx(\tau)h(t-\tau)\dtau}_{t=T}
            = \brlr{\rvx(t)\conv \fh(t)}_{t=T}
         }{matched filter}.
  \]

This shows that $\fh(t)$ is the impulse response of a filter operation
sampled at time $\tau$. % (see \prefpp{fig:mf}).
By \prefpp{thm:mf_maxSNR}, the optimal impulse response is
$\fh(\tau-t)=\ff(t)=\rvx(t)$.
That is, the optimal $\fh(t)$ is just a ``flipped" and shifted version of $\rvx(t)$.




