%============================================================================
% Daniel J. Greenhoe
% LaTeX file
%============================================================================
%======================================
\chapter[Bayesian Network Algorithms]
        {Bayesian Network Algorithms when Joint-pdf Model is Known}
\label{chp:bayesnets}
%======================================
%======================================
\section{Sequential decoding}
%======================================
\begin{figure}
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(550,320)(-100,0)%
  \thicklines%
  \put( -10 , 300 ){\makebox(0,0)[r]{state $00$}}
  \put( -10 , 200 ){\makebox(0,0)[r]{state $01$}}
  \put( -10 , 100 ){\makebox(0,0)[r]{state $10$}}
  \put( -10 ,   0 ){\makebox(0,0)[r]{state $11$}}

  \thicklines
  \put(   0 ,   0 ){\circle*{10}}
  \put(   0 , 100 ){\circle*{10}}
  \put(   0 , 200 ){\circle*{10}}
  \put(   0 , 300 ){\circle*{10}}

\multiput(0,0)(100,0){5}{
  %\thicklines
  \linethickness{1mm}
  \put        (  0,300){\line( 1,-1){100}} % state0 path1
  \put        (  0,200){\line( 1,-1){100}} % state1 path1
  \put        (  0,100){\line( 1, 1){100}} % state2 path1
  \put        (  0,  0){\line( 1, 1){100}} % state3 path1

  %\thicklines
  \linethickness{0.1mm}
  %\put        (  0,300){\line( 1,-3){100}} % state0 path0
  %\put        (  0,200){\line( 1,-2){100}} % state1 path0
  %\put        (  0,100){\line( 1, 2){100}} % state2 path0
  %\put        (  0,  0){\line( 1, 3){100}} % state3 path0

  \qbezier[50](  0,300)(  0,300)(100,  0)  % state0 path0
  \qbezier[50](  0,200)(  0,200)(100,  0)  % state1 path0
  \qbezier[50](  0,100)(  0,100)(100,300)  % state2 path0
  \qbezier[50](  0,  0)(  0,  0)(100,300)  % state3 path0

  \put( 100 ,   0 ){\circle*{10}}
  \put( 100 , 100 ){\circle*{10}}
  \put( 100 , 200 ){\circle*{10}}
  \put( 100 , 300 ){\circle*{10}}
}
\end{picture}
\hspace{1cm}
\begin{tabular}{cl}
   $\cdots$ & $y_n=0$ \\
  ---     & $y_n=1$
\end{tabular}
\caption{
  Viterbi Algorithm trellis
   \label{fig:est_trellis}
   }
\end{figure}

It has been shown that the Viterbi algorithm (trellis) produces
an optimal estimate in the maximal likelihood (ML) sense.
A Verterbi trellis is shown in \prefpp{fig:est_trellis}.

%======================================
\section{References}
%======================================
\begin{enumerate}
  \item \citerg{bengal2008}{9780470018613}: Introduction to Bayesian Networks

  \item \citerg{pourret2008}{9780470994542}: A number of real-life examples including 
    \\\begin{tabular}{cll}
        \imarks & Chapter 2:    & ``Medical diagnosis"
      \\\imarks & Chapter 4:    & ``Complex genetic models"
      \\\imarks & Chapter 5:    & ``Crime risk factors analysis"
      \\\imarks & Chapter 7:    & ``Inference problems in forensic science"
      \\\imarks & Chapter 12:   & ``An information retrieval system"
      \\\imarks & Chapter 14:   & ``Terrorism risk management"
      \\\imarks & Chapter 20:   & ``Risk management in robotics"
      \\\imarks & Chapter 21:   & ``Enhancing Human Cognition"
      \\\imarks & Section 22.1: & ``An artificial intelligence perspective"
      \\\imarks & Section 22.2: & ``A rational approach of knowledge"
    \end{tabular}

  \item \citerg{darwiche2009}{9780521884389}
    \\\begin{tabular}{cll}
        \imarks & Chapter 17:   & ``Learning: The Maximum Likelihood Approach"
      \\\imarks & Chapter 18:   & ``Learning: The Bayesian Approach"
    \end{tabular}

  \item \citerg{fenton2018}{9781351978965}: Risk assessment using Bayesian Networks

  \item \citerg{jensen2013}{9781475735024}: Relationship between Decision Graphs and Bayesian Networks

  \item \citerg{chapmann2017}{9781978304871}: HMM and Bayesian Networks

  \item \citeP{fano1963}
\end{enumerate}

