%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%=======================================
\chapter{Moment Estimation}
%=======================================
%---------------------------------------
\begin{theorem}
\label{thm:mse_mean}
%---------------------------------------
Let $\ds\meanest\eqd\sum_{n=1}^\xN \lambda_n \rvx_n$ with $\sum_{n=1}^{\xN}\lambda_n=1$
be the \fncte{arithmetic mean} \xref{def:am}.

\thmbox{
  \brb{\begin{array}{FMD}
      (A).& $\seqn{\rvx_n}$ is \prope{wide sense stationary} & and
    \\(B).& $\mu\eqd\pE\rvx_n$                               & and
    \\(C).& $\seqn{\rvx_n}$ is \prope{uncorrelated}          & and
    \\(D).& \mc{2}{M}{$\ds\meanest\eqd\sum_{n=1}^\xN \lambda_n \rvx_n$\qquad(\fncte{arithmetic mean})}
  \end{array}}
  \implies
  \brb{\begin{array}{FrclDD}
      (1).& \pE\meanest    &=& \mean                               & (\prope{unbiased})   & and
    \\(2).& \var(\meanest) &=& \mc{2}{l}{\ds\sigma^2\sum_{n=1}^{\xN}\lambda_n^2 }         & and
    \\(3).& \mse(\meanest) &=& \mc{2}{l}{\ds\sigma^2\sum_{n=1}^{\xN}\lambda_n^2 }         &
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \pE\meanest
    &\eqd \pE \sum_{n\in\Z} \lambda_n\rvx_n
    && \text{by definition of \fncte{arithmetic mean}}
    && \text{\xref{def:am}}
  \\&=  \sum_{n\in\Z} \lambda_n\pE\rvx_n
    && \text{by \prope{linearity} of $\pE$}
    && \text{\xref{thm:pE_linop}}
  \\&=  \mean\sum_{n\in\Z}\lambda_n
    && \text{by \prope{WSS} hypothesis}
    && \text{(A)}
  \\&=  \mean
    && \text{by $\sum\lambda_n=1$ hypothesis}
    && \text{\xref{def:am}}
  \\
  \var(\meanest)
    &\eqd \pE\brp{\meanest-\pE\meanest}^2
    && \text{by definition of \fncte{variance}}
  \\&= \pE\brp{\meanest-\mean}^2
    && \text{by previous result}
  \\&= \pE\brp{\sum_{n=1}^{\xN}\lambda_n\rvx_n -\mean}^2
    && \text{by definition of $\meanest$}
  \\&= \pE\brs{\sum_{n=1}^{\xN}\lambda_n\rvx_n -\mean\mcom{\sum_{n=1}^{\xN}\lambda_n}{$1$}}^2
    && \text{by $\sum\lambda_n=1$ hypothesis}
    && \text{\xref{def:am}}
  \\&= \pE\brs{\sum_{n=1}^{\xN}\lambda_n(\rvx_n-\mean)}^2
  \\&= \pE\brs{\sum_{n=1}^{\xN}\lambda_n(\rvx_n-\mean)\sum_{m=1}^{\xN}\lambda_m(\rvx_m-\mean)}
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{(\rvx_n-\mean)(\rvx_m-\mean)}                                     }
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean\pE\brs{\rvx_n}-\mean\pE\brs{\rvx_m}+\mean^2   }
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean^2-\mean^2+\mean^2                             }
  \\&= \sum_{n=1}^{\xN}\sum_{m=1}^{\xN}\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean^2                                             }
  \\&= \mathrlap{
       \sum_{n=1}^{\xN} \lambda_n^2 \brp{\pE\brs{\rvx_n^2    }-\mean^2}
     + \sum_{n=1}^{\xN}\sum_{m\neq n }\lambda_n\lambda_m\brp{\pE\brs{\rvx_n\rvx_m}-\mean^2 }
       }
  \\&= \mathrlap{
       \sum_{n=1}^{\xN} \lambda_n^2 \brp{\pE\brs{\rvx_n^2    }-\mean^2                                             }
     + \sum_{n=1}^{\xN}\sum_{m\neq n  }\lambda_n\lambda_m\brp{\pE\rvx_n\pE\rvx_m  -\mean^2}
       }
  \\&= \mathrlap{
       \sum_{n=1}^{\xN} \lambda_n^2 \sigma^2
     + \cancelto{0}{
       \sum_{n=1}^{\xN}\sum_{m\neq n  }\lambda_n\lambda_m\brp{\mean\mean-\mean^2}
       }}
    && \text{by \prope{WSS} hypothesis}
    && \text{(A)}
  \\&= \sigma^2\sum_{n=1}^{\xN} \lambda_n^2
  \\
  \mse(\meanest)
    &= \pE\brp{\meanest-\pE\meanest}^2
        +\brp{\pE\meanest-\mean}^2
    && \text{by \prefp{thm:mse}}
  \\&= \sigma^2\sum_{n=1}^{\xN}\lambda_n^2 + \brp{\mean-\mean}^2
    && \text{by previous results}
  \\&= \sigma^2\sum_{n=1}^{\xN}\lambda_n^2
\end{align*}
\end{proof}

%---------------------------------------
\begin{corollary}
\label{cor:mse_average}
\footnote{
  \citerpgc{kay1988}{45}{8131733564}{\textsection\scshape``3.3 Estimation Theory"}
  }
%---------------------------------------
\corbox{
  \brb{\begin{array}{FMD}
      (A).& $\seqn{\rvx_n}$ is \prope{wide sense stationary} & and
    \\(B).& $\mu\eqd\pE\rvx_n$                               & and
    \\(C).& $\seqn{\rvx_n}$ is \prope{uncorrelated}          & and
    \\(D).& \mc{2}{M}{$\ds\meanest\eqd\frac{1}{\xN}\sum_{n=1}^\xN \rvx_n$\qquad(\fncte{average})}
  \end{array}}
  \implies
  \brb{\begin{array}{FrclDD}
      (1).& \pE\meanest    &=& \mean                & (\prope{unbiased})   & and
    \\(2).& \var(\meanest) &=& \frac{\sigma^2}{\xN} &                      & and
    \\(3).& \mse(\meanest) &=& \frac{\sigma^2}{\xN} & (\prope{consistent})
  \end{array}}
  }
\end{corollary}
\begin{proof}
These results follow from \prefpp{thm:mse_mean} with $\lambda_n=\frac{1}{\xN}$.
\end{proof}

What is the best sequence $\seqn{\lambda_n}$ to use? 
The next lemma shows that $\lambda_n=\frac{1}{\xN}$, and hence 
of all the possible $\seqn{\lambda_n}$ of the \fncte{arithmetic mean} estimators \xref{thm:mse_mean},
the \fnctb{average} estimator (with $\lambda_n=1/\xN$) \xref{cor:mse_average}, 
leads to the lowest $\mse$ (the best estimator).
%---------------------------------------
\begin{lemma}
\label{lem:lambda_sq}
%---------------------------------------
\lembox{
  \brb{\sum_{n=1}^{\xN} \lambda_n=1}
  \implies
  \sum_{n=1}^{\xN} \lambda_n^2 \ge \frac{1}{\xN}
  }
\end{lemma}
\begin{proof}
\url{http://faculty.wwu.edu/sarkara/ph4.pdf}
\begin{enumerate}
  \item Let the sequence $\seqn{a_n}$ be defined as $\seqn{a_n}\eqd\seqn{\ldots,1,1,1,\ldots}$
        \label{idef:lambda_sq_an}
  \item Let \fncte{inner product} $\inprod{a_n}{b_n}$ be defined as $\inprod{a_n}{b_n}\eqd\sum_{n=1}^{\xN}a_n b_n$
        \label{idef:lambda_sq_inprod}
  \item Let \fncte{norm} $\norm{a_n}$ be defined as $\norm{a_n}\eqd\sum_{n=1}^{\xN}a_n^2$
        \label{idef:lambda_sq_norm}
  \item Proof of lemma:
        \begin{align*}
          \boxed{\frac{1}{\xN}}
            &= \frac{1}{\xN}\brp{\sum_{n=1}^{\xN} \lambda_n}^2
            && \text{by $\sum_{n=1}^{\xN} \lambda_n=1$ hypothesis}
          \\&= \frac{1}{\xN}\brp{\sum_{n=1}^{\xN} a_n\lambda_n}^2
            && \text{by $\seqn{a_n}\eqd\seqn{\ldots,1,1,1,\ldots}$ definition}
            && \text{\xref{idef:lambda_sq_an}}
          \\&\eqd \frac{1}{\xN}\inprod{a_n}{\lambda_n}^2
            && \text{by definition of $\inprodn$}
            && \text{\xref{idef:lambda_sq_inprod}}
          \\&\boxed{\leq} \frac{1}{\xN}\norm{a_n}^2\norm{\lambda_n}^2
            && \text{by \ineqe{Cauchy-Schwarz Inequality}}
            && \text{\xref{thm:cs}}
          \\&\eqd \frac{1}{\xN}\brp{\sum_{n=1}^{\xN} a_n^2} \brp{\sum_{n=1}^{\xN} \lambda_n^2}
            && \text{by definition of $\normn$}
            && \text{\xref{idef:lambda_sq_norm}}
          \\&\eqd \frac{1}{\xN}\brp{\sum_{n=1}^{\xN} 1^2} \brp{\sum_{n=1}^{\xN} \lambda_n^2}
            && \text{by definition of $\seqn{a_n}$}
            && \text{\xref{idef:lambda_sq_an}}
          \\&= \boxed{\sum_{n=1}^{\xN} \lambda_n^2}
        \end{align*}
\end{enumerate}
\end{proof}
