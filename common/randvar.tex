%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%======================================
\chapter{Operations on Random Variables}
\label{chp:rval}
%======================================

%=======================================
\section{Functions of one random variable}
%=======================================
%---------------------------------------
\begin{proposition}
\label{prop:cdf_uniform}
%---------------------------------------
Let $\ps$ be a probability space, and $\rvX$ a \fncte{random variable}
with \fncte{cumulative distribution function} $\pcx(x)$ \xref{def:cdf}.
\propbox{
  \brb{\begin{array}{M}
    $\rvX$ is \prope{uniformly distributed}\\
    \xref{def:uniform}
  \end{array}}
  \iff
  \pcx(x) = \brbl{\begin{array}{cM}
                   0   & for $x<0$\\
                   x   & for $0\le x \le 1$ \\
                   1   & otherwise
            \end{array}}
  }
\end{proposition}

%---------------------------------------
\begin{theorem}[\thmd{Probability integral transform}]
\footnote{
  \citeP{angus1994},
  \citerpgc{roussas2014}{232}{0128004371}{Theorem 10},
  \citerpgc{devroye1986}{28}{0387963057}{Theorem 2.1}
  }
\label{thm:pit}
%---------------------------------------
Let $\ps$ be a probability space.
Let $\rvX$ be a \fncte{random variable} with \fncte{probability density function} $\ppx(x)$ \xref{def:pdf}
and \fncte{cumulative distribution function} $\pcx(x)$.
Let $\rvY$ be a \fncte{random variable} \fncte{cumulative distribution function} $\pcy(y)$.
\thmbox{
  \brb{\begin{array}{FMD}
         (1).&$\rvY=\pcx(\rvX)$               & and\\
         (2).&$\ppx(x)$ is \prope{continuous} &    \\
       \end{array}}
  \quad\implies\quad
  \brb{\begin{array}{M}
    $\rvY$ is \prope{uniformly distributed}\\
    \xref{def:uniform}
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \pcy(y)
    &\eqd \psp\setn{\rvY\le y}
    &&    \text{by definition of \fncte{cdf}}   && \text{\xref{def:cdf}}
  \\&=    \psp\setn{\pcx(\rvX)\le y}
    &&    \text{by hypothesis (1)}
  \\&=    \psp\setn{\rvX\le \pcx^{-1}(y)}
    &&    \text{by hypothesis (2) and}          && \text{\prefp{prop:cdf_monotone}}
  \\&\eqd \pcx\brs{\pcx^{-1}(y)}
    &&    \text{by definition of \fncte{cdf}}   && \text{\xref{def:cdf}}
  \\&=    y
  \\\implies&\quad\text{$\rvY$ is \prope{uniformly distributed}}
    &&    \text{by}                             && \text{\prefp{prop:cdf_uniform}}
\end{align*}
\end{proof}

\begin{figure}
  \centering
  $\ds\begin{array}{|ccccc|}
    \hline
    \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/n1.pdf}}}
    &\quad\rightarrow\quad&
    \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/n1_int.pdf}}}
    &\quad\rightarrow\quad&
    \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/n0.pdf}}}
    \\
    \text{\fncte{pdf} $\ppx(x)$ of $\rvX$}
    &&
    \ff(x)=\pcx(x)
    &&
    \text{\fncte{pdf} $\ppy(x)$ of $\rvY\eqd\ff(\rvX)$}
    \\\hline
  \end{array}$
  \caption{Flow diagram for \prefpp{ex:pit}\label{fig:pit_N1}}
\end{figure}
%---------------------------------------
\begin{example}
\label{ex:pit}
%---------------------------------------
\exboxt{
\begin{minipage}{\tw-58mm}
Let $\rvX$ be a random variable with \fncte{pdf} \xref{def:pdf} $\ppx(x)$ where
$\ppx(x)$ is the triangle-like \fncte{first order B-spline} $\fN_1(x)$ \xref{ex:bspline_N1}.
%(a triangle-like function with height=1, peak at $x=1$, and support $x\in\intcc{0}{2}$).
Then the random variable $\rvY\eqd\pcx(\rvX)$ has \prope{uniform distribution} \xref{def:uniform} where
$\pcx(x)$ is the \fncte{cdf} \xref{def:cdf} of $\rvX$.
\end{minipage}%
\tbox{\includegraphics{../common/math/graphics/pdfs/n1.pdf}}%
}
\\\indentx$\ds
\pcx(x) = \brbl{\begin{array}{r@{}lM}
                   &0                      & for $    x \le 0$\\
                   &\frac{1}{2}x^2         & for $0 < x \le 1$\\
                  -&\frac{1}{2}x^2 +2x - 1 & for $1 < x \le 2$\\
                   &1                      & for $    x >   2$
                \end{array}}
$\qquad\tbox{\includegraphics{../common/math/graphics/pdfs/n1_int.pdf}}%
\\See \prefpp{fig:pit_N1} for an illustration.
Here is some R code if you want to try out the concept on the bench:\footnotemark
\footnotetext{Note: To understand why $\rvX\eqd\rvX1+\rvX2$ has triangle distribution $\fN_1(x)$,
see \exme{Sum of Uniformly Distributed Random Variables} \prefp{ex:pdf_uniform_sums}.}
\begin{lstlisting}[language=R]
N  = 1e6                                                      # N = number of samples
X1 = runif( n=N, min=0, max=1 )                               # X1 has uniform distribution
X2 = runif( n=N, min=0, max=1 )                               # X2 has uniform distribution
X  = X1 + X2                                                  # X  has triangular distribution N1(x)
Y  = ifelse( X<1, X^2/2, -X^2/2+2*Y-1)                        # Y = cdf_x( X ) has uniform distribution
plot( density( Y, bw="SJ" ), lwd=3, xlim=c(0,4), col="blue" ) # plot estimated pdf of Y
\end{lstlisting}
\end{example}
\begin{proof}
\begin{align*}
  \pcx(x)
    &\eqd \int_{-\infty}^x \ppx(u) \du
    && \text{by definition of $\pcx(x)$ \xref{def:cdf}}
  \\&\eqd \int_{-\infty}^x \fN_1(u) \du
    && \text{by definition of $\ppx(x)$}
  \\&= \int_{-\infty}^x
         \brb{\begin{array}{r@{}lM}
            &u     & for $u\in\intcc{0}{1}$\\
           -&u + 2 & for $u\in\intoc{1}{2}$\\
            &0     & otherwise
         \end{array}} \du
    && \text{by definition of $\fN_1(x)$ \xref{ex:bspline_N1}}
  \\&= \brbl{\begin{array}{>{\ds}lM}
         0                   & for $    x \le 0$\\
         \int_0^x  u     \du & for $0 < x \le 1$\\
         \int_1^x -u + 2 \du & for $1 < x \le 2$\\
         1                   & for $    x >   2$
       \end{array}}
   && = \brbl{\begin{array}{>{\ds}lM}
                0                                           & for $    x \le 0$\\
                \brs{\frac{1}{2}u^2}_0^x                    & for $0 < x \le 1$\\
                \frac{1}{2} + \brs{-\frac{1}{2}u^2 +2u}_1^x & for $1 < x \le 2$\\
                1                                           & for $    x >   2$
              \end{array}}
  \\& = \brbl{\begin{array}{lM}
                0                                                                   & for $    x \le 0$\\
                \frac{1}{2}x^2 - \frac{1}{2}0^2                                     & for $0 < x \le 1$\\
                \frac{1}{2} + \brp{-\frac{1}{2}x^2 +2x} - \brp{-\frac{1}{2}1^2 + 2} & for $1 < x \le 2$\\
                1                                                                   & for $    x >   2$
              \end{array}}
   && = \brbl{\begin{array}{r@{}lM}
                &0                       & for $    x \le 0$\\
                &\frac{1}{2}x^2          & for $0 < x \le 1$\\
               -&\frac{1}{2}x^2 +2x - 1  & for $1 < x \le 2$\\
                &1                       & for $    x >   2$
              \end{array}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{theorem}[\thmd{Inverse probability integral transform}]
\footnote{
  \citerpgc{devroye1986}{28}{0387963057}{Theorem 2.1},
  \citerpgc{balakrishnan2009}{624}{0387096140}{14.2.1 Introduction}
  }
\label{thm:its}
%---------------------------------------
Let $\ps$ be a probability space.
Let $\rvX$ be a \fncte{random variable} with \fncte{probability density function} $\ppx(x)$ and \fncte{cumulative distribution function} $\pcx(x)$.
Let $\rvY$ be a \fncte{random variable} \fncte{cumulative distribution function} $\pcy(y)$.
\thmbox{
  \brb{\begin{array}{FMD}
         (1).&$\rvY=\pcz^{-1}(\rvX)$                     & and\\
         (2).&$\rvX$    is \prope{uniformly distributed} & and\\
         (3).&$\ppz(z)$ is \prope{continuous}            &
       \end{array}}
  \quad\implies\quad
  \brb{\begin{array}{M}
    $\ppy(y) = \ppz(y)$\\
    ($\rvY$ has distribution $\ppz(y)$)
  \end{array}}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \pcy(y)
    &\eqd \psp\setn{\rvY\le y}
    &&    \text{by definition of $\pcy$}          &&    \text{\xref{def:cdf}}
  \\&=    \psp\setn{\pcz^{-1}(\rvX)\le y}
    &&    \text{by hypothesis (1)}
  \\&=    \psp\setn{\rvX \le \pcz(y)}
    &&    \text{by hypothesis (3) and}            &&    \text{\prefp{prop:cdf_monotone}}
  \\&\eqd \pcx\brs{\pcz(y)}
    &&    \text{by definition of $\pcx$}          &&    \text{\xref{def:cdf}}
  \\&=    \pcz(y)
    &&    \text{because $0\le\pcz(y)\le1$ and by} && \text{\prefp{prop:cdf_uniform}}
  \\\implies&\quad\text{$\ppy(y)=\ppz(y)$}
    &&    \text{($\rvY$ has the distribution of $\rvZ$)}
\end{align*}
\end{proof}

\begin{figure}\color{figcolor}
\setlength{\unitlength}{0.4mm}
\thicklines
\begin{center}
\begin{footnotesize}
\begin{picture}(250,150)(-100,-20)
  {\color{axis}% axis
    \put(-100,   0){\line(1,0){200}}%
    \put(   0, -20){\line(0,1){120}}%
    }
  {\color{blue}% f(x)
    \qbezier(-100,100)(0,-100)(100,100)%
    \put( 100, 105){\makebox(0,0)[b]{$y=\ff(x)$}}%
    }%
  {\color{red}% dotted guide lines
    \qbezier[8](-40,0)(-40,8)(-40,16)%
    \qbezier[8](40,0)(40,8)(40,16)%
    \qbezier[28](-80,0)(-80,32)(-80,64)%
    \qbezier[28](80,0)(80,32)(80,64)%
    \qbezier[64](-80,64)(0,64)(80,64)%
    \qbezier[40](-40,16)(0,16)(40,16)%
    }%
  \put(   0, 110){\makebox(0,0)[r]{$y$}}
  \put( 110,   0){\makebox(0,0)[r]{$x$}}
  \put(  -5,  64){\makebox(0,0)[r]{$y+\varepsilon$}}
  \put(  -5,  16){\makebox(0,0)[r]{$y$}}
  \put( -40,  -5){\makebox(0,0)[t]{$\fgi_1(y)$}}
  \put(  40,  -5){\makebox(0,0)[t]{$\fgi_2(y)$}}
  \put( -80,  -5){\makebox(0,0)[t]{$\fgi_1(y+\varepsilon)$}}
  \put(  80,  -5){\makebox(0,0)[t]{$\fgi_2(y+\varepsilon)$}}
  \put(-100,  40){\makebox(0,0)[r]{$\left.\frac{\dy}{\dx}\right|_{x=\fgi_1(y)}$}}
  \put( 100,  40){\makebox(0,0)[l]{$\left.\frac{\dy}{\dx}\right|_{x=\fgi_2(y)}$}}
  \put(  95,  40){\vector(-1,0){35}}%
  \put( -95,  40){\vector(1,0){35}}%
  %\put(-100,  20){\makebox(0,0)[r]{$\fgi_1(y) + \left.\frac{\Delta y}{\dy/\dx}\right|_{x=\fgi_1(y)} = \fgi_1(y) + \frac{\varepsilon}{\ffp\brs{\fgi_1(y)}}$}}%
  \put(-100,  20){\makebox(0,0)[r]{$\fgi_1(y) + \frac{\varepsilon}{\ffp\brs{\fgi_1(y)}}$}}%
  \put( 100,  20){\makebox(0,0)[l]{$\fgi_2(y) + \left.\frac{\Delta y}{\dy/\dx}\right|_{x=\fgi_2(y)} = \fgi_2(y) + \frac{\varepsilon}{\ffp\brs{\fgi_2(y)}}$}}%
  \put( -95,  15){\vector( 1,-1){15}}
  \put(  95,  15){\vector(-1,-1){15}}
  \put( -60,   0){$\setA_1$}%
  \put(  60,   0){$\setA_2$}%
  {\color{red}% slope lines
    \put(  40,  16){\line(5,6){40}}%   %straight line
    \put( -40,  16){\line(-5,6){40}}%   %straight line
    }%
  %{\color{green}\qbezier(40,16)(60,40)(80,64)}     %straight line
\end{picture}
\end{footnotesize}
\end{center}
\caption{
  $\rvY=\ff(\rvX)$
  \label{fig:YfX}
  }
\end{figure}

\begin{figure}
  \centering
  $\ds\begin{array}{|ccccc|}
    \hline
    \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/n0.pdf}}}
    &\quad\rightarrow\quad&
    \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/n0_2x1_int_inverse.pdf}}}
    &\quad\rightarrow\quad&
    \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/n0_2x1.pdf}}}
    \\
    \text{\fncte{pdf} $\ppx(x)$ of $\rvX$}
    &&
    \ff(x)=\pcy^{-1}(x)
    &&
    \text{\fncte{pdf} $\ppy(x)$ of $\rvY\eqd\ff(\rvX)$}
    \\\hline
  \end{array}$
  \caption{Flow diagram for \prefpp{ex:ppy_2x1}\label{fig:ppy_2x1}}
\end{figure}
%---------------------------------------
\begin{example}
\label{ex:ppy_2x1}
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} with \prope{uniform distribution}.
Select a function $\ff(x)$ such that $\rvY\eqd\ff(\rvX)$ has distribution
\\\indentx$\ds\ppy(y) \eqd \brbl{\begin{array}{lM}
                              \frac{1}{2} & for $y\in\intoc{1}{3}$\\
                              0           & otherwise
                            \end{array}}$
\exbox{
  \ff(x) \eqd 2x + 1
  \qquad\implies\qquad
  \rvY \eqd \ff(\rvX)\qquad\text{has distribution $\ppy(y)$}
}
\\\ldots as illustrated in \prefpp{fig:ppy_2x1}.
Here's some R code demonstrating the concept: % (the ``blue" plot should look approximately like $\fN_1(x)$):
\begin{lstlisting}[language=R]
N = 1e6                                                       # Number of samples
X = runif( n=N, min=0, max=1 )                                # X = Uniformly distributed RV
Y = 2 * X + 1                                                 # Y = f( X )
plot( X, Y, ylim=c(0,3), col="red" )                          # plot X -> Y mapping
plot( density( Y, bw="SJ" ), lwd=3, xlim=c(0,4), col="blue" ) # plot estimated pdf of Y
\end{lstlisting}
\end{example}
\begin{proof}
\begin{enumerate}
  \item The \fncte{cumulative distribution function} $\pcy(y)$ of $\rvY$ with desired pdf $\ppy(y)$ is
    \begin{align*}
      \pcy(y)
        &  \eqd \int_{-\infty}^y \ppy(u) \du
        &&    \text{by definition of cdf} &&\text{\xref{def:cdf}}
      \\&  \eqd \int_{-\infty}^y \brb{\begin{array}{lM}
                                        \frac{1}{2} & for $u\in\intcc{1}{3}$\\
                                        0           & otherwise
                                      \end{array}} \du
        &&    \text{by definition of $\ppy(y)$}
      \\&= \brb{\begin{array}{lM}
             0            & for $y\le1$\\
             \frac{1}{2}y & for $1<y\le 3$\\
             1            & otherwise
           \end{array}}
    \end{align*}

  \item The inverse cdf $\pcy^{-1}(x)$ is
    $\ds\pcy^{-1}(x) = \brbl{\begin{array}{lMM}
                               \intcc{0}{1} & for $x=0$     & (undefined but in $\intcc{0}{1}$)\\
                               2x + 1       & for $0<x\le 1$\\
                               $undefined$  & otherwise
                             \end{array}}$

  \item Since we are using $\pcy^{-1}(x)$ with $\rvX$ which only yields values of non-zero probablity in $\intoc{0}{1}$ \xref{def:uniform},
        we can simplify $\ff(x)$ to
        only necessarily match $\pcy^{-1}$ in the domain $\intoc{0}{1}$, and be whatever is convenient elsewhere.
        As such, let $\ds \ff(x) \eqd 2x + 1$.

  \item By the \thme{Inverse probability integral transform} \xref{thm:its}, $\rvY\eqd\ff(\rvX)\eqd 2\rvX + 1$ has the
        desired pdf $\ppy(y)$.

  \item Note: For an alternative proof using \prefpp{cor:YaXb}
              rather than the \thme{Inverse probability integral transform} \xref{thm:its},
              see \prefpp{ex:ppy_2x1b}.

\end{enumerate}
\end{proof}

\begin{figure}
  \centering
  $\ds\begin{array}{|ccccc|}
    \hline
    \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/n0.pdf}}}
    &\quad\rightarrow\quad&
    \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/n1_int_inverse.pdf}}}
    &\quad\rightarrow\quad&
    \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/n1.pdf}}}
    \\
    \text{\fncte{pdf} $\ppx(x)$ of $\rvX$}
    &&
    \ff(x)=\pcy^{-1}(x)
    &&
    \text{\fncte{pdf} $\ppy(x)$ of $\rvY\eqd\ff(\rvX)$}
    \\\hline
  \end{array}$
  \caption{Flow diagram for \prefpp{ex:ppy_N1}\label{fig:ppy_N1}}
\end{figure}
%---------------------------------------
\begin{example}
\label{ex:ppy_N1}
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} with \prope{uniform distribution}.
Select a function $\ff(x)$ such that $\rvY\eqd\ff(\rvX)$ has distribution $\fN_1(x)$
as defined in \prefp{ex:pit} (triangle-like 1st order B-spline). Then\ldots
\exbox{
  \tbox{\includegraphics{../common/math/graphics/pdfs/n1_int_inverse.pdf}}%
  \begin{minipage}{\tw-42mm}
    $\ds
      \brs{\begin{array}{rcl}
        \ff(x) &\eqd& \brbl{\begin{array}{lM}
                             \sqrt{2x}         & for $x\in\intcc{0}{\frac{1}{2}}$\\
                             2 - \sqrt{2(1-x)} & for $x\in\intcc{\frac{1}{2}}{1}$\\
                             \text{undefined}  & otherwise
                           \end{array}}
      \end{array}}
      \quad\implies\quad
      \brs{\begin{array}{M}
        $\rvY \eqd \ff(\rvX)$\\
        has \fncte{pdf}\\
        $\fN_1(x)$
      \end{array}}
    $
  \end{minipage}
}
\\\ldots as illustrated in \prefpp{fig:ppy_N1}.
Here's some R code demonstrating the concept (the ``blue" plot should look approximately like $\fN_1(x)$):
\begin{lstlisting}[language=R]
N = 1e6                                                       # Number of samples
X = runif( n=N, min=0, max=1 )                                # X = Uniformly distributed RV
Y = ifelse( X<0.5, sqrt(2*X), (2-sqrt(2*(1-X))) )             # Y = f( X )
plot( X, Y, col="red" )                                       # plot X -> Y mapping
plot( density( Y, bw="SJ" ), lwd=3, xlim=c(0,4), col="blue" ) # plot estimated pdf of Y
\end{lstlisting}
\end{example}
\begin{proof}
\begin{enumerate}
  \item $\pcx(x)\eqd y = \frac{1}{2}x^2$ $\implies$ $x = \pm\sqrt{2y}$ $\implies$ $\pcx^{-1}(x) = \pm\sqrt{2x}$
  \item $\pcx(x)\eqd y = -\frac{1}{2}x^2 + 2x - 1$ $\implies$ $\pcx^{-1}(x) = 2 \pm \sqrt{ 2(1-x) }$
    \begin{align*}
      y = -\frac{1}{2}x^2 + 2x - 1 \quad\implies\quad
      x &= \frac{-2 \pm \sqrt{ 2^2 - 4\brp{-\frac{1}{2}}\brp{-y-1}}}{2\brp{-\frac{1}{2}}}
        && \text{by the \thme{Quadratic Equation}}
      \\&= 2 \pm \sqrt{ 4 - 4\brp{-\frac{1}{2}}\brp{-y-1}}
      \\&= 2 \pm \sqrt{ 4 - 2\brp{y+1}}
      \\&= 2 \pm \sqrt{ 2(1 - y)}
    \end{align*}
  \item Taking (1) and (2) above and the fact that the \fncte{cdf} is always \prope{non-negative}, we have
    \begin{align*}
      \ff(x) \eqd \pcx^{-1}(x)
        &= \brbl{\begin{array}{lM}
                  \sqrt{2x}         & for $x\in\intoc{0}{\frac{1}{2}}$\\
                  2 - \sqrt{2(1-x)} & for $x\in\intcc{\frac{1}{2}}{1}$
                \end{array}}
    \end{align*}
  \item By the \thme{Inverse probability integral transform} \xref{thm:its}, $\rvY\eqd\ff(\rvX)$ has the
        desired pdf $\fN_1(x)$.

\end{enumerate}
\end{proof}

%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{callahan2010}{189}{144197332X}{Definition 6.1}
  }
\label{def:cp}
%---------------------------------------
Let $\ff(x)$ be a \structe{differentiable function} in $\clFrr$.
\defboxt{
  A point $p\in\R$ is a \propd{critical point} of $\ff(x)$ if
  \\\indentx$\ffp(p)=0$.
  }
\end{definition}

%---------------------------------------
\begin{theorem}
\footnote{
  \citerppgc{papoulis1984}{95}{96}{0070484686}{``Fundamental Theorem"},
  \citerpgc{papoulis1990}{157}{0137116985}{``Fundamental Theorem"},
  \citerp{papoulis}{93},
  \citerpgc{haykin1994}{235}{239}{0471571768}{\textsection ``{\scshape 4.5 Transformations of Random Variables}"},
  \citerp{proakis}{30},
  }
\label{thm:YfX}
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s in $\clFrr$.
Let $\ff$ be a \structe{differentiable function} in $\clFrr$
with $\xN$ \prope{critical point}s \xref{def:cp}.
Let the range of $\rvX$ be partitioned into $\xN+1$ partitions
$\set{\setA_n}{n=1,2,\ldots,\xN+1}$
with partition boundaries set at the $\xN$ \structe{critical point}s of $\ff(x)$%
---as illustrated in \prefpp{fig:YfX}.
Let $\fg_n(x)\eqd\ff(x)$ but with domain restricted to $x\in\setA_n$.
\thmbox{
  \brb{\begin{array}{FMD}
    (1).&$\rvY=\ff(\rvX)$  & and \\
    (2).&$\ff$ is \prope{differentiable}
  \end{array}}
  \implies
  \brb{\ppy(y) = \sum_{n=1}^{\xN+1} \frac{\ppx\brp{\fgi_n(y)}}{\abs{\ffp\brp{\fgi_n(y)}}}}
  }
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item \label{item:YfX_gn}
        The problem with a function $\ff(x)$ with at least $\xN=1$ critical point is that
        $\ffi(y)$ is \prope{not invertible}.
        That is,
        $\ffi(y)$ has more than one solution (and thus the \structe{relation} $\ffi(y)$ is not a \structe{function}).
        However, note that in each partition $\setA_n$, $\ff(x)$ is \prope{invertible} and thus
        $\ffi(y)$ in that partition has a \prope{unique} solution.
        Thus, each $\fg_n(x)$ \emph{is} \prop{invertible} in it's domain (and each $\fgi_n(y)$ exists as a function).

  %\item lemma. \label{ilem:YfX_ffp}  \label{ilem:YfX_dygi}
  %  \\\indentx$\ds
  %    \lim_{\varepsilon\to0} \fgi_n(y+\varepsilon)
  %       = \lim_{\varepsilon\to0}\brs{\fgi_n(y) + \left.\Delta y\frac{1}{\dy/\dx}\right|_{x=\fgi_n(y)}}
  %       = \lim_{\varepsilon\to0}\brs{\fgi_n(y) + \frac{\varepsilon}{\ffp\brs{\fgi_n(y)}}}
  %      $

  \item Using \pref{item:YfX_gn}, the remainder of the proof follows \ldots
    \begin{align*}
      \ppy(y)
        &\eqd \ddy\psp\setn{\rvY \le y}
        && \text{by definition of $\ppy$ \xref{def:pdf}}
      \\&= \ddy\psp\setn{\ff(\rvX)\le y}
        && \text{by hypothesis (1)}
      \\&= \ddy\sum_{n=1}^{\xN+1} \psp\setn{\brs{\ff(\rvX)\le y}\land\brs{\rvX\in\setA_n}}
        && \text{by \thme{sum of products} \xref{thm:psp_sop}}
      \\&= \ddy\sum_{n=1}^{\xN+1} \pPc{\ff(\rvX)\le y}{\rvX\in\setA_n}\psp\setn{\rvX\in\setA_n}
        && \text{by definition of $\psp\setn{\setX|\setY}$ \xref{def:conP}}
      \\&= \ddy\sum_{n=1}^{\xN+1} \pPc{\fg_n(\rvX)\le y}{\rvX\in\setA_n}\psp\setn{\rvX\in\setA_n}
        && \text{by definition of $\fg_n(x)$}
      \\&= \mathrlap{\ddy\brb{\begin{array}{>{\ds}lM}
            \sum_{n=1}^{\xN+1} \pPc{\rvX \le \fgi_n(y)}{\rvX\in\setA_n}\psp\setn{\rvX\in\setA_n} & for $\ffp(x)\ge0$ \\
            \sum_{n=1}^{\xN+1} \pPc{\rvX \ge \fgi_n(y)}{\rvX\in\setA_n}\psp\setn{\rvX\in\setA_n} & otherwise
           \end{array}}\quad\text{by \pref{item:YfX_gn}}}
      \\&= \mathrlap{\ddy\brb{\begin{array}{>{\ds}lM}
            \sum_{n=1}^{\xN+1} \pPc{\rvX \le \fgi_n(y)}{\rvX\in\setA_n}\psp\setn{\rvX\in\setA_n}          & for $\ffp(x)\ge0$ \\
            \sum_{n=1}^{\xN+1} \brs{1-\pPc{\rvX \le \fgi_n(y)}{\rvX\in\setA_n}\psp\setn{\rvX\in\setA_n}}  & otherwise
           \end{array}}}
      \\&= \mathrlap{\ddy\brb{\begin{array}{>{\ds}lM}
            \sum_{n=1}^{\xN+1} \psp\set{\rvX \le \fgi_n(y)}{\rvX\in\setA_n}          & for $\ffp(x)\ge0$ \\
            \sum_{n=1}^{\xN+1} \brs{1-\psp\set{\rvX \le \fgi_n(y)}{\rvX\in\setA_n}}  & otherwise
           \end{array}}\quad\text{by definition of $\psp\setn{\setX|\setY}$}}
      \\&= \mathrlap{\ddy\brb{\begin{array}{>{\ds}lM}
            \sum_{n=1}^{\xN+1} \brp{\psp\setn{\rvX \le \fgi_n(y)}-\psp\setn{\rvX < \min\setA_n}}        & for $\ffp(x)\ge0$ \\
            \sum_{n=1}^{\xN+1} \brs{1-\brp{\psp\setn{\rvX \le \fgi_n(y)}-\psp\setn{\rvX<\min\setA_n}}}  & otherwise
           \end{array}}}
      \\&= \ddy\brb{\begin{array}{>{\ds}lM}
            \sum_{n=1}^{\xN+1} \psp\setn{\rvX \le \fgi_n(y)}          & for $\ffp(x)\ge0$ \\
            \sum_{n=1}^{\xN+1} \brs{1-\psp\setn{\rvX \le \fgi_n(y)}}  & otherwise
           \end{array}}
        && \text{because $\ddy\psp\setn{\rvX<\text{constant}}=0$}
      \\&= \brb{\begin{array}{>{\ds}lM}
            \sum_{n=1}^{\xN+1} \ddy\pcx\brs{\fgi_n(y)}         & for $\ffp(x)\ge0$ \\
            \sum_{n=1}^{\xN+1} \ddy\brs{1-\pcx\brp{\fgi_n(y)}} & otherwise
           \end{array}}
        && \text{by \prope{linearity} of $\ddy$ operator}
      \\&= \brb{\begin{array}{>{\ds}lM}
            \sum_{n=1}^{\xN+1}       \ppx\brs{\fgi_n(y)}\ddy\brs{\fgi_n(y)}   & for $\ffp(x)\ge0$ \\
            \sum_{n=1}^{\xN+1} \brs{-\ppx\brs{\fgi_n(y)}\ddy\brs{\fgi_n(y)}}  & otherwise
           \end{array}}
        && \text{\begin{tabular}[t]{@{}l}by definition of $\ppx$ \xref{def:pdf}\\and the \thme{chain rule}\end{tabular}}
      \\&= \sum_{n=1}^{\xN+1} \ppx\brp{\fgi_n(y)}\abs{\ddy\brs{\fgi_n(y)}}
      \\&= \sum_{n=1}^{\xN+1} \frac{\ppx\brp{\fgi_n(y)}}{\abs{\ffp\brp{\fgi_n(y)}}}
        && \text{by \prefp{lem:ddyffi}}
    \end{align*}

\end{enumerate}
\end{proof}

%---------------------------------------
\begin{corollary}
\footnote{
  \citerpgc{papoulis1984}{96}{0070484686}{``Illustrations" 1},
  \citerp{papoulis}{95},
  \citerp{proakis}{29}
  }
\label{cor:YaXb}
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s in $\clFrr$.
Let $a,b\in\R$.
\corbox{
  \brb{\begin{array}{FlCD}
    (1).&\rvY=a\rvX+b & and \\
    (2).&a\neq0       &
  \end{array}}
  \implies
  \brb{
  \ppy(y) = \mcom{\frac{1}{\abs{a}} \ppx\brp{\frac{y-b}{a}}}
                 {\ope{Dilation} by $a$ and \ope{Translation} by $b$}
  }}
\end{corollary}
\begin{proof}
%\begin{enumerate}
%  \item This follows from \prefpp{thm:YfX}:
    \begin{enumerate}
      \item \label{item:YaXb_ffp}
            Note that $\ff(x)=ax+b$ is a \prope{differentiable function} with $\xN=0$ \prope{critical point}s
            and $\ffp(x)=a$.
      \item \label{item:YaXb_ffi}
            The inverse of $\ff(x)$ is $\fg_1(y)=\ffi(y)=\frac{y-b}{a}$.
      \item It follows that
        \begin{align*}
          \ppy(y)
            &= \sum_{n=1}^{\xN+1} \frac{\ppx\brp{\fgi_n(y)}}{\abs{\ff'\brp{\fgi_n(y)}}}
            && \text{by \prefpp{thm:YfX}}
          \\&= \frac{\ppx\brp{\ffi(y)}}{\abs{\ff'\brp{\ffi(y)}}}
            && \text{because $\xN=0$}
          \\&= \frac{\ppx\brp{\ffi(y)}}{\abs{a}}
            && \text{by \pref{item:YaXb_ffp}}
          \\&= \frac{1}{\abs{a}}\ppx\brp{\frac{y-b}{a}}
            && \text{by \pref{item:YaXb_ffi}}
        \end{align*}
    \end{enumerate}

\end{proof}

%---------------------------------------
\begin{example}
\label{ex:ppy_2x1b}
%---------------------------------------
Revisit \prefpp{ex:ppy_2x1}, but this time using \prefpp{cor:YaXb}
rather than the \thme{Inverse probability integral transform} \xref{thm:its}:
Suppose we have a random generator that yields a value $\rvX$ with
\prope{uniform distribution}.
Choose a function $\ff(x)$ such that $\rvY\eqd\ff(\rvX)$ has distribution
\\\indentx$\ds\ppy(y) \eqd \brbl{\begin{array}{lM}
                              \frac{1}{2} & for $y\in\intoc{1}{3}$\\
                              0           & otherwise
                            \end{array}}$
\exbox{
  \ff(x) \eqd 2x + 1
  \qquad\implies\qquad
  \rvY \eqd \ff(\rvX)\qquad\text{has distribution $\ppy(y)$}
}
\end{example}
\begin{proof}
\begin{align*}
  \ppy(y)
    &\eqd \brbl{\begin{array}{lM}
                  \frac{1}{2} & for $y\in\intoc{1}{3}$\\
                  0           & otherwise
                \end{array}}
  \\&= \frac{1}{2} \ppx\brp{\frac{x-1}{2}}
    && \text{by definition of \fncte{uniform distribution}}
    && \text{\xref{def:uniform}}
  \\\implies \ff(x)
     &= 2x + 1
    && \text{by \prefp{cor:YaXb}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{corollary}
\footnote{
  \citerpgc{papoulis1984}{97}{0070484686}{Example 5-10},
  \citerp{papoulis}{94}
  }
\label{cor:Yf1X}
%---------------------------------------
\corbox{
  \brb{\rvY=\frac{1}{\rvX}}
  \implies
  \brb{\ppy(y) = \begin{array}{lM}
     %                                             & for $y<0$ \\
     %0                                            & for $y=0$ \\
     \frac{1}{y^2} \ppx\left( \frac{1}{y} \right) & for $y>0$
  \end{array}}
  }
\end{corollary}
\begin{proof}
\begin{enumerate}
  \item lemma: \label{ilem:Yf1X_ffp}
        $\ff(x)=1/x$ is a \prope{differentiable function} in $x>0$
        with $\xN=0$ \prope{critical point}s
        and $\ffp(x)=-1/x^2$.
  \item lemma: \label{ilem:Yf1X_ffi}
        The inverse of $y=\fg_1(x)=\ff(x)=\frac{1}{x}$ is $x=\fgi_1(y)=\ffi(y)=\frac{1}{y}$.
  \item It follows that
    \begin{align*}
      \ppy(y)
        &= \sum_{n=1}^{\xN+1} \frac{\ppx\brp{\fgi_n(y)}}{\abs{\ff'\brp{\fgi_n(y)}}}
        && \text{by \prefpp{thm:YfX}}
      \\&= \frac{\ppx\brp{\ffi(y)}}{\abs{\ff'\brp{\ffi(y)}}}
        && \text{because $\xN=0$}
      \\&= \frac{\ppx\brp{\frac{1}{y}}}{\abs{\ff'\brp{\frac{1}{y}}}}
        && \text{by \pref{ilem:Yf1X_ffi}}
      \\&= \frac{1}{\abs{-1/(1/y)^2}}\ppx\brp{\frac{1}{y}}
        && \text{by \pref{ilem:Yf1X_ffp}}
      \\&= \frac{1}{y^2}\ppx\brp{\frac{1}{y}}
        && \text{by definition of $\absn$}
    \end{align*}
\end{enumerate}
\end{proof}

%---------------------------------------
\begin{corollary}
\footnote{
  \citerpgc{papoulis1984}{95}{0070484686}{Example 5-9},
  \citerpgc{devroye1986}{27}{0387963057}{Example 4.4},
  \citerp{papoulis}{95},
  \citerp{proakis}{29}
  }
\label{cor:YX2}
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s.
\corbox{
  \brb{\rvY=\rvX^2}
  \implies
  \brb{\ppy(y) = \frac{1}{2\sqrt{y}} \brs{\ppx\brp{-\sqrt{y}} + \ppx\brp{\sqrt{y}}}}
  }
\end{corollary}
\begin{proof}
  \begin{enumerate}
    \item lemma: \label{ilem:YX2_ffp}
          $\ff(x)=x^2$ is a \prope{differentiable function} for $x\in\Real$
          with $\xN=1$ \prope{critical point}s.
    \item \label{ilem:YX2_ffi}
          The inverse of $y=\fg_1(x)=x^2$ for $x\ge0$ is $x=\fgi_1(y)=+\sqrt{y}$ and\\
          the inverse of $y=\fg_2(x)=x^2$ for $x<0$   is $x=\fgi_2(y)=-\sqrt{y}$.
    \item The derivative of $\ff(x)=x^2$ is $\ff'(x)=2x$. \label{ilem:YX2_derivative}
    \item And so it follows that \ldots
      \begin{align*}
        \ppy(y)
          &= \sum_{n=1}^{\xN+1} \frac{\ppx\brp{\fgi_n(y)}}{\abs{\ff'\brp{\fgi_n(y)}}}
          && \text{by \prefpp{thm:YfX}}
        \\&= \frac{\ppx\brp{\fgi_1(y)}}{\abs{\ff'\brp{\fgi_1(y)}}} +
             \frac{\ppx\brp{\fgi_2(y)}}{\abs{\ff'\brp{\fgi_2(y)}}}
          && \text{because $\xN=1$ \xref{ilem:YX2_ffp}}
        \\&= \frac{\ppx\brp{+\sqrt{y}}}{\abs{\ff'\brp{+\sqrt{y}}}}
           + \frac{\ppx\brp{-\sqrt{y}}}{\abs{\ff'\brp{-\sqrt{y}}}}
          && \text{by \pref{ilem:YX2_ffi}}
        \\&= \frac{\ppx\brp{+\sqrt{y}}}{\abs{2\brp{+\sqrt{y}}}}
           + \frac{\ppx\brp{-\sqrt{y}}}{\abs{2\brp{-\sqrt{y}}}}
          && \text{by \pref{ilem:YX2_derivative}}
        \\&= \frac{1}{2\sqrt{y}} \brs{\ppx\brp{-\sqrt{y}} + \ppx\brp{\sqrt{y}}}
          && \text{by definition of $\absn$}
      \end{align*}
  \end{enumerate}
\end{proof}

\begin{figure}\color{figcolor}
  \centering
  \boxed{\tbox{\includegraphics{../common/math/graphics/pdfs/tan.pdf}}}
\caption{
  $\rvZ=\tan\Theta$
  \label{fig:Z=tan0}
  }
\end{figure}
%---------------------------------------
\begin{corollary}
\footnote{
  \citerpp{papoulis}{99}{100}
  }
\label{cor:ppztan}
%---------------------------------------
Let $\rvZ=\tan\Theta$. Then
\corbox{
  \brb{\rvZ=\tan\Theta}
  \quad\implies\quad
  \brb{\ppz(z) = \frac{1}{1+z^2}  \sum_{n\in\rvZ} \ppth(\atan(z)+n\pi)}
  }
\end{corollary}
\begin{proof}
%\begin{enumerate}
%  \item The theorem can also be proved using \prefpp{thm:YfX}:
    \begin{enumerate}
      \item The roots of $z=\tan\theta$ are $\set{\theta_n=\atan{z}+n\pi}{n\in\Z}$. \label{item:ppztan_roots}
      \item The derivative of $z=\tan\theta$ is  $\ff'(\theta)=\sec^2 \theta$. \label{item:ppztan_derivative}
      \item It follows that
        \begin{align*}
          \ppz(z)
            &= \sum_{n=1}^\xN \frac{\ppth(\theta_n)}{|\ff'(\theta_n)|}
          \\&= \sum_n \frac{\ppth(\atan{z}+n\pi)}{|\ff'(\atan{z}+n\pi)|}
          \\&= \sum_n \frac{\ppth(\atan{z}+n\pi)}{|\sec^2(\atan{z}+n\pi)|}
          \\&= \sum_n \cos^2(\atan{z}+n\pi)  \ppth(\atan{z}+n\pi)
          \\&= \cos^2(\atan{z}) \sum_n  \ppth(\atan{z}+n\pi)
          \\&= \frac{1}{1+z^2}  \sum_n \ppth(\atan{z}+n\pi)
        \end{align*}
    \end{enumerate}

%  \item Alternatively \ldots
%    \begin{enumerate}
%      \item Let $z=\frac{y}{x}$ and $x^2 + y^2 = r^2$.
%            \begin{align*}
%              \cos^2\atan z
%                &= \cos^2\theta
%                 = \frac{x^2}{r^2}
%                 = \frac{x^2}{x^2+y^2}
%                 = \frac{\frac{x^2}{x^2}}{\frac{x^2}{x^2}+\frac{y^2}{x^2}}
%                 = \frac{1}{1+z^2}
%            \end{align*}
%      \item Let $h\to0$.
%            \begin{align*}
%              \atan{z+h}
%                &= y_1 + \frac{1}{m} \Delta y
%              \\&= \atan{z} + \left.\frac{1}{\dz/\dth}\right|_{\theta=\atan{z}} h
%              \\&= \atan{z} + \left.\frac{1}{\sec^2\theta}\right|_{\theta=\atan{z}} h
%              \\&= \atan{z} + \left.\cos^2\theta\right|_{\theta=\atan{z}} h
%              \\&= \atan{z} + h\cos^2\atan{z}
%              \\&= \atan{z} + h\frac{1}{1+z^2}
%            \end{align*}
%
%      \item Now we prove the theorem using the above relation.
%            \begin{align*}
%              \ppz(z)h
%                &= \psp\setn{z\le Z < z+h}
%              \\&= \psp\setn{z\le \tan\Theta < z+h}
%              \\&= \sum_n \psp\setn{z\le \tan\Theta < z+h \land
%                          \pi\left(n-\frac{1}{2}\right) \le \Theta < \pi\left(n+\frac{1}{2}\right)
%                          }
%              \\&= \sum_n \psp\setn{z\le \tan\Theta < z+h \left|
%                          \pi\left(n-\frac{1}{2}\right) \le \Theta < \pi\left(n+\frac{1}{2}\right)
%                          \right.}
%                          \psp\setn{\pi\left(n-\frac{1}{2}\right) \le \Theta \pi\left(n+\frac{1}{2}\right)}
%              \\&= \sum_n \psp\setn{\atan{z}+n\pi \le \Theta < \atan(z+h)+n\pi \left|
%                          \pi\left(n-\frac{1}{2}\right) \le \Theta < \pi\left(n+\frac{1}{2}\right)
%                          \right.}
%                          \psp\setn{\pi\left(n-\frac{1}{2}\right) \le \Theta \pi\left(n+\frac{1}{2}\right)}
%              \\&= \sum_n \psp\setn{\atan{z}+n\pi \le \Theta < \atan(z+h)+n\pi \land
%                          \pi\left(n-\frac{1}{2}\right) \le \Theta < \pi\left(n+\frac{1}{2}\right)
%                          }
%              \\&= \sum_n \psp\setn{\atan{z}+n\pi \le \Theta < \atan(z+h)+n\pi }
%              \\&= \sum_n \psp\setn{\atan{z}+n\pi \le \Theta < +\atan(z)+n\pi + h\frac{1}{1+z^2} }
%              \\&= h\frac{1}{1+z^2} \sum_n \ppth(\atan{z}+n\pi)
%            \\\implies
%              \ppz(z) &=  \frac{1}{1+z^2} \sum_n \ppth(\atan{z}+n\pi)
%            \end{align*}
%  \end{enumerate}
%\end{enumerate}
\end{proof}




%=======================================
\section{Functions of two random variables}
%=======================================
%---------------------------------------
\begin{theorem}
\footnote{
  \citerpgc{papoulis1990}{160}{0137116985}{Example 5.16}
  }
\label{thm:pdfconv}
%---------------------------------------
Let $\rvX$, $\rvY$, and $\rvZ$ be \fncte{random variable}s.
Let $\conv$ be the \ope{convolution} operator \xref{def:conv}.
\thmbox{
  \brb{\begin{array}{FMD}
    (1). & $\rvZ \eqd \rvX + \rvY$ & and \\
    (2). & $\rvX$ and $\rvY$ are \prope{independent} & \xref{def:independent}
  \end{array}}
  \implies
  \brb{\ppz(z) = \ppx(z)\conv\ppy(z)}
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \ppz(z)
    &\eqd \ddz\pcz(z)
    &&    \text{by definition of $\ppz$} &&\text{\xref{def:pdf}}
  \\&\eqd \ddz\psp\setn{\rvZ \le z}
    &&    \text{by definition of $\pcz$} &&\text{\xref{def:cdf}}
  \\&=    \ddz\psp\setn{\rvX+\rvY \le z}
    &&    \text{by hypothesis (1)}
  \\&=    \ddz\lim_{\varepsilon\to0}\sum_{n\in\Z} \psp\set{\rvX+\rvY \le z}{y+n\varepsilon<\rvY\le y+(n+1)\varepsilon}
    &&    \text{by \thme{sum of products}}&&\text{\xref{thm:psp_sop}}
  %\\&=    \ddz\int_y \pPc{\rvX+y \le z}{y<\rvY\le y+\varepsilon}\psp\setn{y<\rvY\le y+\varepsilon} \dy
  %  &&    \text{by definiton of $\pPc{\rvX}{\rvY}$ \xref{def:conprob}}
  \\&=    \ddz\int_{y\in\R} \pPc{\rvX+\rvY \le z}{\rvY=y}\ppy(y) \dy
    &&    \text{by definiton of $\pPc{\rvX}{\rvY}$}&&\text{\xref{def:conprob}}
  \\&=    \ddz\int_{y\in\R} \pPc{\rvX \le z-y}{\rvY=y} \ppy(y) \dy
  \\&=    \ddz\int_{y\in\R} \psp\setn{\rvX \le z-y}\ppy(y) \dy
    &&    \text{by hypothesis (2)}
  \\&\eqd \ddz\int_{y\in\R} \pcx(z-y)\ppy(y) \dy
    &&    \text{by definition of $\pcx$}&&\text{\xref{def:cdf}}
  \\&=    \int_{y\in\R} \ddz\brs{\pcx(z-y)\ppy(y)} \dy
    &&    \text{by \prope{linearity} of $\ddz$}
  \\&=    \int_{y\in\R} \brs{\ddz\pcx(z-y)}\ppy(y) \dy
    &&    \mathrlap{\text{because $y$ is fixed inside the integral}}
  \\&\eqd \int_y \ppx(z-y) \ppy(y)  \dy
    &&    \text{by definition of $\ppx$}&&\text{\xref{def:pdf}}
  \\&=    \ppx(z) \conv \ppy(z)
    &&    \text{by definition of $\conv$}&&\text{\xref{def:conv}}
\end{align*}
%
%\begin{align*}
%  \ppz(z)
%    &\eqd \lim_{\varepsilon\to0}\frac{1}{\varepsilon} \psp\setn{z \le Z < z+\varepsilon }
%    &&    \text{by definition of $\ppz$ \xref{def:pdf}}
%  \\&=    \lim_{\varepsilon\to0}\frac{1}{\varepsilon} \psp\setn{z \le X+Y < z+\varepsilon }
%    &&    \text{by definition of $\rvZ$}
%  \\&=    \lim_{\varepsilon\to0}\frac{1}{\varepsilon} \int_y \psp\setn{(z \le X+y < z+\varepsilon) \land (y\le Y<y+\varepsilon) } \dy
%    &&    \text{by \thme{sum of products} \xref{thm:psp_sop}}
%  \\&=    \lim_{\varepsilon\to0}\frac{1}{\varepsilon} \int_y \psp\setn{(z-y \le\rvX < z-y+\varepsilon) | (y\le Y<y+\varepsilon) }\psp\setn{y\le Y<y+\varepsilon} \dy
%  \\&=    \lim_{\varepsilon\to0}\frac{1}{\varepsilon} \int_y \psp\setn{(z-y \le\rvX < z-y+\varepsilon) | Y=y }\psp\setn{y\le Y<y+\varepsilon} \dy
%  \\&=    \lim_{\varepsilon\to0}\frac{1}{\varepsilon} \int_y \psp\setn{(z-y \le\rvX < z-y+\varepsilon)}\psp\setn{y\le Y<y+\varepsilon} \dy
%    &&    \text{by \prope{independence} hypothesis}
%  \\&\eqd \int_y \ppx(z-y) \ppy(y)  \dy
%    &&    \text{by definition of $\ppx$ \xref{def:pdf}}
%  \\&=    \ppx(z) \conv \ppy(z)
%    &&    \text{by definition of $\conv$ \xref{def:conv}}
%\end{align*}
\end{proof}

\begin{figure}
  \centering%
  \Huge%
  \begin{tabular}{c>{\Huge}c c>{\Huge}c l}%
    \tboxc{\includegraphics{../common/math/graphics/pdfs/pdf_fairdie_blue.pdf}} &\tbox{+}&%
    \tboxc{\includegraphics{../common/math/graphics/pdfs/pdf_fairdie_red.pdf}}  &\tbox{=}&%
    \tboxc{\includegraphics{../common/math/graphics/pdfs/pdf_dicesum_bluered.pdf}}%
    \\
    \tboxc{\includegraphics{../common/math/graphics/pdfs/n0.pdf}}               &\tbox{+}&%
    \tboxc{\includegraphics{../common/math/graphics/pdfs/n0.pdf}}               &\tbox{=}&%
    \tboxc{\includegraphics{../common/math/graphics/pdfs/n1.pdf}}%
    \\
    \tboxc{\includegraphics{../common/math/graphics/pdfs/n0.pdf}}               &\tbox{+}&%
    \tboxc{\includegraphics{../common/math/graphics/pdfs/n1.pdf}}               &\tbox{=}&%
    \tboxc{\includegraphics{../common/math/graphics/pdfs/n2.pdf}}%
  \end{tabular}
  \caption{\opb{Sum} of random variables yields \opb{convolution} of pdfs \xref{thm:pdfconv}}
\end{figure}

%---------------------------------------
\begin{theorem}
\label{thm:x1x2->y1y2}
%---------------------------------------
Let
\begin{liste}
  \item $\rvX_1$ and $\rvX_2$ be random variables with joint distribution
        $\ppx[\rvX_1,\rvX_2](x_1,x_2)$
  \item $\rvY_1=\ff_1(x_1,x_2)$ and $\rvY_2=\ff_2(x_1,x_2)$
\end{liste}
Then the joint distribution of $\rvY_1$ and $\rvY_2$ is
\thmbox{
  \ppx[Y_1,Y_2](y_1,y_2)
    = \frac{\ppx[\rvX_1,\rvX_2](x_1,x_2)}{|J(x_1,x_2)|}
    = \frac{\ppx[\rvX_1,\rvX_2](x_1,x_2)}{
        \left|\begin{array}{cc}
          \pderiv{\ff_1}{x_1} & \pderiv{\ff_1}{x_2}   \\
          \pderiv{\ff_2}{x_1} & \pderiv{\ff_2}{x_2}
        \end{array}\right|
        }
    = \frac{\ppx[\rvX_1,\rvX_2](x_1,x_2)}{
        \pderiv{\ff_1}{x_1}\pderiv{\ff_2}{x_2} -
        \pderiv{\ff_1}{x_2}\pderiv{\ff_2}{x_1}
        }
  }
\end{theorem}

%---------------------------------------
\begin{proposition}
\label{prop:XY->RT}
%---------------------------------------
Let $\rvX$ and $\rvY$ be random variables with joint distribution
$\ppxy(x,y)$ and
\[ R^2 \eqd X^2 + Y^2 \hspace{10ex} \Theta \eqd \atan\frac{\rvY}{\rvX}. \]
Then
\propbox{
  \ppx[R,\Theta](r,\theta)
    =  r\;\ppxy(r\cos\theta,r\sin\theta)
  }
\end{proposition}
\begin{proof}
\begin{align*}
  \ppx[R,\Theta](r,\theta)
    &= \frac{\ppxy(x,y)}{|J(x,y)|}
     =  \frac{\ppxy(x,y)}{
        \left|\begin{array}{cc}
          \pderiv{R}{x}      & \pderiv{R}{y}   \\
          \pderiv{\theta}{x} & \pderiv{\theta}{y}
        \end{array}\right|
        }
     =  \frac{\ppxy(x,y)}{
        \left|\begin{array}{cc}
          \frac{ x}{\sqrt{x^2+y^2}}  & \frac{y}{\sqrt{x^2+y^2}}   \\
          \frac{-y}{x^2+y^2}         & \frac{x}{x^2+y^2}
        \end{array}\right|
        }
  \\&= \frac{\ppxy(x,y)}{
         \frac{x}{\sqrt{x^2+y^2}}\frac{x}{x^2+y^2}  -
         \frac{y}{\sqrt{x^2+y^2}}\frac{-y}{x^2+y^2}
       }
  \\&= \frac{\ppxy(x,y)}{
         \frac{x^2+y^2}{(x^2+y^2)^{3/2}}
       }
  \\&= \ppxy(x,y)\frac{(x^2+y^2)^{3/2}}{x^2+y^2}
  \\&= \ppxy(r\cos\theta,r\sin\theta)\frac{r^3}{r^2}
  \\&= r\;\ppxy(r\cos\theta,r\sin\theta)
\end{align*}
\end{proof}


%---------------------------------------
\begin{proposition}
\label{prop:XY->RT_n}
%---------------------------------------
Let $\rvX\sim\pN{0}{\sigma^2}$ and $\rvY\sim\pN{0}{\sigma^2}$ be
independent random variables and
\\\indentx$R^2 \eqd X^2 + Y^2 \hspace{10ex} \Theta \eqd \atan\frac{\rvY}{\rvX}.$\\
Then
\propbox{\begin{array}{FMrcl}
  1. & $R$ and $\Theta$ are independent with joint distribution
     & \ppx[R,\Theta](r,\theta) &=& \ppr(r)\ppth(\theta)
\\
  2. & $R$ has Rayleigh distribution
     & \ppr(r)  &=& \frac{r}{\sigma^2}\exp{\frac{r^2}{-2\sigma^2}}
\\
  3. & $\Theta$ has uniform distribution
     & \ppth(\theta) &=& \frac{1}{2\pi}
\end{array}}
\end{proposition}
\begin{proof}
\begin{align*}
  \ppx[R,\Theta](r,\theta)
    &= r\;\ppxy(r\cos\theta,r\sin\theta)
    && \text{by \prefpp{prop:XY->RT}}
  \\&= r\;\ppx(r\cos\theta) \; \ppy(r\sin\theta)
    && \text{by independence hypothesis}
  \\&= r\;
       \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\frac{(r\cos\theta-0)^2}{-2\sigma^2}}
       \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\frac{(r\sin\theta-0)^2}{-2\sigma^2}}
  \\&= \frac{1}{2\pi\sigma^2}\;r\;
       \exp{\frac{r^2(\cos^2\theta + \sin^2\theta)}{-2\sigma^2}}
  \\&= \frac{1}{2\pi\sigma^2}\;r\;
       \exp{\frac{r^2}{-2\sigma^2}}
  \\&= \left[\frac{1}{2\pi}\right]
       \left[\frac{r}{\sigma^2}\exp{\frac{r^2}{-2\sigma^2}}\right]
\end{align*}
\end{proof}


%---------------------------------------
\begin{proposition}
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s
with covariance $\pvarxy$ on a
\structe{probability space} $\ps$.
\propbox{
  \brb{\begin{array}{FMD}
      (A).&$\rvX$ is \prope{Gaussian} with $\pN{\pmeanx}{\pvarx}$ & and
    \\(B).&$\rvY$ is \prope{Gaussian} with $\pN{\pmeany}{\pvary}$ & and
    \\(C).&$\pvarxy=\cov{\rvX}{\rvY}$
  \end{array}}
  \implies
  \brb{\psp\setn{\rvX>\rvY} = \pQ\brp{\frac{-\pmeanx + \pmeany}{\pvarx+\pvary-2\pvarxy}}}
  }
\end{proposition}
\begin{proof}
Because $\rvX$ and $\rvY$ are jointly Gaussian,
their linear combination $\rvZ=rvX-\rvY$ is also Gaussian.
A Gaussian distribution is completely defined by its mean and variance.
So, to determine the distribution of $\rvZ$,
we just have to determine the mean and variance of $\rvZ$.
\begin{align*}
  \pE Z
    &= \pE\rvX - \pE Y
  \\&= \pmeanx - \pmeany
\\
\\
  \var Z
    &= \pE Z^2 - (\pE Z)^2
  \\&= \pE (\rvX-\rvY)^2 - (\pE\rvX - \pE Y)^2
  \\&= \pE (\rvX^2-2XY+Y^2) - [(\pE\rvX)^2 -2\pE\rvX \pE Y + (\pE Y)^2 ]
  \\&= [\pE X^2- (\pE\rvX)^2]  + [Y^2- (\pE Y)^2] - 2[\pE XY - \pE\rvX \pE Y]
  \\&= \var\rvX + \var Y - 2\cov{\rvX}{\rvY}
  \\&\eqd \pvarx + \pvary -2\pvarxy
\\
\\
  \psp\setn{\rvX>\rvY}
    &= \psp\setn{\rvX-\rvY>0}
  \\&= \psp\setn{Z>0}
  \\&= \left.\pQ\left(\frac{z-\pE Z}{\var Z} \right)\right|_{z=0}
  \\&= \pQ\left(\frac{0-\pmeanx+\pmeany}{\pvarx+\pvary-2\pvarxy} \right)
\end{align*}
\end{proof}

\begin{figure}
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \includegraphics{../common/math/graphics/pdfs/n0.pdf}&\includegraphics{../common/math/graphics/pdfs/n1.pdf}&\includegraphics{../common/math/graphics/pdfs/n2.pdf}\\
    pdf of $\rvX_1$                  &pdf of $\rvY\eqd\rvX_1+\rvX_2$   &pdf of $\rvY\eqd\rvX_1+\rvX_2+\rvX_3$\\
    \hline
    \mc{2}{|c|}{\includegraphics{../common/math/graphics/pdfs/n3.pdf}}&\includegraphics{../common/math/graphics/pdfs/n4.pdf}\\
    \mc{2}{|c|}{pdf of $\rvY\eqd\rvX_1+\rvX_2+\rvX_3+\rvX_4$}&pdf of $\rvY\eqd\rvX_1+\rvX_2+\rvX_3+\rvX_4+\rvX_5$\\
    \hline
  \end{tabular}
  \caption{\label{fig:pdf_uniform_sums}
           The distributions of sums of independent uniformly distributed random variables
           \xref{ex:pdf_uniform_sums}}
\end{figure}
%---------------------------------------
\begin{example}[\exmd{Sum of Uniformly Distributed Random Variables}]
\label{ex:pdf_uniform_sums}
%---------------------------------------
Let $\seqn{\rvX_1, \rvX_2, \rvX_3, \ldots}$ be a \fncte{sequence}
of \prope{independent} \xref{def:independent}
\prope{uniformly distributed} random variables.
Let $\ppx[\xN](x)$ be the \fncte{probability density function} of
$\rvY\eqd\sum_{n=1}^{\xN}\rvX_n$.
Some of these distributions are illustrated in \prefpp{fig:pdf_uniform_sums}.
Note that the distributions of the sequence $\seqn{\ppx[1],\ppx[2],\ppx[3],\ldots}$
are all \fncte{B-spline}s\ifsxref{spline}{def:Bspline} and all form a
\prope{partition of unity}\ifsxref{partuni}{def:pun}.
\end{example}