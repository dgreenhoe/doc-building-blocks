%============================================================================
% Daniel J. Greenhoe
% LaTeX file
%============================================================================
%============================================================================
\chapter{System Identification}
%============================================================================
\qboxnpq{
  Karl R. Popper (1902--1994)
  \index{Popper, Karl}
  \index{quotes!Popper, Karl}
  \footnotemark
  }
  {../common/graphics/portraits/popper_karl_wkp_pdomain.jpg}
  %{Mathematics compares the most diverse phenomena and discovers the secret analogies that unite them.}
  {%
  I can therefore gladly admit that falsificationists like myself much
  prefer an attempt to solve an interesting problem by a bold conjecture,
  even (and especially) if it so turns out to be false,
  to any recital of a sequence of irrelevant truisms.
  We prefer this because we believe that this is the way in which we can
  learn from our mistakes and that in finding that our conjecture was false
  we shall have learned much about the truth,
  and shall have got nearer to the truth.
  }
  \footnotetext{\begin{tabular}[t]{ll}
    quote: & \citerp{popper1962}{231}, \citerpg{popper1963}{313}{1135971307}\\
    image: & \url{https://en.wikipedia.org/wiki/File:Karl_Popper.jpg}, ``no known copyright restrictions"
  \end{tabular}}

%=======================================
\section{Estimation techniques}
%=======================================
Let $\opS$ be a \structe{system} with \fncte{impulse response} $\fh(n)$ with
with \ope{DTFT} $\FH(\omega)$,
input $\rvx(n)$, and output $\rvy(n)$.
Often in the field of ``digital signal processing" (DSP), $\opS$ is a ``filter"
with known $\fh(n)$ and $\FH(\omega)$ because the filter $\opS$ was
designed by a designer who had direct control over $\fh(n)$.

However in many other practical situations, $\opS$ is some other system
for which $\fh(n)$ and $\FH(\omega)$ is \emph{not} known\ldots but which we may
want to \ope{estimate}. Examples of such $\opS$ is a
device on an industrial shaker table, a communication channel, or the entire earth.

Determining $\fh(n)$ and/or $\FH(\omega)$ is part of an operation called ``\ope{system identication}".
Determining $\FH(\omega)$ in particular is referred to as
``\ope{Frequency Response Identification}"\footnote{\citerpg{shin2008}{292}{0470725648}}
or as
``\ope{Frequency Response Function}" (``\ope{FRF}") estimation.\footnote{\citePpc{cobb1988}{1}{FRF ``measurement"}}
\ope{FRF} estimation is a challenging problem and one that
many people have devoted much effort to.
This chapter describes some of that effort.

In the early days, people used a rather obvious technique for determining $\FH(\omega)$---the simple
\fncte{sine sweep}. That is, they made the input a sine wave with slowly increasing (or decreasing)
frequency while measuring the resulting output.
This technique, although effective, was ``very slow".\footnote{
  \citeP{leuridan1986}{911}{``Stepped Sine Testing"},
  \citePpc{cobb1988}{1}{Chapter 1---Introduction},
  \citerppgc{ewins1986}{125}{140}{0863800173}{3.7 \scshape Use of different excitation types}
  }
\emph{And} there is another problem---we don't always have control over the input signal.
Examples of this include earthquake and volcanic activity analysis.

An alternative to the sine-sweep input is \fncte{random sequence} input.
All the techniques that follow in this chapter are of this type.
A problem with using random sequences directly for estimating $\FH(\omega)$ is that the
estimate $\estH(\omega)$ is itself also random.
This is not what we want. We want an estimate that we can actually write down
on paper or at least plot on paper.

A solution to this is to not use the random sequences directly to estimate $\FH(\omega)$,
but instead to first use the \ope{expectation} operator $\pE$ \xref{def:pE}.
The expectation operator takes a quantity $\rvX$ that is inherently ``random"
(with some probability distribution $\pdfp(x)$) and
turns it into a deterministic ``constant" $\pE\rvX$.

The operator $\pE$ is also used by the spectral density functions
$\Swxx(\omega)$ and $\Swxy(\omega)$ \xref{def:Swxy}.
And $\Swxx(\omega)$ and $\Swxy(\omega)$ are what are typically used to calculate
an estimate $\estH(\omega)$.

%=======================================
\section{Additive noise system models}
%=======================================
\begin{figure}
  \centering
  \begin{tabular}{|c|c|}
    \hline
     \tbox{\includegraphics{graphics/sysH_xyuvpq.pdf}}
    &\tbox{\includegraphics{graphics/sysH_mnoise.pdf}}
    \\
      \opd{communications additive noise model}
     &\opd{measurement    additive noise model}
    \\
      The ``input signal" is $\rvx(n)$.
     &The ``input signal" is $\rvp(n)$.
    \\\hline
      \mc{2}{|l|}{In each model, $\rvx(n)$ and $\rvy(n)$ are ``known", and $\rvu(n)$, $\rvv(n)$, and $\rvw(n)$ are \emph{not}.}
    \\\mc{2}{|l|}{In definition, the two models are \textbf{equivalent} under the relation $\rvu(n)=-\rvw(n)$.}
    \\\mc{2}{|l|}{In practice, they are \textbf{different}:}
    \\\mc{2}{|l|}{\indentx\imarks on the left, $\rvx$ and $\rvu$ would be typically \prope{uncorrelated};}
    \\\mc{2}{|l|}{\indentx\imarks on the right, $\rvx$ and $\rvw=-\rvu$ are very much \prope{correlated} ($\rvx$ is a function of $\rvu$).}
    \\\hline
  \end{tabular}
  \caption{Additive noise models\label{fig:addnoise}}
\end{figure}

Consider the additive noise systems illustrated in \prefpp{fig:addnoise}.
\begin{liste}
  \item The illustration on the left is suitable for modeling a communications system where
$\rvx$ is the transmitted signal, $\rvy$ is the received signal, $\rvu$ and $\rvv$ are thermal noise,
and the ``transfer function" $\fH$ is the communications channel (air, water, wires, etc.) that one wishes to estimate.

  \item The illustration on the right is suitable for modeling a testing system where
$\rvp$ is an input test signal (from an industrial shaker or from a naturally occurring signal originating 
from geophysical activity), $\rvw$ is measurement noise, $\rvx$ is the measured input contaminated by noise,
and $\fH$ is the device under test (a piece of equipment, a building, or the entire earth).
\end{liste}

Note that the two models are an equivalent system $\opS$ under the relation $\rvu=-\rvw$.
But although one might expect such a sign difference to wreak mathematical havoc in resulting equations,
this is simply not the case here because
\\\indentx$\ds\Swww = \opFT\pE\brs{\rvw(m)\rvw\ast(0)}
                    = \opFT\pE\brs{\brp{-\rvu(m)}\brp{-\rvu^\ast(0)}}
                    = \opFT\pE\brs{\brp{\rvu(m)}\brp{\rvu^\ast(0)}}
                    = \Swuu$
\\
So the sign difference is not that big of a difference after all.
But there are some key differences in practice:
\begin{liste}
  \item In the communications model (on the left), the ``input signal" is $\rvx(n)$
      and the frequency-domain input \fncte{signal-to-noise ratio} (\fncte{SNR}) is $\ffrac{\Swxx(\omega)}{\Swuu(\omega)}$.
      In the measurement model (on the right), the ``input signal" is $\rvp(n)$
      and the frequency-domain input \fncte{signal-to-noise ratio} (\fncte{SNR}) is 
         $\ffrac{\Swpp(\omega)}{\Swww(\omega)} = \ffrac{\Swpp(\omega)}{\Swuu(\omega)}$.

  \item On the left, $\rvx$ and $\rvu$ would be typically \prope{uncorrelated};
        on the right, $\rvx$ and $\rvw=-\rvu$ are very much \prope{correlated} ($\rvx$ is a function of $\rvu$).
\end{liste}

%=======================================
\section{Transfer function estimate definitions}
%=======================================
As a first attempt at estimating the transfer function $\fH$ of $\opS$,
or at least the magnitude squared of $\fH$,
we might assume $\fH$ to be \prope{LTI}, take a cue from the relation
$\Swyy=\Swxx\abs{\FH}^2$ of \prefpp{cor:Swxy},
%somehow deem the squaring operation to be of little consequence, 
and arrive at a function called ``\fncte{transmissibility}"
(next definition).
%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{bendat2011}{469}{1118210824}{$\abs{H(f)}=\brs{G_{yy}(f)/G_{xx}(f)}^{1/2}$},
  \citerpgc{goldman1999}{179}{0831130881}{Transmissibility \ldots $H_{ab}'=\ffrac{G_{bb}}{G_{aa}}$},
  \citeP{zhang2016},
  \citeP{yang2012},
  \citePp{zhou2018}{824},
  \url{https://link.springer.com/chapter/10.1007/978-3-319-54109-9_4}
  }
\label{def:Txy}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
  \fnctd{transmissibility $\Twxy(\omega)$} is defined as
  \qquad$\ds
  \Twxy(\omega) \eqd \frac{\Swyy(\omega)}{\Swxx(\omega)}
  $
  }
\end{definition}

Transmissibility is in essence the ratio of ``\fncte{spectral power}" \xref{rem:spower} 
output to \fncte{spectral power} input.
Note that it is a real-valued function (because $\Swxx$ and $\Swyy$ are real-valued).
We might suspect that we could attain better estimates of $\fH$ by allowing the estimates to be complex-valued. %\ldots 
%not to mention the idea of taking seriously the squaring operation.
And in fact, all the remaining estimates in this section are in general complex-valued.

And so to start (again), and in the very special (a.k.a unrealistic) case of $\opS$ having 
\prope{zero measurement noise} (\prope{zero measurement error})
($\rvv=\rvu=\rvw=0$),
$\fh(n)$ being \prope{linear time invariant} (\prope{LTI}),
and input $\rvx(n)$ being \prope{wide sense stationary}\ldots
then we can determine (a.k.a ``identify") $\fh(n)$ or $\FH(\omega)$
exactly by $\FH(\omega)=\ffrac{\Swyx(\omega)}{\Swxx(\omega)}$ \xref{cor:Swxy}.

However, in practical situations, there is measurement noise/error. % \xref{fig:xhyplusv}.
Examples may include
``road noise" from a test being performed in a moving vehicle or \fncte{quantization noise} from an 
\hie{analog-to-digital converter} (\hie{ADC}).

\begin{minipage}{\tw-70mm}
If the measurement error is at the output only (and under the assumptions of \prope{LTI} and \prope{WSS})
then $\estH_1$ (next definition) is the ideal estimator in the sense that $\estH_1=\FH$ \xref{thm:estH1}.
\end{minipage}
\hfill\tbox{\includegraphics{graphics/sysH_xy0v--.pdf}}
\\
%---------------------------------------
\begin{definition}[\fnctd{least squares estimate} of $\FH(\omega)$]
\footnote{
  \citerppgc{bendat1993}{106}{109}{0471570559}{5.1.1 Optimality of Calculations},
  \citerpgc{bendat2011}{185}{1118210824}{$H_1(f) = \ffrac{G_{xy}(f)}{G_{xx}(f)}$ (6.37)},
  \citerpgc{shin2008}{293}{0470725648}{
    $H_1(f)=\ffrac{\Swxy(f)}{\Swxx(f)}$ (9.63);
    which differs from \pref{def:H1}, but see \prefp{rem:Rxym}
    },
  \citeP{bendat1978}{cited by Cobb(1988)---variance estimate for $\estH_1$},
  \citePc{allemang1979}{cited by Shin(2008)},
  \citePpc{leuridan1986}{910}{\fncte{Least Squares Technique}; (8) $[G_{xx}](H)=[G_{xy}]$},
  \citeP{abom1986}{cited by Cobb(1988)---variance estimate for $\estH_1$},
  \citePppc{allemang1987}{54}{55}{5.3.1 $H_1$ Technique; $[H]=[G_{XF}][G_{FF}]^{-1}$ (11)},
  \citePpc{cobb1988}{2}{$^1\hat{H}(f)=\ffrac{\hat{G}_{yx}(f)}{\hat{G}_{xx}(f)}$ (1)},
  \citePpc{goyder1984}{438}{$H(i\omega)=\ffrac{S_{qp}}{S_{pp}}$ (3)},
  \citerpgc{pintelon2012}{233}{1118287398}{$\hat{G}(\Omega_k)=S_{yu}(j\omega_k)S_{uu}^{-1}(j\omega_k)$ (7-30)}
  }
\label{def:H1}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defbox{\begin{array}{Mrc>{\ds}l}
  The \fnctd{transfer function estimate $\estH_1(\omega)$} is defined as & 
  \estH_1(\omega) &\eqd& \frac{\Swyx(\omega)}{\Swxx(\omega)}
\end{array}}
\end{definition}

The estimator $\estH_1$ is a good start.
However in the early 1980s, L. D. Mitchell pointed out that in the presence of input noise,
$\estH_1$ is far from ideal in that it is \prope{biased} with respect to $\FH$;
in fact, $\estH_1$ \prope{under estimates} $\FH$ \xref{thm:estH2}.
Mitchell proposed a new estimator $\estH_2$ (next definition).


\begin{minipage}{\tw-70mm}
This estimator has the special property that when there is input noise but no
output noise (and under \prope{LTI}, \prope{WSS}, and \prope{uncorrelated} assumptions),
then it is ideal in the sense that $\estH_2(\omega)=\FH(\omega)$ \xref{thm:estH2}.
\end{minipage}
\hfill\tbox{\includegraphics{graphics/sysH_xyu0--.pdf}}
\\
Note also that in the case of both no input and no output noise, then $\estH_1=\estH_2$ \xref{cor:Swxy}.
%---------------------------------------
\begin{definition}[\fnctd{Inverse Method}]
\footnote{
  \citerpgc{shin2008}{293}{0470725648}{$H_2(f)=\ffrac{\Swyy(f)}{\Swyx(f)}$ (9.65);
    which differs from \pref{def:H2}, but see \prefp{rem:Rxym}
    },
  \citerpgc{bendat2011}{186}{1118210824}{$H_2(f) = \ffrac{G_{yy}(f)}{G_{yx}(f)}$ (6.42)},
  \citePc{mitchell1980}{cited by Cobb(1988)},
  \citePpc{mitchell1982}{278}{``Define what will be called an inverse method for calculation of a FRF as\ldots";
    $\~H_2(f)=\ffrac{G_{yy}}{G_{yx}}$ (6);
    Note this differs with \pref{def:H2} by a conjugate, but note that Mitchell seems to follow Bendat (see his [3] and [4]),
    which would explain this difference \xref{rem:Rxym}},
  \citePpc{cobb1988}{3}{$^2\hat{H}(f)=\ffrac{\hat{G}_{yy}(f)}{\hat{G}_{xy}(f)}$ (1)}
  }
\label{def:H2}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defbox{\begin{array}{Mrc>{\ds}l}
  The \fnctd{transfer function estimate $\estH_2(\omega)$} is defined as 
  & \estH_2(\omega) &\eqd& \frac{\Swyy(\omega)}{\Swxy(\omega)}
\end{array}}
\end{definition}

Mitchell's $\estH_2$ contribution ``generated a flurry of activity"\footnote{\citePp{cobb1988}{3}}
and soon more $\FH$ estimators appeared.
%As it turns out, in the presence of \emph{both} input and output measurement noise,
%the traditional estimator $\estH_1$, although ideal when input noise = 0, tends to \prope{under estimate} $\FH$ \xref{thm:estH1}
%and Mitchell's $\estH_2$, although ideal when output noise = 0,
%tends to \prope{over estimate} $\FH$ \xref{thm:estH2}.
%But what about the case when there is noise on both input and output?
%The next estimator is ideal when noise is present on both and the noise power of the two is equal.
So far we have
\begin{listi}
  \item $\estH_1$ which is ideal when there is no input noise but
        \prope{under estimate}s $\FH$ when there is \xref{thm:estH1}
  \item $\estH_2$ which is ideal when there is no output noise but
        \prope{over estimate}s $\FH$ when there is \xref{thm:estH2}.
\end{listi}
But what about estimators for when there is noise on both input and output?
Armed with two estimators that between them account for both input and output noise,
an ``ad hoc" solution might be to somehow take mean values of $\estH_1$ and $\estH_2$
to induce new estimators---this approach summarizes the next three definitions.
An arguably more mature approach is to find estimators that are optimal with respect to least squares measures---and
this approach summarizes \pref{def:Hv} -- \prefpp{def:Hkappa}.

%---------------------------------------
\begin{definition}[\fnctd{Arithmetic mean estimator}]
\footnote{
  \citePpc{mitchell1982}{279}{``Frequency Response Calculation: The Average Method"},
  \citePpc{zheng2002}{918}{``1.3 Arithmetic Mean Estimator $H_3$"}
  }
\label{def:Havg}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defbox{\begin{array}{Mrc>{\ds}>{\ds}l}
  The \fnctd{transfer function estimate $\estHavg(\omega)$} is defined as 
  & \estHavg(\omega) &\eqd& \frac{\estH_1(\omega) + \estH_2(\omega)}{2}
\end{array}}
\end{definition}

%---------------------------------------
\begin{definition}[\fnctd{Geometric mean estimator}]
\footnote{
  \citePpc{zheng2002}{918}{``1.4 Geometric Mean Estimator $H_4$"}
  }
\label{def:Hgm}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defbox{\begin{array}{Mrc>{\ds}>{\ds}l}
  The \fnctd{transfer function estimate $\estHgm(\omega)$} is defined as 
  & \estHgm(\omega) &\eqd& \sqrt{\estH_1(\omega)\estH_2(\omega)}
\end{array}}
\end{definition}

%---------------------------------------
\begin{remark}
%---------------------------------------
Note that the \fncte{geometric mean estimator} \xref{def:Hgm} and \fncte{transmissibility} \xref{def:Txy}
are closely related:
\begin{align*}
  \estHgm(\omega) 
    &\eqd \sqrt{\estH_1(\omega)\estH_2(\omega)}
    && \text{by definition of $\estHgm$}
    && \text{\xref{def:Hgm}}
  \\&\eqd \sqrt{\frac{\Swxy^\ast(\omega)}{\Swxx(\omega)}\frac{\Swyy(\omega)}{\Swxy(\omega)}}
    && \text{by definitions of $\estH_1$ and $\estH_2$}
    && \text{\xxref{def:H1}{def:H2}}
  \\&= \sqrt{\frac{\Swyy(\omega)}{\Swxx(\omega)}\frac{\Swxy^\ast(\omega)}{\Swxy(\omega)}}
  \\&= \sqrt{\Twxy(\omega)}\sqrt{\frac{\Swxy^\ast(\omega)}{\Swxy(\omega)}}
    && \text{by definition of $\Twxy$}
    && \text{\xref{def:Txy}}
  \\&= \sqrt{\Twxy(\omega)}\sqrt{\frac{\abs{\Swxy(\omega)}e^{-i\phi(\omega)}}{\abs{\Swxy(\omega)}e^{i\phi(\omega)}}}
    && \text{where $\Swxy(\omega)\eqd{\abs{\Swxy(\omega)}e^{-i\phi(\omega)}}$}
  \\&= \sqrt{\Twxy(\omega)}\sqrt{e^{-i2\phi(\omega)}}
  \\&= \sqrt{\Twxy(\omega)}\,e^{-i\phi(\omega)}
\end{align*}

And hence, $\abs{\estHgm}^2 = \Twxy$.
\end{remark}

%---------------------------------------
\begin{definition}[\fnctd{Harmonic mean estimator}]
\footnote{
  \citePc{carne2006}{$H_C = [H_A^{-1} + H_B^{-1}]^{-1}$}
  %\citergc{ewins1985}{086380036X}{cited by Carne (2006)}
  %\citerpgc{ewins1986}{???}{0863800173}{...}
  }
\label{def:Hharm}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defbox{\begin{array}{Mrc>{\ds}>{\ds}l}
  The \fnctd{transfer function estimate $\estHharm(\omega)$} is defined as 
  & \estHharm(\omega) &\eqd& \frac{1}{\frac{1}{\ds\estH_1(\omega)} + \frac{1}{\ds\estH_2(\omega)}}
\end{array}}
\end{definition}


%---------------------------------------
\begin{definition}[\fnctd{Total Least Squares estimator}]
\footnote{
  \citePpc{white2006}{679}{(6)},
  \citerpgc{shin2008}{294}{0470725648}{(9.69)}
  }
\label{def:Hv}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
  The \fnctd{transfer function estimate $\estHv(\omega)$} is defined as
  \\\indentx$\ds
  \estHv(\omega) \eqd
  \frac{2\Swyx(\omega)}
       {\Swxx(\omega)-\Swyy(\omega) + \sqrt{\brs{\Swxx(\omega)-\Swyy(\omega)}^2 + 4\abs{\Swxy(\omega)}^2}}
  $
  }
\end{definition}

The estimate $\estHv$ and it's refusal to assume zero input or output noise
was definitely a step in the right direction.
But it still assumes that the input and output noise power to be equal.
And what if it's not?
Enter Wicks and Vold who in 1986 introduced such an estimator (next definition)
that makes no such assumption.
It features a parameter $s$ in the range $\intco{0}{\infty}$
that can be adjusted depending on the ratio of output and input noise.
And as it turns out, $\estHs = \estH_1$ when $s=0$, $\estHs=\estHv$ when $s=1$, and
$\estHs\to\estH_2$ as $s\to\infty$. % \xref{thm:Hs}.
%---------------------------------------
\begin{definition}[\fnctd{scaling estimate}]
\footnote{
  \citePc{leclere2012}{(10) with $x$ and $y$ swapped}
  }
\label{def:Hs}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
  The \fnctd{transfer function estimate $\estHs(\omega)$} with \vald{scaling parameter} $s\in\intco{0}{\infty}$ is defined as
  \\\indentx$\ds
  \estHs(\omega; s) \eqd
  \frac{2\Swyx(\omega)}
       {\Swxx(\omega)-s^2\Swyy(\omega) + \sqrt{\brs{\Swxx(\omega)-s^2\Swyy(\omega)}^2 + 4s^2\abs{\Swxy(\omega)}^2}}
  $
  }
\end{definition}

Wicks and Vold's $\estHs$ estimates $\FH$ using a single parameter $s$
that is \prope{constant} with respect to frequency.
This is suitable for additive white noise $\rvw(n)$, which has constant spectral density
$\Swww(\omega)=\pvar_w$.
However, not all noise is white noise.
And later in 2006, White, Tan and Hammond proposed a generalization (next definition) of $\estHs$ with a scaling
function $\kappa(\omega)$---that is, one that is not in general constant with respect to frequency
and is thus suitable in the presence of \prope{colored} noise.
%---------------------------------------
\begin{definition}
\footnote{
  \citerpgc{shin2008}{293}{0470725648}{(9.67)},
  \citeP{white2006},
  \citePc{leclere2012}{(10) $\kappa(f)=1/s^2$ and $x$ and $y$ swapped}
  }
\label{def:Hkappa}
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
  The \fnctd{transfer function estimate $\estHkappa(\omega; \kappa)$} is defined as
  \\\indentx$\ds
  \estHkappa(\omega) \eqd
  \frac{2\kappa(\omega)\Swyx(\omega)}
       {\kappa(\omega)\Swxx(\omega)-\Swyy(\omega) + \sqrt{\brs{\kappa(\omega)\Swxx(\omega)-\Swyy(\omega)}^2 + 4\kappa(\omega)\abs{\Swxy(\omega)}^2}}
  $
  }
\end{definition}

The previous estimators all assumed two signals: an input $\rvx(n)$ and an output $\rvy(n)$.
However, in many practical systems, there is a third signal that is ``driving" the system.
In 1984 Goyder proposed an estimator (next definition) that is based on three signals.
%---------------------------------------
\begin{definition}[Three channel estimate]
\footnote{
  \citerpgc{shin2008}{297}{0470725648}{$H_3(f)=\ffrac{S_{ry}(f)}{S_{rx}(f)}$ (9.78)},
  \citePpc{cobb1988}{4}{$^c\hat{H}(f)=\ffrac{\hat{G}_{ys}(f)}{\hat{G}_{xs}(f)}$ (1.4)},
  \citePpc{goyder1984}{440}{$H(i\omega)=\ffrac{S_{qz}}{S_{pz}}$ (5)},
  \citePpc{cobb1990}{450}{(1)},
  \citerpgc{pintelon2012}{241}{1118287398}{$\hat{G}(\Omega_k)=\hat{G}_{ry}(\Omega_k)\hat{G}_{ru}^{-1}(\Omega_k)$ (7-49)}
  }
\label{def:Hc}
%---------------------------------------
Let $\opS$ be a system with input $\rvx(n)$, output $\rvy(n)$, and a driver $\rvp(n)$.
\defboxt{
  The \fnctd{transfer function estimate $\estHc(\omega)$} is defined as
  \\\indentx$\ds
  \estHc(\omega) \eqd \frac{\Swxx[py](\omega)}{\Swxx[px](\omega)}
  $
  }
\end{definition}

%=======================================
\section{Properties}
%=======================================
%---------------------------------------
\begin{proposition}
\footnote{
  \citerpgc{shin2008}{293}{0470725648}{(9.67)},
  \citeP{white2006},
  \citePc{leclere2012}{(10) $\kappa(f)=1/s^2$ and $x$ and $y$ swapped}
  }
\label{prop:Hkappa}
%---------------------------------------
Let $\estHkappa(\omega)$ be defined as in \prefpp{def:Hkappa}.
\propbox{
  \estHkappa(\omega) = \frac{\Swyy(\omega)-\kappa(\omega)\Swxx(\omega) + \sqrt{\brs{\Swyy(\omega) - \kappa(\omega)\Swxx(\omega)}^2 + 4\kappa(\omega)\abs{\Swxy(\omega)}^2}}
                            {2\Swxy(\omega)}
  }
\end{proposition}

\begin{minipage}{\tw-50mm}
Let $\opS$ be the \structe{system} illustrated to the right.
\textbf{If} there is no measurement noise on the input and output and \textbf{if}
$\fh$ is \prope{linear time invariant}, then
$\fH = \Swxy/\Swxx$ \xref{cor:Rxyh}.
But what if there is output measurement noise?
And what if $\fH$ is not \prope{LTI}?
What is the best least-squares estimate of $\FH$?
The answer is the estimator $\estH_1$ \xref{def:H1},
as demonstrated in the next theorem.
\end{minipage}
\hfill\tbox{\includegraphics{graphics/sysHw_xy.pdf}}

%---------------------------------------
\begin{minipage}{\tw-75mm}
\begin{theorem}[Best LTI least-squares estimate $\estH$ of $\fH$]
\footnotemark
\label{thm:H1LS}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated to the right.
Let $\norm{\rvX}^2\eqd\pE\brs{\rvX\rvX^\ast}$.
\end{theorem}
\end{minipage}
\hfill\tbox{\includegraphics{graphics/sysH_estH.pdf}}
\footnotetext{
  \citerppgc{bendat1980}{98}{100}{0471058874}{5.1.1 Optimal Character of Calculations; note: proof minimizing $\Swvv$ but yields same result},
  \citerppgc{bendat1993}{106}{109}{0471570559}{5.1.1 Optimality of Calculations}
  }
\thmbox{
  \brbr{\begin{array}{FMMD}
      (A).& $\rvx$, $\rvu$, and $\rvv$ are & \prope{WSS}          & and
    \\(B).& $\rvx$, $\rvu$, and $\rvv$ are & \prope{uncorrelated} & and
    \\(C).& $\pE\rvu=\pE\rvu=0$            & (\prope{zero-mean})  & and
    \\(D).& $\estH$ is \prope{LTI}         &                      &
  \end{array}}
  \implies
  \brbl{\begin{array}{c}
    \ds\argmin_{\estH}\norm{\FW(\omega)-\FQ(\omega)}^2 = \frac{\Swxy^\ast(\omega)}{\Swxx(\omega)+\Swuu(\omega)}
    \\
    \brp{\begin{array}{M}
      \scs The optimal \prope{LTI} \prope{least-squares} estimate of $\FH$ is
      \\$\estH=\ffrac{\Swxy^\ast}{[\Swxx+\Swuu]}$)
    \end{array}}
  \end{array}}
  }
\\
\begin{proof}
\begin{enumerate}
  \item Define the \fncte{cost function} $\fCost\brp{\estH}$ for spectral \prope{least-squares} estimate as \label{item:H1LS_cost_def}
    $\fCost\brp{\estH} \eqd \norm{\FW(\omega)-\FQ(\omega)}^2$
    \\where $\FW(\omega)$ is the \ope{DTFT} \xref{def:dtft} of $\rvw(n)$
      and   $\FQ(\omega)$ is the \ope{DTFT} of $\rvq(n)$.

  \item lemma: $\fCost\brp{\estH}=\Swxx\abs{\estH}^2 - \estH(\omega)\Swxy - \brs{\estH\Swxy}^\ast + \Swqq$.
        Proof: \label{ilem:H1LS_cost}
    \begin{align*}
      \fCost\brp{\estH}
        &\eqd \norm{\FW(\omega)-\FQ(\omega)}^2
        && \text{by \pref{item:H1LS_cost_def}}
      \\&\eqd \inprod{\FW(\omega)-\FQ(\omega)}{\FW(\omega)-\FQ(\omega)}
      \\&\eqd \pE\brp{\brs{\FW(\omega)-\FQ(\omega)}\brs{\FW(\omega)-\FQ(\omega)}^\ast}
      \\&=\mathrlap{
              \pE\brs{\FW(\omega)\FW^\ast(\omega)}
            - \pE\brs{\FW(\omega)\FQ^\ast(\omega)}
            - \pE\brs{\FQ(\omega)\FW^\ast(\omega)}
            + \pE\brs{\FQ(\omega)\FQ^\ast(\omega)}
           }
      \\&= \mathrlap{
           \Swww(\omega) - \Swwq(\omega) - \Swwq^\ast(\omega) + \Swqq(\omega)
           \qquad\text{by \thme{Wiener-Khinchin relationship}}
           }
      \\&= \mathrlap{
           \Swpp(\omega)\abs{\estH(\omega)}^2
         - \estH(\omega)\Swpy(\omega)
         - \brs{\estH(\omega)\Swpy(\omega)}^\ast
         + \Swqq(\omega)
           \quad\text{by (A)--(D) and \prefpp{cor:GHz}}
           }
      \\&= \Swxx(\omega)\abs{\estH(\omega)}^2
         + \Swuu(\omega)\abs{\estH(\omega)}^2
         - \estH(\omega)\Swpy(\omega)
         - \brs{\estH(\omega)\Swpy(\omega)}^\ast
         + \Swqq(\omega)
        && \text{by (A)--(C) and \pref{cor:xvy}}
      \\&= \Swxx(\omega)\abs{\estH(\omega)}^2
         + \Swuu(\omega)\abs{\estH(\omega)}^2
         - \estH(\omega)\Swxy(\omega)
         - \brs{\estH(\omega)\Swxy(\omega)}^\ast
         + \Swqq(\omega)
        && \text{by \prefpp{cor:sysH_addnoise}}
    \end{align*}

  \item lemma: $\estH_R = \ffrac{\Real\Swyx}{\Swxx}$. Proof: \label{ilem:H1LS_R}
    \begin{align*}
      0 &= \pderiv{}{\estH_R}\fCost\brp{\estH}
      \\&= \pderiv{}{\estH_R}\brp{
               \Swxx\abs{\estH}^2 
             + \Swuu\abs{\estH}^2 
             - \estH\Swxy 
             - \estH^\ast\Swxy^\ast 
             + \Swqq
             }
        && \text{by \prefp{ilem:H1LS_cost}}
      \\&= \pderiv{}{\estH_R}\brp{
               \Swxx\brs{\estH_R^2+\estH_I^2} 
             + \Swuu\brs{\estH_R^2+\estH_I^2} 
             - (\estH_R+i\estH_I)\Swxy 
             - (\estH_R+i\estH_I)^\ast\Swxy^\ast 
             + \Swqq
             }
      \\&= \mathrlap{
           2\estH_R\Swxx + 2\estH_R\Swuu - \Swxy - \Swxy^\ast + \cancelto{0}{\pderiv{}{\estH_R}\Swqq}
           \qquad\text{because $\rvq$ does not vary with $\estH$}
           }
      \\&= 2\estH_R\Swxx + 2\estH_R\Swuu - 2\Real\Swxy
      \\&= 2\estH_R\Swxx + 2\estH_R\Swuu - 2\Real\Swyx
        && \text{by \prefp{cor:Swxy_sym}}
      \\&\implies \boxed{\estH_R = \frac{\Real\Swyx}{\Swxx+\Swuu}}
    \end{align*}

  \item lemma: $\estH_I = \ffrac{\Imag\Swyx}{\Swxx}$. Proof: \label{ilem:H1LS_I}
    \begin{align*}
      0
        &= \pderiv{}{\estH_I}\fCost\brp{\estH}
      \\&= \pderiv{}{\estH_I}
           \brp{\Swxx\abs{\estH}^2 + \Swuu\abs{\estH}^2 - \estH\Swxy - \estH^\ast\Swxy^\ast + \Swqq}
        && \text{by \prefp{ilem:H1LS_cost}}
      \\&= \pderiv{}{\estH_I}\brs{
           \Swxx\brs{\estH_R^2+\estH_I^2}
         + \Swuu\brs{\estH_R^2+\estH_I^2}
         - (\estH_R+i\estH_I)\Swxy
         - (\estH_R-i\estH_I)\Swxy^\ast
         + \Swqq}
      \\&= \mathrlap{
           2\estH_I\Swxx - i\Swxy + i\Swxy^\ast + \cancelto{0}{\pderiv{}{\estH_R}\Swqq}
           \qquad\text{because $\rvq$ does not vary with $\estH$}
           }
      \\&= 2\estH_I(\Swxx+\Swuu) - 2i(i\Imag\Swxy)
      \\&= 2\estH_I(\Swxx+\Swuu) + 2i(i\Imag\Swyx)
        && \text{by \prefp{cor:Swxy_sym}}
      \\&= 2\estH_I(\Swxx+\Swuu) - 2\Imag\Swyx
      \\&\implies \boxed{\estH_I = \frac{\Imag\Swyx}{\Swxx+\Swuu}}
     \end{align*}

  \item Proof that $\estH = \ffrac{\Swyx}{\Swxx+\Swuu} \eqd \estH_1$:
    \begin{align*}
      \estH(\omega)
        &= \estH_R(\omega) + i\estH_I(\omega)
      \\&= \frac{ \Real\Swyx(\omega)}{\Swxx(\omega)+\Swuu(\omega)} 
         + \frac{i\Imag\Swyx(\omega)}{\Swxx(\omega)+\Swuu(\omega)}
        && \text{by \pref{ilem:H1LS_R} and \pref{ilem:H1LS_I}}
    \end{align*}
\end{enumerate}
\end{proof}

It follows immediately from the previous theorem that, in the special case
of no input noise ($\rvu(n)=0$), $\estH_1$ is the best
least-squares estimate of $\FH$ (next corollary).
Moreover, in the special case of $\FH$ being \prope{linear time invariant} (\prope{LTI}),
then $\estH_1$ is not only the best least-squares estimate, but $\estH_1$ actually exactly equals $\FH$
\xref{thm:estH1}.
%---------------------------------------
\begin{corollary}
\footnote{
  \citerppgc{bendat1980}{98}{100}{0471058874}{5.1.1 Optimal Character of Calculations; note: proof minimizing $\Swvv$ but yields same result},
  \citerppgc{bendat1993}{106}{109}{0471570559}{5.1.1 Optimality of Calculations}
  }
\label{cor:H1LS}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated in \prefpp{thm:H1LS}.
Let $\norm{\rvX}^2\eqd\pE\brs{\rvX\rvX^\ast}$.
\corbox{
  \brb{\begin{array}{FMD}
      (A).&hypotheses of \prefpp{thm:H1LS} & and
    \\(B).&$\rvu(n) = 0$\quad\text{\scs(zero input noise)}
  \end{array}}
  \implies
  {\mcom{\argmin_{\estH}\norm{\FW(\omega)-\FQ(\omega)}^2 = \estH_1(\omega)}
       {$\estH_1$ is the optimal \prope{least-squares estimator} of $\FH$}}
  }
\end{corollary}
\begin{proof}
  \begin{align*}
    \estH(\omega)
      &= \frac{\Swyx(\omega)}{\Swxx(\omega) + \Swuu(\omega)}
      && \text{by \prefp{thm:H1LS}}
    \\&= \frac{\Swyx(\omega)}{\Swxx(\omega)}
      && \text{by \prope{zero input noise} hypothesis}
      && \text{(B)}
    \\&\eqd \estH_1(\omega)
      && \text{by definition of $\estH_1$}
      && \text{\xref{def:H1}}
  \end{align*}
\end{proof}

%=======================================
\section{Properties of LTI systems}
%=======================================
The previous section did assume the estimates $\estH_1$ and $\estH_2$  to be 
\prope{linear time invariant} (\prope{LTI}), but it did \emph{not} assume that 
the system transfer function $\fH$ itself to be \prope{LTI}. 
But making the LTI assumption on $\fH$ yields some interesting and insightful
results, such as those in this section.

%---------------------------------------
\begin{theorem} %[LTI no output noise]
\footnote{
  \citerpgc{shin2008}{294}{0470725648}{$H_1(f)=H(f)$ (9.70); $H_2(f)=H(f)\brp{1+\ffrac{S_{n_yn_y}(f)}{S_{yy}(f)}}$ (9.71)},
  \citerpgc{shin2008}{294}{0470725648}{$H_1(f)=\ffrac{H(f)}{\brp{1+\ffrac{S_{n_xn_x}}{S_{xx}(f)}}}$ (9.72); $H_2(f)=H(f)$ (9.73)},
  \citePpc{mitchell1982}{277}{$\~H_1(f)=\ffrac{H_0(f)}{\brp{1+\ffrac{G_{nn}}{G_{uu}}}}$}
  \citePpc{mitchell1982}{278}{$\~H_2(f)=H_0(f)\brp{1+\ffrac{G_{mm}}{G_{vv}}}$}
  }
\label{thm:estH1}
\label{thm:estH2}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated in \prefpp{fig:addnoise}.
\thmboxt{
  $\brb{\begin{array}{FMMD}
      (A).& $\fH$ is                                &\prope{linear time invariant} & and
    \\(B).& $\rvx(n)$ is                            &\prope{wide-sense stationary} & and
    \\(C).& $\rvx(n)$, $\rvu(n)$, and $\rvv(n)$ are &\prope{uncorrelated}          & 
  \end{array}}$
  \\\indentx$\implies
  \brb{\begin{array}{F>{\ds} rc>{\ds}l rc>{\ds}l D}
    \mc{3}{M}{\scs\prope{biased}:}
                             & \text{\scs\prope{over-estimated}}
                           & & \text{\scs\prope{under-estimated}}
    \\(1).&\estH_1(\omega) &=& \FH(\omega)\brs{1 + \frac{\ds\Swuu(\omega)}
                                                        {\ds\Swxx(\omega)}}
                           &=& \FH(\omega)\brs{\frac{1}{1+\frac{\ds\Swww(\omega)}{\ds\Swpp(\omega)}}}
                           &and
    \\(2).&\estH_2(\omega) &=& \FH(\omega)\brs{1 + \frac{\Swvv(\omega)}{\Swqq(\omega)}}
                           & & 
                           &
  \end{array}}$
  }
\end{theorem}
\begin{proof}
\begin{align*}
  \estH_1(\omega)
    &\eqd \frac{\Swyx(\omega)}{\Swxx(\omega)}
    && \text{by definition of $\estH_1$}
    && \text{\xref{def:H1}}
  \\&= \frac{\Swqp(\omega)}{\Swxx(\omega)}
    && \text{by hypotheses (B)--(C)}
    && \text{and \prefp{cor:sysH_addnoise}}
  \\&= \frac{\Swpp(\omega)\FH(\omega)}{\Swxx(\omega)}
    && \text{by hypothesis (A)}
    && \text{and \prefp{cor:Swxy}}
  \\&= \FH(\omega)\brs{\frac{\Swxx(\omega)+\Swuu(\omega)}{\Swxx(\omega)}}
    && \text{by hypotheses (B)--(C)}
    && \text{and \prefp{cor:xvy}}
  \\&= \FH(\omega)\brs{1 + \frac{\Swuu(\omega)}{\Swxx(\omega)}}
  \\
  \estH_1(\omega)
    &\eqd \frac{\Swyx(\omega)}{\Swxx(\omega)}
    && \text{by definition of $\estH_1$}
    && \text{\xref{def:H1}}
  \\&= \frac{\Swqp(\omega)}{\Swxx(\omega)}
    && \text{by hypotheses (B)--(C)}
    && \text{and \prefp{cor:sysH_addnoise}}
  \\&= \frac{\Swqp(\omega)}{\Swpp(\omega)+\Swww(\omega)}
    && \text{by hypotheses (B)--(C)}
    && \text{and \prefp{cor:sysH_addnoise}}
  \\&= \frac{\Swpp(\omega)\FH(\omega)}{\Swpp(\omega)+\Swww(\omega)}
    && \text{by hypothesis (A)}
    && \text{and \prefp{cor:Swxy}}
  \\&= \FH(\omega)\brs{\frac{1}{1+\frac{\Swww(\omega)}{\Swpp(\omega)}}}
  \\
  \estH_2(\omega)
    &\eqd \frac{\Swyy(\omega)}{\Swxy(\omega)}
    && \text{by definition of $\estH_2$}
    && \text{\xref{def:H2}}
  \\&= \frac{\Swqq(\omega) + \Swvv(\omega)}{\Swxy(\omega)}
    && \text{by hypothesis (C)}
    && \text{and \prefp{cor:xvy}}
  \\&= \frac{\Swqq(\omega) + \Swvv(\omega)}{\Swpq(\omega)}
    && \text{by hypothesis (C)}
    && \text{and \prefp{cor:sysH_addnoise}}
  \\&= \frac{\Swqq(\omega) + \Swvv(\omega)}{\Swqq(\omega)/\FH(\omega)}
    && \text{by \prope{LTI} hypothesis (A)}
    && \text{and \prefp{cor:Swxy}}
  \\&= \FH(\omega)\brs{1 + \frac{\Swvv(\omega)}{\Swqq(\omega)}}
    && \text{by hypotheses (A) and (B)}
    && \text{and \prefp{cor:Swxy}}
\end{align*}
\end{proof}

%---------------------------------------
\begin{corollary} %[LTI no output noise]
\label{cor:estH1}
\label{cor:estH2}
%---------------------------------------
Let $\opS$ be the \structe{system} illustrated in \prefpp{fig:addnoise}.
\corbox{\begin{array}{lcl}
  \brb{\begin{array}{FMM}
      (A).& hypotheses of \pref{thm:estH1}    & and
    \\(B).& $\rvu(n)=\rvw(n)=0$               & (\prope{no input noise})
  \end{array}}
  &\implies&
  \brb{\begin{array}{>{\ds}rc>{\ds}lD}
    \estH_1(\omega) &=& \FH(\omega) & (\prope{unbiased})
  \end{array}}
  \\
  \brb{\begin{array}{FMM}
      (A).& hypotheses of \pref{thm:estH1}    & and
    \\(B).& $\rvv(n)=0$                       & (\prope{no output noise})
  \end{array}}
  &\implies&
  \brb{\begin{array}{>{\ds}rc>{\ds}lD}
    \estH_2(\omega) &=& \FH(\omega) & (\prope{unbiased})
  \end{array}}
\end{array}}
\end{corollary}

%=======================================
\section{Coherence}
%=======================================
%---------------------------------------
\begin{definition}
\footnote{
  \citerppgc{liang2015}{363}{365}{1498702341}{7.4.2 Coherence function},
  \citerpgc{ewins1986}{131}{0863800173}{$\gamma^2=\ffrac{H_1(\omega)}{H_2(\omega)}$ (3.8)}
  }
%---------------------------------------
Let $\opS$ be a \structe{system} with input $\rvx(n)$ and output $\rvy(n)$.
\defboxt{
  The \fnctd{coherence} function $\ds\Cxy(\omega) \eqd \frac{\Swyx(\omega)}{\Swxx(\omega)}$
  }
\end{definition}

%=======================================
\section{Beware of estimators}
%=======================================
Estimators yield, as the name implies, estimates.
These estimates in general contain some error.

%---------------------------------------
\begin{example}[The K=1 Welch estimate of coherence]
%---------------------------------------
Suppose we have two \prope{uncorrelated} stationary sequences $\rvx(n)$ and $\rvy(n)$. Then, there
CSD $\Sxy(\omega)$ should be $0$ because
\begin{align*}
  \Sxy(\omega)
    &\eqd \opDTFT\pE\Rxy(m)
  \\&=    \opDTFT\pE\brs{x(n)y[n+m]}
  \\&=    \opDTFT\brs{\pEx(n)} \brs{\pEy[n+m]}
  \\&=    \opDTFT\brs{0} \brs{0}
  \\&=    0
\end{align*}

This will give a coherence of $0$ also:
\[ C(\omega) = \frac{\Sxy}{\sqrt{\Sxx\Syy}} = 0\]

However, the Welch estimate with $K=1$ will yield
\begin{align*}
  \abs{C(\omega)}
    &= \abs{\frac{\ds\Sxy}{\sqrt{\ds\Sxx\Syy}}}
  \\&= \abs{\frac{\ds (\opFT x)(\opFT y)^\ast}{\sqrt{\ds\abs{\opFT x}^2\abs{\opFT y}^2}}}
  \\&= 1
\end{align*}

\end{example}


