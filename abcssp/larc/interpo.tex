%============================================================================
% Daniel J. Greenhoe
% LaTeX file
%============================================================================




%=======================================
%\section{Interpolation}
%=======================================
%=======================================
\section{Polynomial interpolation}
%=======================================
%=======================================
\subsection{Lagrange interpolation}
%=======================================
%--------------------------------------
\begin{definition}
\citetbl{
  \citePp{waring1779}{60},
  \citePpc{euler1783}{165}{\textsection. 10. Problema 2. Corollarium 3.},
  \citeP{gauss1866},
  \citeP{lagrange1877},
  \citerp{matthews}{206},
  \citePc{meijering2002}{historical background}
  }
\label{def:Ln(x)}
\label{def:lagpoly}
\label{def:laginterp}
\index{polynomial!Lagrange}
%--------------------------------------
The \hid{Lagrange polynomial} $L_{P,n}(x)$ with respect to the $n+1$ points \\
$P=\set{(x_k,y_k)}{k=0,1,2,\ldots,n}$ is defined as
\defbox{
  L_{P,n}(x) \eqd \sum_{k=0}^n y_k \prod_{m\ne n} \frac{x-x_m}{x_k-x_m}
  }
\end{definition}

%--------------------------------------
\begin{proposition}
%--------------------------------------
Let $L_{P,n}(x)$ be the Lagrange polynomial with respect to the points\\
$P=\set{(x_k,y_k)}{k=0,1,2,\ldots,n}$.
\propbox{\begin{tabular}{ll}
  1.& $L_{P,n}(x)$ is an $n$th order polynomial. \\
  2.& $L_{P,n}(x)$ intersects all $n+1$ points in $P$.
\end{tabular}}
\end{proposition}

%--------------------------------------
\begin{example}[Lagrange interpolation]
%--------------------------------------
The {Lagrange polynomial} $L_{P,3}(x)$ with respect to the $4$ points\\
$P=\{(-2,1),\; (-1,3),\; (3,2),\; (5,4)\}$ is
\exbox{
  L_{P,3}(x) = \frac{   79}{840} x^3 
             + \frac{- 378}{840} x^2
             + \frac{-   7}{840} x
             + \frac{ 2970}{840}
  }
\end{example}
\begin{proof}
\begin{align*}
  L_{P,3}(x)
    &= \sum_{k=0}^n y_k \prod_{m\ne n} \frac{x-x_m}{x_k-x_m}
    \qquad\text{by Definition~\ref{def:Ln(x)}}
  \\&=  y_0\frac{(x+1)(x-3)(x-5)}{(x_0-x_1)(x_0-x_2)(x_0-x_3)} +
        y_1\frac{(x+2)(x-3)(x-5)}{(x_1-x_0)(x_1-x_2)(x_1-x_3)} 
  \\&\quad+
        y_2\frac{(x+2)(x+1)(x-5)}{(x_2-x_0)(x_2-x_1)(x_2-x_3)} +
        y_3\frac{(x+2)(x+1)(x-3)}{(x_3-x_0)(x_3-x_1)(x_3-x_2)}
%
  \\&=  1\frac{(x+1)(x-3)(x-5)}{(-2+1)(-2-3)(-2-5)} +
        3\frac{(x+2)(x-3)(x-5)}{(-1+2)(-1-3)(-1-5)} 
  \\&\quad+
        2\frac{(x+2)(x+1)(x-5)}{(3+2)(3+1)(3-5)}    +
        4\frac{(x+2)(x+1)(x-3)}{(5+2)(5+1)(5-3)}
  \\&=  1\mcom{\frac{x^3-7x^2+7x+15}{-35} }{roots=$-1,3,5$}   +
        3\mcom{\frac{x^3-6x^2-x+30}{24}  }{roots=$-2,3,5$}   +
        2\mcom{\frac{x^3-2x^2-13x-10}{-40}}{roots=$-2,-1,5$}  +
        4\mcom{\frac{x^3-7x-6}{84}        }{roots=$-2,-1,3$}
  \\&= -  \frac{x^3-7x^2+7x+15}{35}  
       +  \frac{x^3-6x^2-x+30}{8}   
       -  \frac{x^3-2x^2-13x-10}{20}  
       +  \frac{x^3-7x-6}{21}       
  \\&=   x^3 \left(
         \frac{-8\cdot20\cdot21 +35\cdot20\cdot21 - 35\cdot8\cdot21 + 35\cdot8\cdot20}
              {35 \cdot 8 \cdot 20 \cdot 21}
             \right)
  \\&\quad
         +x^2 \left(
         \frac{7\cdot8\cdot20\cdot21 -6\cdot35\cdot20\cdot21+ 2\cdot35\cdot8\cdot21+0\cdot35\cdot8\cdot20}
              {35 \cdot 8 \cdot 20 \cdot 21}
             \right)
  \\&\quad
         +x \left(
         \frac{-7\cdot8\cdot20\cdot21 -35\cdot20\cdot21+13\cdot35\cdot8\cdot21-7\cdot35\cdot8\cdot20}
              {35 \cdot 8 \cdot 20 \cdot 21}
             \right)
  \\&\quad
         + \left(
         \frac{-15\cdot8\cdot20\cdot21 +30\cdot35\cdot20\cdot21+10\cdot35\cdot8\cdot21-6\cdot35\cdot8\cdot20}
              {35 \cdot 8 \cdot 20 \cdot 21}
             \right)
  \\&=   \frac{ 11060}{117600} x^3
       + \frac{-52920}{117600} x^2
       + \frac{-  980}{117600} x
       + \frac{415800}{117600}
  \\&=   \frac{   79}{840} x^3
       + \frac{- 378}{840} x^2
       + \frac{-   7}{840} x
       + \frac{ 2970}{840}
\end{align*}
\end{proof}


%=======================================
\subsection{Newton interpolation}
%=======================================
%--------------------------------------
\begin{definition}
\citetbl{
  \citeI{newton1711},
  \citeIppc{fraser1919}{9}{17}{Methodus differentialis: ``A photographic reproduction of the original Latin text"},
  \citeIppc{fraser1919}{18}{25}{Methodus differentialis: English translation},
  \citerppc{fraser1919}{1}{8}{historical background and notes},
  \citePc{meijering2002}{historical background},
  \citerp{matthews}{220}
  }
\label{def:Nn(x)}
\label{def:newpoly}
\label{def:newinterp}
\index{polynomial!Newton}
%--------------------------------------
The \hid{Newton polynomial} $N_{P,n}(x)$ with respect to the $n+1$ points\\
$P=\set{(x_k,y_k)}{k=0,1,2,\ldots,n}$ is defined as
\defbox{
  N_{P,n}(x) \eqd \sum_{k=0}^n \alpha_k \prod_{m=0}^k (x-x_m)
  }
\end{definition}


%--------------------------------------
\begin{proposition}
%--------------------------------------
Let $N_{P,n}(x)$ be the Newton polynomial with respect to the points\\
$P=\set{(x_k,y_k)}{k=0,1,2,\ldots,n}$.
\propbox{\begin{tabular}{ll}
  1.& $N_{P,n}(x)$ is an $n$th order polynomial. \\
  2.& $N_{P,n}(x)$ intersects all $n+1$ points in $P$.
\end{tabular}}
\end{proposition}

%--------------------------------------
\begin{example}[Newton polynomial interpolation]
\index{polynomial!Newton}
%--------------------------------------
\exboxp{
The {Newton polynomial} $N_{P,3}(x)$ with respect to the $4$ points 
$P=\{(-2,1),\; (-1,3),\; (3,2),\; (5,4)\}$ is
\\\indentx$
  N_{P,3}(x) = \frac{   79}{840} x^3
             + \frac{- 378}{840} x^2
             + \frac{-   7}{840} x
             + \frac{ 2970}{840}
$
  }
\end{example}
\begin{proof}
\begin{align*}
  N_{P,3}(x)
    &= \sum_{k=0}^n \alpha_k \prod_{m=1}^k (x-x_m)
  \\&= \alpha_0 + \alpha_1(x-x_0) + \alpha_2(x-x_0)(x-x_1) + \alpha_3(x-x_0)(x-x_1)(x-x_2) 
  \\&= \alpha_0 + \alpha_1(x+2) + \alpha_2(x+2)(x+1) + \alpha_3(x+2)(x+1)(x-3) 
  \\&= \alpha_0 + \alpha_1(x+2) + \alpha_2(x^2+3x+2) + \alpha_3(x^3-7x-6)
  \\&= x^3(\alpha_3) + x^2(\alpha_2) + x(-7\alpha_3+3\alpha_2+\alpha_1)
      +(-6\alpha_3 + 2\alpha_2 + 2\alpha_1 + \alpha_0)
  \\&= \Big[ \alpha_0 \; \alpha_1 \; \alpha_2 \; \alpha_3 \Big] 
       \left[\begin{array}{rrrr}
          1  &   0  & 0  & 0  \\
          2  &   1  & 0  & 0  \\
          2  &   3  & 1  & 0  \\
         -6  &  -7  & 0  & 1
       \end{array}\right]
       \left[\begin{array}{l}
         1   \\
         x   \\
         x^2 \\
         x^3
       \end{array}\right]
  \\
  \\
  \left[\begin{array}{l}
     1  \\
     3  \\
     2  \\
     4  
  \end{array}\right]
    &= \left[\begin{array}{l}
         y_0 \\
         y_1 \\
         y_2 \\
         y_3 
       \end{array}\right]
  \\&= \left[\begin{array}{llll}
         1 & 0         & 0                  & 0                           \\
         1 & (x_1-x_0) & 0                  & 0                           \\
         1 & (x_2-x_0) & (x_2-x_0)(x_2-x_1) & 0                           \\
         1 & (x_3-x_0) & (x_3-x_0)(x_3-x_1) & (x_3-x_0)(x_3-x_1)(x_3-x_2) 
       \end{array}\right]
       \left[\begin{array}{l}
         \alpha_0 \\
         \alpha_1 \\
         \alpha_2 \\
         \alpha_3 
       \end{array}\right]
  \\&= \left[\begin{array}{llll}
         1 & 0     & 0          & 0                           \\
         1 & (-1+2)& 0          & 0                           \\
         1 & (3+2) & (3+2)(3+1) & 0                           \\
         1 & (5+2) & (5+2)(5+1) & (5+2)(5+1)(5-3) 
       \end{array}\right]
       \left[\begin{array}{l}
         \alpha_0 \\
         \alpha_1 \\
         \alpha_2 \\
         \alpha_3 
       \end{array}\right]
  \\&= \left[\begin{array}{llll}
         1 & 0     & 0          & 0                           \\
         1 & 1     & 0          & 0                           \\
         1 & 5     & 20         & 0                           \\
         1 & 7     & 42         & 84                
       \end{array}\right]
       \left[\begin{array}{l}
         \alpha_0 \\
         \alpha_1 \\
         \alpha_2 \\
         \alpha_3 
       \end{array}\right]
  \end{align*}

  \begin{align*}
  \left[\begin{array}{*{8}{r}}
    1 & 0 & 0  & 0  & 1 & 0 & 0 & 0 \\
    1 & 1 & 0  & 0  & 0 & 1 & 0 & 0 \\
    1 & 5 & 20 & 0  & 0 & 0 & 1 & 0 \\
    1 & 7 & 42 & 84 & 0 & 0 & 0 & 1
  \end{array}\right]
    &= \left[\begin{array}{*{8}{r}}
         1 & 0 & 0  & 0  & 1 & 0 & 0 & 0 \\
         0 & 1 & 0  & 0  &-1 & 1 & 0 & 0 \\
         0 & 5 & 20 & 0  &-1 & 0 & 1 & 0 \\
         0 & 7 & 42 & 84 &-1 & 0 & 0 & 1
       \end{array}\right]
  \\&= \left[\begin{array}{*{8}{r}}
         1 & 0 & 0  & 0  & 1 & 0 & 0 & 0 \\
         0 & 1 & 0  & 0  &-1 & 1 & 0 & 0 \\
         0 & 0 & 20 & 0  & 4 &-5 & 1 & 0 \\
         0 & 0 & 42 & 84 & 6 &-7 & 0 & 1
       \end{array}\right]
  \\&= \left[\begin{array}{*{8}{r}}
         1 & 0 & 0  & 0  & 1 & 0 & 0 & 0 \\
         0 & 1 & 0  & 0  &-1 & 1 & 0 & 0 \\
         0 & 0 & 1  & 0  &\frac{1}{5} &-\frac{1}{4} & \frac{1}{20} & 0 \\
         0 & 0 & 42 & 84 & 6 &-7 & 0 & 1
       \end{array}\right]
  \\&= \left[\begin{array}{*{8}{r}}
         1 & 0 & 0  & 0  & 1 & 0 & 0 & 0 \\
         0 & 1 & 0  & 0  &-1 & 1 & 0 & 0 \\
         0 & 0 & 1  & 0  &\frac{1}{5} &-\frac{1}{4} & \frac{1}{20} & 0 \\
         0 & 0 & 0  & 84 & 6-\frac{42}{5} &-7+\frac{42}{4} & -\frac{42}{20} & 1
       \end{array}\right]
  \\&= \left[\begin{array}{*{8}{r}}
         1 & 0 & 0  & 0  & 1 & 0 & 0 & 0 \\
         0 & 1 & 0  & 0  &-1 & 1 & 0 & 0 \\
         0 & 0 & 1  & 0  &\frac{1}{5} &-\frac{1}{4} & \frac{1}{20} & 0 \\
         0 & 0 & 0  & 84 & -\frac{12}{5} &\frac{14}{4} & -\frac{42}{20} & 1
       \end{array}\right]
  \\&= \left[\begin{array}{*{8}{r}}
         1 & 0 & 0  & 0  & 1 & 0 & 0 & 0 \\
         0 & 1 & 0  & 0  &-1 & 1 & 0 & 0 \\
         0 & 0 & 1  & 0  &\frac{4}{20} &-\frac{5}{20} & \frac{1}{20} & 0 \\
         0 & 0 & 0  & 84 & -\frac{24}{10} &\frac{35}{10} & -\frac{21}{10} & \frac{10}{10}
       \end{array}\right]
  \\&= \left[\begin{array}{*{8}{r}}
         1 & 0 & 0  & 0  & 1 & 0 & 0 & 0 \\
         0 & 1 & 0  & 0  &-1 & 1 & 0 & 0 \\
         0 & 0 & 1  & 0  &\frac{4}{20} &-\frac{5}{20} & \frac{1}{20} & 0 \\
         0 & 0 & 0  & 1  & -\frac{24}{840} &\frac{35}{840} & -\frac{21}{840} & \frac{10}{840}
       \end{array}\right]
  \end{align*}

  \begin{align*}
  \left[\begin{array}{l}
    \alpha_0 \\
    \alpha_1 \\
    \alpha_2 \\
    \alpha_3 
  \end{array}\right]
    &= \left[\begin{array}{*{4}{r}}
          1 & 0 & 0 & 0 \\
         -1 & 1 & 0 & 0 \\
         \frac{4}{20} &-\frac{5}{20} & \frac{1}{20} & 0 \\
          -\frac{24}{840} &\frac{35}{840} & -\frac{21}{840} & \frac{10}{840}
       \end{array}\right]
       \left[\begin{array}{l}
         1 \\
         3 \\
         2 \\
         4 
       \end{array}\right]
  \\&= \left[\begin{array}{l}
         1 \\
         2 \\
         -\frac{9}{20} \\
         \frac{79}{840} 
       \end{array}\right]
  \\
  \\
  \\
  N_{P,3}(x)
    &= \Big[ \alpha_0 \; \alpha_1 \; \alpha_2 \; \alpha_3 \Big] 
       \left[\begin{array}{rrrr}
          1  &   0  & 0  & 0  \\
          2  &   1  & 0  & 0  \\
          2  &   3  & 1  & 0  \\
         -6  &  -7  & 0  & 1
       \end{array}\right]
       \left[\begin{array}{l}
         1   \\
         x   \\
         x^2 \\
         x^3
       \end{array}\right]
  \\&= \left[\begin{array}{c|c|c|c} 
         1 & 2 & -\frac{9}{20} & \frac{79}{840} 
       \end{array}\right]
       \left[\begin{array}{rrrr}
          1  &   0  & 0  & 0  \\
          2  &   1  & 0  & 0  \\
          2  &   3  & 1  & 0  \\
         -6  &  -7  & 0  & 1
       \end{array}\right]
       \left[\begin{array}{l}
         1   \\
         x   \\
         x^2 \\
         x^3
       \end{array}\right]
  \\&= \left[\begin{array}{c|c|c|c}
          1 + 4 - \frac{9}{10} - \frac{79}{140}  &
          2 - \frac{27}{20} -\frac{79}{120}      &
          -\frac{9}{20}                          &
          \frac{79}{840}
       \end{array}\right]
       \left[\begin{array}{l}
         1   \\
         x   \\
         x^2 \\
         x^3
       \end{array}\right]
  \\&= \frac{79}{840}x^3 - \frac{378}{840}x^2 - \frac{7}{840}x + \frac{2970}{840}
\end{align*}
\end{proof}



%
%%---------------------------------------
%\begin{example}[Least squares polynomial interpolation]
%\index{polynomial!least squares}
%\index{least squares}
%\label{ex:ls}
%%---------------------------------------
%The best 3rd order polynomial in the \hib{least squares} $S_{P,3}(x)$ sense
%with respect to the $4$ points\\
%$P=\{(-2,1),\; (-1,3),\; (3,2),\; (5,4)\}$ is
%\exbox{
%  S_{P,3}(x) = \frac{   79}{840} x^3
%             + \frac{- 378}{840} x^2
%             + \frac{-   7}{840} x
%             + \frac{ 2970}{840}
%  }
%\end{example}
%\begin{proof}
%%The {\em least squares} technique is presented in 
%%Section~\ref{sec:ls} (page~\pageref{sec:ls}).
%\[
%  \vx
%    =
%    \left[\begin{array}{l}
%       x_0  \\
%       x_1  \\
%       x_2  \\
%       x_3  
%    \end{array}\right] 
%    =
%    \left[\begin{array}{r}
%       -2   \\
%       -1   \\
%        3   \\
%        5   
%    \end{array}\right] 
%   \qquad\qquad
%   \vy 
%     =
%     \left[\begin{array}{r}
%         1   \\
%         3   \\
%         2   \\
%         4   \\
%     \end{array}\right] 
%     =
%     \left[\begin{array}{l}
%         1   \\
%         3   \\
%         2   \\
%         4   \\
%     \end{array}\right] 
%\]
%
%We want to find a third order polynomial 
%  \[ d x^3 + c x^2 + bx + a \]
%that best approximates
%the 4 points in the least squares sense.
%We define the matrix $U$ (known) and vector $\estn$ (to be computed)
%as follows:
%\[
%   U^H \eqd 
%   \left[\begin{array}{llll}
%      1  & x_0 & x_0^2 & x_0^3  \\
%      1  & x_1 & x_1^2 & x_1^3  \\
%      1  & x_2 & x_2^2 & x_2^3  \\
%      1  & x_3 & x_3^2 & x_3^3  
%   \end{array}\right]
%   \qquad\qquad
%   \estn \eqd 
%   \left[\begin{array}{l}
%      a  \\
%      b  \\
%      c  \\
%      d
%   \end{array}\right] 
%\]
%Then, %using Theorem~\ref{thm:ls} (page~\ref{thm:ls}), 
%the best coefficients for the polynomial are
%\begin{align*}
%  \estn
%    &= \left[\begin{array}{l}
%          a  \\
%          b  \\
%          c  \\
%          d
%       \end{array}\right] 
%  \\&= R^{-1}W
%  \\&= (UU^H)^{-1}\; (U\vy)
%  \\&= \left(
%   \left[\begin{array}{llll}
%      1  & x_0 & x_0^2 & x_0^3  \\
%      1  & x_1 & x_1^2 & x_1^3  \\
%      1  & x_2 & x_2^2 & x_2^3  \\
%      1  & x_3 & x_3^2 & x_3^3  
%   \end{array}\right]^H
%   \left[\begin{array}{llll}
%      1  & x_0 & x_0^2 & x_0^3  \\
%      1  & x_1 & x_1^2 & x_1^3  \\
%      1  & x_2 & x_2^2 & x_2^3  \\
%      1  & x_3 & x_3^2 & x_3^3  
%   \end{array}\right]
%       \right)^{-1}
%       \left(
%   \left[\begin{array}{llll}
%      1  & x_0 & x_0^2 & x_0^3  \\
%      1  & x_1 & x_1^2 & x_1^3  \\
%      1  & x_2 & x_2^2 & x_2^3  \\
%      1  & x_3 & x_3^2 & x_3^3  
%   \end{array}\right]^H
%       \left[\begin{array}{l}
%          y_0  \\
%          y_1  \\
%          y_2  \\
%          y_3  
%       \end{array}\right] 
%       \right)
%  \\&= \left(
%   \left[\begin{array}{rrrr}
%      1  & (-2) & (-2)^2 & (-2)^3  \\
%      1  & (-1) & (-1)^2 & (-1)^3  \\
%      1  & (3) & (3)^2 & (3)^3  \\
%      1  & (5) & (5)^2 & (5)^3  
%   \end{array}\right]^H
%   \left[\begin{array}{rrrr}
%      1  & (-2) & (-2)^2 & (-2)^3  \\
%      1  & (-1) & (-1)^2 & (-1)^3  \\
%      1  & (3) & (3)^2 & (3)^3  \\
%      1  & (5) & (5)^2 & (5)^3  
%   \end{array}\right]
%       \right)^{-1}
%       \left(
%   \left[\begin{array}{rrrr}
%      1  & (-2) & (-2)^2 & (-2)^3  \\
%      1  & (-1) & (-1)^2 & (-1)^3  \\
%      1  & (3) & (3)^2 & (3)^3  \\
%      1  & (5) & (5)^2 & (5)^3  
%   \end{array}\right]^H
%       \left[\begin{array}{l}
%          1  \\
%          3  \\
%          2  \\
%          4  
%       \end{array}\right] 
%       \right)
%  \\&= \left(
%   \left[\begin{array}{rrrr}
%      1  & -2 &  4 &  -8  \\
%      1  & -1 &  1 &  -1  \\
%      1  &  3 &  9 &  27  \\
%      1  &  5 & 25 & 125  
%   \end{array}\right]^H
%   \left[\begin{array}{rrrr}
%      1  & -2 &  4 &  -8  \\
%      1  & -1 &  1 &  -1  \\
%      1  &  3 &  9 &  27  \\
%      1  &  5 & 25 & 125  
%   \end{array}\right]
%       \right)^{-1}
%       \left(
%   \left[\begin{array}{rrrr}
%      1  & -2 &  4 &  -8  \\
%      1  & -1 &  1 &  -1  \\
%      1  &  3 &  9 &  27  \\
%      1  &  5 & 25 & 125  
%   \end{array}\right]^H
%       \left[\begin{array}{l}
%          1  \\
%          3  \\
%          2  \\
%          4  
%       \end{array}\right] 
%       \right)
%\\&= \left[\begin{array}{r}
%       2970   \\
%         -7   \\
%       -378   \\
%         79
%     \end{array}\right]
%\end{align*}
%\end{proof}
%
%
%
%
%
%
%
%%=======================================
%\subsection{Hermite interpolation}
%\label{sec:interpo_hermite}
%%=======================================
%
%\begin{table}
%\footnotesize
%\exbox{
%  \renewcommand{\arraystretch}{1}
%  \renewcommand{\arraycolsep}{0.4ex}
%  \begin{array}{r|@{\qquad}*{13}{r}}
%     p & \mc{13}{l}{\ds(1-y)^p\fP_m(y)= (1-y)^p \sum_{k=0}^{p-1} {p-1+k\choose k} y^k}
%  %    p & \mc{20}{l}{(1-y)^p\fP_m(y)}
%  \\ \hline
%     1 & 1 &-&   y
%  \\ 2 & 1 &-&  3y^2 &+&  2y^3
%  \\ 3 & 1 &-& 10y^3 &+&  15y^4 &-&   6y^5
%  \\ 4 & 1 &-&  35y^4 &+&  84y^5 &-&   70y^6  &+&    20y^7
%  \\ 5 & 1 &-& 126y^5 &+&  420y^6 &-&  540y^7 &+&   315y^8 &-&    70y^9
%  \\ 6 & 1 &-& 462y^6 &+& 1980y^7 &-& 3465y^8 &+&  3080y^9 &-&  1386y^{10} &+& 252y^{11}
%  \end{array}}
%\caption{
%  Low-pass term $(1-y)^p\fP_m(y)$
%  \label{tbl:P_lp}
%  }
%\end{table}
%
%
%
%
%\begin{figure}[h]
%\begin{scriptsize}
%\begin{center}
%\color[rgb]{0.2,0.2,0.2}
%\begin{fsL}
%\setlength{\unitlength}{0.3mm}
%\begin{picture}(335,150)(-10,-30)
%  %\graphpaper[10](-100,0)(300,150)
%  \thicklines
%  \put(   0,   0 ){\line(1,0){240} }
%  \put(   0,   0 ){\line(0,1){120} }
%
%  \qbezier[10](100,50)(100, 25)( 100,0)
%  \qbezier[40](0,100)(100,100)( 200,100)
%
%  {\color{blue}
%    \qbezier(   0, 100)(  60, 100)( 100,  50)
%    \qbezier( 100,  50)( 140,   0)( 200,   0)
%    \put( 130,  25 ){\makebox(0,0)[l]{$\leftarrow (1-y)^p \fP(y)$} }
%    \put(   5, 105 ){\makebox(0,0)[lb]{$\downarrow$ first $p-1$ derivatives are zero at $y=0$} }
%    \put( 200,   5 ){\makebox(0,0)[lb]{$\downarrow$ first $p-1$ derivatives are zero at $y=1$} }
%    }
%
%  {\color{red}
%    \qbezier(   0,   0)(  60,   0)( 100,  50)
%    \qbezier( 100,  50)( 140, 100)( 200, 100)
%    \put( 130,  75 ){\makebox(0,0)[l]{$\leftarrow y^p \fP(1-y)$} }
%    \put(   5,  -3 ){\makebox(0,0)[lt]{$\uparrow$ first $p-1$ derivatives are zero at $y=0$} }
%    \put( 200,  95 ){\makebox(0,0)[lt]{$\uparrow$ first $p-1$ derivatives are zero at $y=1$} }
%    }
%
%  \put( 100,   3 ){\makebox(0,0)[b]{$\frac{\pi}{2}$} }
%  \put( 260,   0 ){\makebox(0,0)[l]{$\omega$} }
%  \put( -10, 100 ){\makebox(0,0)[r]{$1$} }
%\end{picture}
%\end{fsL}
%\end{center}
%\end{scriptsize}
%\caption{
%  Polynomial quadrature condition low-pass and high-pass terms
%  \label{fig:lphp}
%  }
%\end{figure}
%\prefpp{lem:quadcon_y} expresses the quadrature condition
%as a polynomial in $y=\sin^2\frac{\omega}{2}$.
%The first term in this polynomial quadrature condition is a
%low-pass response and the second term is a high pass;
%and they meet in the middle at $\omega=\frac{\pi}{2}$.
%    \[
%      \mcom{(1-y)^p \fP(y)}{low-pass} + \mcom{y^p \fP(1-y)}{high-pass} = 1
%    \]
%The low-pass and high-pass terms are especially smooth at
%$\omega=0$ ($y=0$) and $\omega=\pi$ ($y=1$)
%in that the first $p-1$ derivatives at both points are zero
%for both terms.
%This is illustrated in \prefpp{fig:lphp}.
%%--------------------------------------
%\begin{theorem}[\thme{Hermite Interpolation}]
%\label{thm:interpo_hermite}
%%--------------------------------------
%\thmbox{\begin{array}{*{4}{>{\ds}l}}
%  \left.\deriv{^n}{y^n} \left[
%        (1-y)^p \sum_{k=0}^{p-1}{p+k-1\choose k} y^k
%        \right]\right|_{y=0}
%    &=& \kdelta_n
%    &   \text{ for } n=0,1,2,\ldots,p-1
%  \\
%  \left.\deriv{^n}{y^n} \left[
%        (1-y)^p \sum_{k=0}^{p-1} {p+k-1\choose k} y^k
%        \right]\right|_{y=1}
%    &=& 0
%    &   \text{ for } n=0,1,2,\ldots,p-1
%  \\
%  \left.\deriv{^n}{y^n} \left[
%        y^p \sum_{k=0}^{p-1}{p+k-1\choose k} (1-y)^k
%        \right]\right|_{y=0}
%    &=& 0
%    &   \text{ for } n=0,1,2,\ldots,p-1
%  \\
%  \left.\deriv{^n}{y^n} \left[
%        y^p \sum_{k=0}^{p-1} {p+k-1\choose k} (1-y)^k
%        \right]\right|_{y=1}
%    &=& \kdelta_n
%    &   \text{ for } n=0,1,2,\ldots,p-1
%  \end{array}}
%\end{theorem}
%\begin{proof}
%Let
%\begin{align*}
%  \ff(y) &\eqd (1-y)^p \sum_{n=0}^{p-1} {p-1+n \choose n} y^n      \\
%  \fg(y) &\eqd y^p     \sum_{n=0}^{p-1} {p-1+n \choose n} (1-y)^n  \\
%  q      &\eqd p-1
%\end{align*}
%%\begin{enumerate}
%%  \item Proofs for $\ff^{(n)}(0)$:
%  \begin{enumerate}
%  \item Proof that $\ff(0)=1$:
%  \begin{align*}
%  \ff(0)
%    &= \left.(1-y)^p \sum_{m=0}^{p-1} {p-1+m\choose m} y^m \right|_{y=0}
%  \\&= \left.(1-y)^p \left[{p-1 \choose 0} + \sum_{m=1}^{p-1} {p-1+m\choose m} y^m\right]\right|_{y=0}
%  \\&= 1
%  \end{align*}
%
%  \item Proof that
%      $\ds\ff(y)=
%       p \sum_{n=0}^{2p-1}
%       \left[\sum_{k=\max(0,n-q)}^{\min(n,p)} (-1)^k \frac{(p+n-k-1)!}{(p-k)!(n-k)!\,k!}\right]
%       y^n$:
%  \begin{align*}
%  (1-y)^p \fP_m(y)
%    &= \sum_{n=0}^p     {p\choose n} (-1)^n y^n
%       \sum_{m=0}^{p-1} {p-1+m\choose m} y^m
%  \\&= \sum_{n=0}^{2p-1} \sum_{k=\max(0,n-q)}^{\min(n,p)}
%       {p\choose k} (-1)^k {p-1+n-k\choose n-k} y^n
%    && \text{by \pref{thm:polymult} \prefpo{thm:polymult}}
%  \\&= \sum_{n=0}^{2p-1} \sum_{k=\max(0,n-q)}^{\min(n,p)}
%       (-1)^k \frac{p!}{(p-k)!k!} \frac{(p-1+n-k)!}{(p-1)!(n-k)!} y^n
%  \\&= p \sum_{n=0}^{2p-1}
%       \left[\sum_{k=\max(0,n-q)}^{\min(n,p)} (-1)^k \frac{(p+n-k-1)!}{(p-k)!(n-k)!\,k!}\right]
%       y^n
%  \end{align*}
%
%  \item Proof that $\ff^{(n)}(0)=\kdelta_n$ for $n=0,1,2,\ldots,p-1$:
%  \begin{align*}
%  \left.\deriv{^n}{y^n}\left[ (1-y)^p \fP_m(y) \right]\right|_{y=0}
%    &= \left.
%       \deriv{^n}{y^n}\left[
%         p \sum_{m=0}^{2p-1}
%         \left[\sum_{k=\max(0,m-q)}^{\min(m,p)} (-1)^k \frac{(p+m-k-1)!}{(p-k)!(m-k)!\,k!}\right]
%         y^m
%       \right]
%       \right|_{y=0}
%    && \text{by 1.}
%  \\&= \left. p
%       \sum_{m=n}^{2p-1} \sum_{k=\max(0,m-q)}^{\min(m,p)}
%       (-1)^k \frac{(p-1+m-k)!}{(p-k)!(m-k)!k!} \frac{m!}{(m-n)!}y^{m-n}
%       \right|_{y=0}
%  \\&= p \sum_{k=\max(0,n-q)}^{\min(n,p)}
%       (-1)^k \frac{(p-1+n-k)!}{(p-k)!} \frac{n!}{(n-k)!k!}
%  \\&= p \sum_{k=0}^n
%       (-1)^k {n\choose k} \frac{(p+n-k-1)!}{(p-k)!}
%  \\&\eqq \kdelta_n \qquad\text{for } n=0,1,2,\ldots,p-1
%  \end{align*}
%
%  \item Proof that $\ff^{(n)}(0)=\kdelta_n$ for $n=0,1,2,\ldots,p-1$:
%  \begin{align*}
%  &\left.\deriv{^n}{y^n}\left[ (1-y)^p \fP_m(y) \right]\right|_{y=0}
%  \\&= \left.
%       \sum_{k=0}^n {n\choose k}
%       \left[\deriv{^{n-k}}{y^{n-k}} (1-y)^p \right]
%       \left[\deriv{^k}{y^k}\fP_m(y) \right]
%       \right|_{y=0}
%    \qquad\text{by \pref{lem:LGPR} (Leibnitz rule)}
%  \\&= \left.
%       \sum_{k=0}^n {n\choose k}
%       \left[\deriv{^{n-k}}{y^{n-k}} (1-y)^p \right]
%       \left[\deriv{^k}{y^k} \sum_{m=0}^{p-1}{p-1+m \choose m} y^m \right]
%       \right|_{y=0}
%    \quad\text{by definition of $\fP_m(y)$}
%  \\&= \left.
%       \sum_{k=0}^n {n\choose k}
%       \left[(-1)^{n-k}\frac{p!}{(p-n+k)!} (1-y)^{(p-n+k)} \right]
%       \left[\sum_{m=k}^{p-1}{p-1+m \choose m} \frac{m!}{(m-k)!}y^{m-k} \right]
%       \right|_{y=0}
%  \\&= \sum_{k=0}^n {n\choose k}
%       \left[(-1)^{n-k}\frac{p!}{(p-n+k)!}\right]
%       \left[{p-1+k \choose k} k! \right]
%  \\&= \sum_{k=0}^n {n\choose k}
%       \left[(-1)^{n-k}\frac{p!}{(p-n+k)!}\right]
%       \left[\frac{(p-1+k)!}{(p-1)!k!}\; k! \right]
%  \\&= (-1)^n p
%       \sum_{k=0}^n {n\choose k}
%       (-1)^k\frac{(p+k-1)!}{(p+k-n)!}
%  \\&\eqq \kdelta_n \qquad \text{for } k=0,1,2,\ldots,p-1
%  \end{align*}
%
%  \item Proof that $\ff^{(n)}(1)=0$ for $n=0,1,2,\ldots,p-1$:
%  \begin{align*}
%  \left.\deriv{^n}{y^n}\left[ (1-y)^p \fP_m(y) \right]\right|_{y=1}
%    &= \left.
%       \sum_{k=0}^n {n\choose k}
%       \left[\deriv{^k}{y^k} (1-y)^p \right]
%       \fP_m^{(n-k)}(y)
%       \right|_{y=1}
%    && \text{by \pref{lem:LGPR} (Leibnitz rule)}
%  \\&= \left.
%       \sum_{k=0}^n {n\choose k}
%       \left[(-1)^k\frac{p!}{(p-k)!} (1-y)^{p-k} \right]
%       \fP_m^{(n-k)}(y)
%       \right|_{y=1}
%  \\&= \left.
%       \sum_{k=0}^n {n\choose k} \; 0\cdot
%       \fP_m^{(n-k)}(y)
%       \right|_{y=1}
%    && \text{by \pref{lem:LGPR}}
%  \\&= 0 \qquad \text{for } k=0,1,2,\ldots,p-1
%  \end{align*}
%\end{enumerate}
%%\end{enumerate}
%\end{proof}
%
%
%%=======================================
%\subsection{Cardinal Series and Sampling}
%%=======================================
%%======================================
%\subsubsection{Cardinal series basis}
%\label{sec:cardinal}
%%======================================
%The \prope{Paley-Wiener} class of functions (next definition) are those with a bandlimited Fourier transform.
%The cardinal series forms an orthogonal basis for such a space \xrefP{thm:cardinalSeries}.
%The \fncte{Fourier coefficients} \xref{def:fcoef} for a projection of a function $\ff$ onto the Cardinal series basis elements is particularly 
%simple---these coefficients are samples of $\ff$ taken at regular intervals \xrefP{thm:t_sampling}.
%In fact, one could represent the coefficients using inner product notation with the 
%\structe{Dirac delta distribution} $\delta$ \ifxref{relation}{def:dirac} as
%follows:
%  \\\indentx$\ds\inprod{\ff(x)}{\delta(x-nT)} \eqd \int_{\R} \ff(x)\delta(x-nT) \dt \eqd \ff(nT)$
%
%%--------------------------------------
%\begin{definition}
%\citetbl{
%  %\citerp{higgins1985}{56}\\
%  \citerpgc{higgins1996}{52}{0198596995}{Definition 6.15}
%  %\citerp{hardy1941}{332}
%  }
%\label{def:PW}
%%--------------------------------------
%\defbox{\begin{array}{M}
%  A function $\ff\in\clFcc$ is in the \hid{Paley-Wiener} class of functions $\ds\spPW^p_\sigma$ if\\
%  there exists $\fF\in\spL^p\intoo{-\sigma}{\sigma}$ such that 
%  \\\qquad$\ds\ff(x) = \int_{-\sigma}^{\sigma} \fF(\omega)e^{ix\omega}\dw$
%  \qquad({\scs $\ff$ has a \prope{bandlimited} Fourier transform $\fF$  with bandwidth $\sigma$})
%  \\for $p\in\intco{1}{\infty}$ and $\sigma\in\intoo{0}{\infty}$.
%\end{array}}
%\end{definition}
%
%%--------------------------------------
%\begin{theorem}[\thm{Paley-Wiener Theorem for Functions}]
%\citetbl{
%  \citerpgc{boas1954}{103}{0123745829}{6.8.1 Theorem of Paley and Wiener},
%  \citerpgc{katznelson2004}{212}{0521543592}{7.4 Theorem},
%  \citerppgc{zygmund1968v2}{272}{273}{0521890535}{(7$\cdot$2) \scshape Theorem of Paley-Wiener},
%  %\citerpg{yosida1971}{161}{3540055061},
%  \citerpg{yosida1980}{161}{3540586547},
%  \citerpgc{rudinr}{375}{0070542341}{19.3 Theorem},
%  \citerpgc{young2001}{85}{0127729550}{Theorem 18}
%  }
%%--------------------------------------
%Let $\ff$ be an \structe{entire function} (the domain of $\ff$ is the entire complex plane $\C$).
%Let $\sigma\in\Rp$.
%\thmbox{
%  \brb{\ff\in\spPW^2_\sigma}
%  \iff
%  \brb{\begin{array}{F>{\ds}lDD}
%    1. & \exists C\in\Rp\st\abs{\ff(z)} \le Ce^{\sigma\abs{z}} & (\prope{exponential type}) & and\\
%    2. & \ff\in\spLLR
%    %2. & \int_{\R} \abs{\ff(x)}^2 \ds < \infty 
%  \end{array}}
%  }
%\end{theorem}
%
%%--------------------------------------
%\begin{theorem}[\thm{Cardinal series}]
%%\begin{theorem}
%\citetbl{
%  \citerpgc{higgins1996}{52}{0198596995}{Definition 6.15},
%  \citerc{hardy1941}{\prope{orthonormality}},
%  \citerpc{higgins1985}{56}{H1.; historical notes}
%  %
%  %\citerp{higgins1985}{47}\\
%  }
%%\footnotetext{
%%  The {\em sampling theorem} was ``discovered" and published by multiple people: 
%%  Nyquist in 1928 (DSP?), 
%%  Whittaker in 1935 (interpolation theory),
%%  and Shannon in 1949 (communication theory). \\
%%  references: \citerp{mallat}{43} \\
%%  \citerp{os}{143}.
%%  }
%\label{thm:cardinalSeries}
%%--------------------------------------
%%Let $\ff\in\spLLR$ and $\Ff(\omega)$ have bandwidth $W$ such that
%%$\Ff(\omega)=0$ for $\abs{\frac{\omega}{2\pi}}>W$. \\
%%If the sample rate $$ then
%\thmbox{
%  %\mcom{\frac{1}{T}\ge 2\sigma}{sample rate $\frac{1}{T}$ $\ge$ $2\times$ bandwidth $\sigma$}
%  \brb{\frac{1}{T}\ge 2\sigma}
%  \implies
%  \text{The set }
%  \setbigleft{\frac{\sin\left[\frac{\pi}{T}(x-nT)\right]}{\frac{\pi}{T}(x-nT)}}{n\in\Z}
%  \text{ is an \prope{orthonormal} \structe{basis} for $\spPW_\sigma^2$.}
%  }
%\end{theorem}
%
%
%%--------------------------------------
%\begin{theorem}[\thm{Sampling Theorem}]
%%\begin{theorem}
%\citetbl{
%  %\citerpc{higgins1985}{56}{H1.; historical notes}\\
%  %
%  %\citerp{higgins1985}{47},
%  \citerpg{higgins1996}{5}{0198596995},
%  \citerpg{marks1991}{1}{0387973915},
%  \citer{nashed1991},
%  \citor{etwhittaker1915},
%  \citor{kotelnikov1933e},
%  \citor{jmwhittaker1935},
%  \citorc{shannon1948}{Theorem 13},
%  \citorp{shannon1949}{11}
%  }
%\label{thm:t_sampling}
%%\footnotetext{
%%  The {\em sampling theorem} was ``discovered" and published by multiple people: 
%%  Nyquist in 1928 (DSP?), 
%%  Whittaker in 1935 (interpolation theory),
%%  and Shannon in 1949 (communication theory). \\
%%  references: \citerp{mallat}{43} \\
%%  \citerp{os}{143}.
%%  }
%%--------------------------------------
%%Let $\ff\in\spLLR$ and $\Ff(\omega)$ have bandwidth $W$ such that
%%$\Ff(\omega)=0$ for $\abs{\frac{\omega}{2\pi}}>W$. \\
%%If the sample rate  then
%\thmbox{
%  \brb{\begin{array}{F>{\ds}lD}
%    1. & \ff\in\spPW_\sigma^2 & and\\
%    2. & \frac{1}{T}\ge 2\sigma
%  \end{array}}
%  \qquad\implies\qquad
%  \ff(x) = \mcom{\ds\sum_{n=1}^\infty \ff(nT) \frac{\sin\brs{\frac{\pi}{T}(x-nT)}}{\frac{\pi}{T}(x-nT)}}
%                {\hie{Cardinal series}}.
%  }
%\end{theorem}
%\begin{proof}
%\[ \mbox{Let }\hspace{3ex}
%   \fs(x) \eqd \frac{\sin\left[\frac{\pi}{T}x\right]}{\frac{\pi}{T}x}
%   \iff
%   \Fs(\omega) = \left\{\begin{array}{ll}
%      T & : |f|\le \frac{1}{2T} \\
%      0 & : \mbox{otherwise}
%      \end{array}\right.
%\]
%
%\begin{enumerate}
%  \item Proof that the set is \prope{orthonormal}: see \citer{hardy1941}
%
%  \item Proof that the set is a \prope{basis}:
%    \begin{align*}
%      \ff(x)
%        &= \int_\omega \Ff(\omega) e^{i\omega t} \dw
%        && \text{by \thme{inverse Fourier transform} \xref{thm:opFTi}}
%      \\&= \int_\omega T \Ffd(\omega)\Fs(\omega) e^{i\omega t} \dw
%        && \text{if $W\le\frac{1}{2T}$}
%      \\&= T \ffd(x) \conv \fs(x) 
%        && \text{by \thme{Convolution theorem} \xref{thm:conv}}
%      \\&= T \int_u [\ffd(u)] \fs(x-u) \du
%        && \text{by \ope{convolution} definition \xref{def:conv}} 
%      \\&= T \int_u \left[ \sum_n\ff(u)\delta(u-nT) \right] \fs(x-u) \du
%        && \text{by \ope{sampling} definition \xref{thm:f_sampling}}
%      \\&= T \sum_{n\in\Z} \int_u \ff(u)\fs(x-u)\delta(u-nT) \du
%      \\&= T \sum_{n\in\Z} \ff(nT)\fs(x-nT)
%        && \text{by property of \fncte{Dirac delta} distribution}
%      \\&= T \sum_{n\in\Z} \ff(nT) 
%             \frac{\sin\left[\frac{\pi}{T}(x-nT)\right]}
%                  {\frac{\pi}{T}(x-nT)       }
%        && \text{by definition of $\fs(x)$}
%    \end{align*}
%\end{enumerate}
%\end{proof}
%
%
%%======================================
%\subsubsection{Sampling}
%\label{sec:sampling}
%\index{sampling}
%%======================================
%If $\ffd(x)$ is the function $\ff(x)$ sampled at rate $1/T$, 
%then $\Ffd(\omega)$ is simply $\Ff(\omega)$
%{\em replicated} every $1/T$ Hertz and {\em scaled} by $1/T$.
%This is proven in Theorem~\ref{thm:f_sampling} (next) and 
%illustrated in \prefpp{fig:f_sampling}.
%
%
%\begin{figure}[ht]
%\index{nyquist sampling rate}
%\setlength{\unitlength}{0.1mm}
%\begin{center}
%\begin{fsL}
%\begin{tabular}{c}
%%
%\begin{picture}(1550,200)(-750,-50)
%  %\graphpaper[10](0,0)(600,200)
%  \thinlines
%  \put(  40,  70){\makebox(0,0)[lb]{$\Ff(\omega)$} }
%  \put(-750,   0){\line( 1,0){1500} }
%  \put(   0, -50){\line( 0,1){ 200} }
%  \put(-100,   0){\line( 1,1){ 100} }
%  \put( 100,   0){\line(-1,1){ 100} }
%  \put(- 10, 100){\line( 1,0){  20} }
%  \put(- 20, 100){\makebox(0,0)[r]{$A$} }
%  \put(-100, -10){\line( 0,1){  20} }
%  \put( 100, -10){\line( 0,1){  20} }
%  \put(-100, -20){\makebox(0,0)[t]{$-W$} }
%  \put( 100, -20){\makebox(0,0)[t]{$+W$} }
%  \put( 770,   0){\makebox(0,0)[l]{$f$} }
%\end{picture}
%\\
%\begin{picture}(1550,200)(-750,-50)
%  %\graphpaper[10](0,0)(600,200)
%  \thinlines
%  \put(  20, 140){\makebox(0,0)[lt]{$\Ffd(\omega)$ at sample rate $\frac{1}{T}=3W$ (``oversampling")} }
%  \multiput(-600,0)(+300,0){5}{
%     \put(-100,   0){\line( 1,1){ 100} }
%     \put( 100,   0){\line(-1,1){ 100} }
%     }
%  \put(-750,   0){\line( 1,0){1500} }
%  \put(   0, -50){\line( 0,1){ 200} }
%  \put(- 10, 100){\line( 1,0){  20} }
%  \put(- 20, 100){\makebox(0,0)[r]{$\frac{A}{T}$} }
%  \put(-100, -10){\line( 0,1){  20} }
%  \put( 100, -10){\line( 0,1){  20} }
%  \put(-100, -20){\makebox(0,0)[t]{$-W$} }
%  \put( 100, -20){\makebox(0,0)[t]{$+W$} }
%  \put( 770,   0){\makebox(0,0)[l]{$f$} }
%
%  \put(-600, -10){\line( 0,1){  20} }
%  \put(-600, -20){\makebox(0,0)[t]{$\frac{-2}{T}$} }
%  \put(-300, -10){\line( 0,1){  20} }
%  \put(-300, -20){\makebox(0,0)[t]{$\frac{-1}{T}$} }
%  \put( 300, -10){\line( 0,1){  20} }
%  \put( 290, -20){\makebox(0,0)[lt]{$\frac{1}{T}=3W$} }
%  \put( 600, -10){\line( 0,1){  20} }
%  \put( 600, -20){\makebox(0,0)[t]{$\frac{2}{T}$} }
%\end{picture}
%\\
%\begin{picture}(1550,200)(-750,-50)
%  %\graphpaper[10](0,0)(600,200)
%  \thinlines
%  \put(  20, 80){\makebox(0,0)[lb]{$\Ffd(\omega)$ at sample rate $\frac{1}{T}=2W$ (at Nyquist rate)} }
%  \multiput(-600,0)(+200,0){7}{
%     \put(-100,   0){\line( 3,2){ 100} }
%     \put( 100,   0){\line(-3,2){ 100} }
%     }
%  \put(-750,   0){\line( 1,0){1500} }
%  \put(   0, -50){\line( 0,1){ 200} }
%  \put(- 10,  67){\line( 1,0){  20} }
%  \put(- 20,  67){\makebox(0,0)[r]{$\frac{A}{T}$} }
%  \put(-100, -10){\line( 0,1){  20} }
%  \put( 100, -10){\line( 0,1){  20} }
%  \put(-100, -20){\makebox(0,0)[t]{$-W$} }
%  \put( 100, -20){\makebox(0,0)[t]{$+W$} }
%  \put( 770,   0){\makebox(0,0)[l]{$f$} }
%
%  \put(-600, -10){\line( 0,1){  20} }
%  \put(-600, -20){\makebox(0,0)[t]{$\frac{-3}{T}$} }
%  \put(-400, -10){\line( 0,1){  20} }
%  \put(-400, -20){\makebox(0,0)[t]{$\frac{-2}{T}$} }
%  \put(-200, -10){\line( 0,1){  20} }
%  \put(-200, -20){\makebox(0,0)[t]{$\frac{-1}{T}$} }
%  \put( 200, -10){\line( 0,1){  20} }
%  \put( 190, -20){\makebox(0,0)[lt]{$\frac{1}{T}=2W$} }
%  \put( 400, -10){\line( 0,1){  20} }
%  \put( 400, -20){\makebox(0,0)[t]{$\frac{2}{T}$} }
%  \put( 600, -10){\line( 0,1){  20} }
%  \put( 600, -20){\makebox(0,0)[t]{$\frac{3}{T}$} }
%\end{picture}
%\\
%\begin{picture}(1550,200)(-750,-50)
%  %\graphpaper[10](0,0)(600,200)
%  \thinlines
%  \put(  20, 43){\makebox(0,0)[lb]{$\Ffd(\omega)$ at sample rate $\frac{1}{T}=W$ (``undersampling")} }
%  \multiput(-600,0)(+100,0){13}{
%     \put(-100,   0){\line( 3,1){ 100} }
%     \put( 100,   0){\line(-3,1){ 100} }
%     }
%  \put(-750,   0){\line( 1,0){1500} }
%  \put(   0, -50){\line( 0,1){ 200} }
%  \put(- 10,  33){\line( 1,0){  20} }
%  \put(- 20,  33){\makebox(0,0)[r]{$\frac{A}{T}$} }
%  \put(-100, -10){\line( 0,1){  20} }
%  \put( 100, -10){\line( 0,1){  20} }
%  %\put(-100, -20){\makebox(0,0)[t]{$-W$} }
%  %\put( 100, -20){\makebox(0,0)[t]{$+W$} }
%  \put( 770,   0){\makebox(0,0)[l]{$f$} }
%
%  \put(-600, -10){\line( 0,1){  20} }
%  \put(-600, -20){\makebox(0,0)[t]{$\frac{-6}{T}$} }
%  \put(-500, -10){\line( 0,1){  20} }
%  \put(-500, -20){\makebox(0,0)[t]{$\frac{-5}{T}$} }
%  \put(-400, -10){\line( 0,1){  20} }
%  \put(-400, -20){\makebox(0,0)[t]{$\frac{-4}{T}$} }
%  \put(-300, -10){\line( 0,1){  20} }
%  \put(-300, -20){\makebox(0,0)[t]{$\frac{-3}{T}$} }
%  \put(-200, -10){\line( 0,1){  20} }
%  \put(-200, -20){\makebox(0,0)[t]{$\frac{-2}{T}$} }
%  \put(-100, -10){\line( 0,1){  20} }
%  \put(-100, -20){\makebox(0,0)[t]{$\frac{-1}{T}$} }
%  \put( 100, -10){\line( 0,1){  20} }
%  \put( 100, -20){\makebox(0,0)[ct]{$\frac{1}{T}=W$} }
%  \put( 200, -10){\line( 0,1){  20} }
%  \put( 200, -20){\makebox(0,0)[t]{$\frac{2}{T}$} }
%  \put( 300, -10){\line( 0,1){  20} }
%  \put( 300, -20){\makebox(0,0)[t]{$\frac{3}{T}$} }
%  \put( 400, -10){\line( 0,1){  20} }
%  \put( 400, -20){\makebox(0,0)[t]{$\frac{4}{T}$} }
%  \put( 500, -10){\line( 0,1){  20} }
%  \put( 500, -20){\makebox(0,0)[t]{$\frac{5}{T}$} }
%  \put( 600, -10){\line( 0,1){  20} }
%  \put( 600, -20){\makebox(0,0)[t]{$\frac{6}{T}$} }
%\end{picture}
%\end{tabular}
%\end{fsL}
%\end{center}
%\caption{
%   Sampling in frequency domain
%   \label{fig:f_sampling}
%   }
%\end{figure}
%
%
%%--------------------------------------
%\begin{theorem}
%\label{thm:f_sampling}
%%--------------------------------------
%Let $\ff,\ffd\in\spLLR$ and $\Ff,\Ffd\in\spLLR$ 
%be their respective fourier transforms.
%Let $\ffd(x)$ be the {\bf sampled} $\ff(x)$ such that
%  \[ \ffd(x) \eqd \sum_{n\in\Z} \ff(x) \delta(x-nT). \]
%Then
%\thmbox{
%   \Ffd(\omega) = \frac{2\pi}{T}\sum_{n\in\Z} \Ff\left(\omega -\frac{2\pi}{T}n \right).
%   }
%\end{theorem}
%\begin{proof}
%\begin{align*}
%   \Ffd(\omega) 
%     &\eqd \int_t \ffd(x) e^{-i\omega t} \dt
%   \\&=    \int_t \left[\sum_{n\in\Z} \ff(x) \delta(x-nT)\right] e^{-i\omega t} \dt
%   \\&=    \sum_{n\in\Z} \int_t \ff(x) \delta(x-nT) e^{-i\omega t} \dt
%   \\&=    \sum_{n\in\Z} \ff(nT) e^{-i\omega nT}
%     &&    \text{by definition of $\delta$ \ifxref{relation}{def:dirac}}
%   \\&=    \frac{2\pi}{T} \sum_{n\in\Z} \Ff\left(\omega + \frac{2\pi}{T}n \right)
%     &&    \text{by \thme{IPSF} \xref{thm:ipsf}}
%   \\&=    \frac{2\pi}{T} \sum_{n\in\Z} \Ff\left(\omega-\frac{2\pi}{T}n\right)
%\end{align*}
%\end{proof}
%
%Suppose a waveform $\ff(x)$ is sampled at every time $T$
%generating a sequence of sampled values $\ff(nT)$.
%Then in general, we can {\em approximate} $\ff(x)$ by 
%using interpolation between the points $\ff(nT)$.
%Interpolation can be performed using several interpolation techniques. %:\\
%%\begin{tabular}{lll}
%%  $\imark$ & 0th order interpolation: & convolution with rectangular functions \\
%%  $\imark$ & 1st order interpolation: & convolution with triangular functions  \\
%%  $\imark$ & 3rd order interpolation: & cubic splines \footnotemark            \\
%%  $\imark$ & lowpass smoothing        & convolution with the ``sinc" function 
%%\end{tabular}
%%\footnotetext{
%%  A {\em cubic spline} is a third order polynomial $ax^3+bx^2+cx+d$.
%%  Such a polynomial has 4 {\em degrees of freedom}. 
%%  Two of these degrees of freedom are used to make the polymomial match
%%  at the interpolation interval endpoints.
%%  The other two degrees of freedom are used to make the slopes of the 
%%  polynomial match at the interval endpoints.
%%  }
%
%In general all techniques lead only to an approximation of $\ff(x)$.
%However, if $\ff(x)$ is \prope{bandlimited} with bandwidth 
%$W\le\frac{1}{2T}$,
%then $\ff(x)$ is {\em perfectly reconstructed} (not just approximated)
%from the sampled values $\ff(nT)$ \xref{thm:t_sampling}.
% %using ``\hie{lowpass smoothing}" (the last technique in the list).
%%This perfect reconstruction is demonstrated in \prefp{thm:t_sampling}.
%
%
%






