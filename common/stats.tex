%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%======================================
\chapter{Statistics}
\label{chp:stats}
%======================================
%=======================================
\section{Expectation operator}
\index{expectation operator}
%=======================================
In a \structe{probability space} $\ps$, all probability information
is contained in the \fncte{measure} $\psp$ (or equivalently in the pdf or cdf
defined in terms of $\psp$).
Often times this information is overwhelming and a simpler statistic,
which does not offer so much information, is sufficient.
Some of the most common statistics can be conveniently expressed in terms
of the \hie{expectation operator} $\pE$.
%---------------------------------------
\begin{definition}
\label{def:pE}
%---------------------------------------
Let $\ps$ be a \structe{probability space} and
$\rvX$ a \fncte{random variable} on $\ps$ with
\fncte{probability density function} $\ppx$.
\defboxt{ 
  The \opd{expectation operator} $\pEx$ on $\rvX$ is defined as
  \\\indentx$\ds\pEx\rvX \eqd \int_{x\in\F} x \ppx(x) \dx$. 
}
\end{definition}

We already said that a \fncte{random variable} $\rvX$ is neither random nor a variable,
but is rather a function of an underlying process that does appear to be random.
However, because it is a function of a process that does appear random,
the \fncte{random variable} $\rvX$ also appears to be random.
That is, if we don't know the outcome of of the underlying experimental
process, then we also don't know for sure what $\rvX$ is, and so $\rvX$ does
indeed appear to be random.
However, eventhough $\rvX$ appears to be random,
the expected value $\pEx\rvX$  of $\rvX$ is {\bf not random}.
Rather it is a fixed value (like $0$ or $7.9$ or $-2.6$).

On the other hand, eventhough $\pE\rvX$ is {\bf not random},
note that $\pE(\rvX|\rvY)$ {\bf is random}.
This is because $\pE(\rvX|\rvY)$ is a function of $\rvY$.
That is, once we know that $\rvY$ equals some fixed value $y$
(like $0$ or $2.7$ or $-5.1$) then $\pE(\rvX|\rvY=y)$ is also fixed.
However, if we don't know the value of $\rvY$,
then $\rvY$ is still a \fncte{random variable} and the expression $\pE(\rvX|\rvY)$
is also random (a function of \fncte{random variable} $\rvY$).

Two common statistics that are conveniently expressed in terms of the
expectation operator are the \hie{mean} and \hie{variance}.
The mean is an indicator of the ``middle" of a probability distribution and the
variance is an indicator of the ``spread".
%---------------------------------------
\begin{definition}
\label{def:Mx}
\label{def:pvar}
\label{def:pVar}
\index{mean}
\index{variance}
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} on the \structe{probability space} $\ps$.
The {\bf mean} $\pmeanx$ and {\bf variance} $\pVar(\rvX)$ of $\rvX$ are
\defbox{\begin{array}{FM>{\ds}rc>{\ds}l}
  (1).&The \fnctd{mean} $\pmeanx$ of $\rvX$ is 
    & \pmeanx  &\eqd& \pEx\rvX 
  \\
  (2).&The \fnctd{variance} $\pVar(\rvX)$ or $\pvarx$ of $\rvX$ is 
    & \pVar(\rvX) &\eqd& \pEx\left[(\rvX-\pEx\rvX)^2 \right]
\end{array}}
\end{definition}

The next theorem gives some useful relations for simple statistics.
%---------------------------------------
\begin{theorem}
\label{thm:pE}
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} on the \structe{probability space} $\ps$.
\thmbox{\begin{array}{>{\ds}rc>{\ds}lCD}
    \pEx(a\rvX+b)    &=& \brp{a\pEx\rvX} + b       & \forall a,b\in\R & (\prope{linear})
  \\\pVar(a\rvX+b)   &=& a^2\pVar(\rvX)            & \forall a,b\in\R &
  \\\pVar(\rvX)      &=& \pEx(\rvX^2) - (\pEx\rvX)^2
\end{array}}
\end{theorem}
\begin{proof}
\begin{align*}
  \pEx(a\rvX+b)
    &\eqd \int_{x\in\R}\brs{ax+b} \ppx(x)  \dx
  \\&=    a \int_{x\in\R}x \ppx(x)  \dx + b\int_{x\in\R}\ppx(x)  \dx
  \\&= \brs{a\pEx\rvX} + b\brs{1}
  \\&= \brs{a\pEx\rvX} + b
\\
\\
  \pVar(\rvX)
    &\eqd \pEx\left[(\rvX-\pEx\rvX)^2\right]
    &&    \text{by definition of $\pVar$}
    &&    \text{\xref{def:pvar}}
  \\&=    \pEx\brs{\rvX^2-2X\pEx\rvX + (\pEx\rvX)^2 }
  \\&=    \pEx[\rvX^2]  - \pEx 2X\pEx\rvX  + \pEx (\pEx\rvX)^2
  \\&=    \pEx[\rvX^2] - 2(\pEx\rvX)[\pEx\rvX] + (\pEx\rvX)^2
  \\&=    \pEx(\rvX^2) - (\pEx\rvX)^2
\\
\\
  \pVar(a\rvX+b)
    &=    \pEx(a\rvX+b)^2  - [\pEx(a\rvX+b)]^2
  \\&=    \pEx(a^2X^2+2ab\rvX+b^2)  - [a(\pEx\rvX)+b]^2
  \\&=    a^2 \pEx\rvX^2  +2ab\pEx\rvX + b^2 - \brs{a^2[\pEx\rvX]^2 + 2ab\pEx\rvX + b^2}
  \\&=    a^2\brs{ \pEx\rvX^2  - (\pEx\rvX)^2 }
  \\&\eqd a^2 \pVar(\rvX)
\end{align*}
\end{proof}

\begin{figure}[ht]
\setlength{\unitlength}{0.3mm}%
\begin{center}%
\begin{picture}(200,110)(-50,-10)%
  \thicklines
  \color{axis}%
    \put(  0,  0){\line(1, 0){120}}%
    \put(  0,  0){\line(0, 1){100}}%
    \qbezier[20](0,60)(30,60)(60,60)%
    \qbezier[20](60,0)(60,30)(60,60)%
  \color{blue}%
    \qbezier(33,100)(85,0)(110,100)%
    \put( 60, 60){\circle*{5}}%
  \color{red}%
    \put( 20,100){\line(1,-1){80}}%
  \color{label}%
  \put(125,  0){\makebox(0,0)[l]{$x$}}%
  \put( -5, 60){\makebox(0,0)[r]{$\ff(\pE\rvX)$}}%
  \put( 60, -5){\makebox(0,0)[t]{$\pE\rvX$}}%
  \put(130,60){\vector(-1,0){32}}%
  \put(130,40){\vector(-1,0){47}}%
  \put(135,60){\makebox(0,0)[l]{$\ff(x)$ (convex function)}}%
  \put(135,40){\makebox(0,0)[l]{$mx+c$ (support line)}}%
\end{picture}
\end{center}
\caption{
  Jensen's inequality
  \label{fig:jensen}
  }
\end{figure}

\fncte{Jensen's inequality} is an extremely useful application of \prope{convex}ity \xref{def:convex} to the
\ope{expectation} operator.
Jensen's inequality is stated in \pref{cor:jensen} (next)
and illustrated in \prefpp{fig:jensen}.
%--------------------------------------
\begin{corollary}[\thmd{Jensen's inequality}]
\footnote{
  \citerp{cover}{25},
  \citerpp{jensen1906}{179}{180}
  }
\label{cor:jensen}
%--------------------------------------
Let $\ff$ be a function in $\clFrr$ and $\rvX$ be a \fncte{random variable} on $\ps$.
\corbox{
  \brb{\text{$\ff$ is \prope{convex}}} 
  \quad\implies\quad 
  \brb{\ff(\pE\rvX) \le \pE\ff(\rvX)}
  }
\end{corollary}
\begin{proof}
\begin{enumerate}
  \item Proof 1:
Let $mx+c$ be a ``support line" under $\ff(x)$ \xref{fig:jensen} such that
\[
  \begin{array}{rcll}
    mx+c &<& \ff(x) & \mbox{for } x\ne \pE\rvX \\
    mx+c &=& \ff(x) & \mbox{for } x=\pE\rvX.
  \end{array}
\]
Then
\begin{align*}
  \ff(\pE\rvX)
    &=   m[\pE\rvX] + c
  \\&=   \pE[mX + c]
  \\&\le \pE\ff(\rvX)
\end{align*}

  \item Proof 2 (alternate proof):
    \begin{align*}
      \ff\brp{\pE\rvX}
        &\eqd \ff\brp{\sum_{x\in\pse} x \psp(x)}
      \\&\le \sum_{x\in\pse} \ff(x) \psp(x)
        && \text{by \thme{Jensen's inequality} for convex sets}
        && \text{\xref{thm:jensenineq}}
    \end{align*}
\end{enumerate}
\end{proof}

%=======================================
\section{Upper bounds on probability}
%=======================================
%---------------------------------------
\begin{theorem}[\thmd{Markov's inequality}]
\citetbl{
  \citerp{ross}{395}
  }
\label{thm:markovineq}
%---------------------------------------
Let $\rvX:\Omega\to[0,\infty)$ be a non-negative valued \fncte{random variable} and
$a\in(0,\infty)$. Then
\thmbox{ \psp\setn{\rvX\ge a} \le \frac{1}{a} \pE\rvX }
\end{theorem}
\begin{proof}
\begin{align*}
  I &\eqd \left\{ \begin{array}{l@{\hspace{4ex}\mbox{for}\hspace{4ex}}l}
    1 &\rvX\ge a \\
    0 &\rvX < a
    \end{array}\right.
\\
  aI &\le\rvX           \\
   I &\le \frac{1}{a}\rvX \\
   \pE I &\le \pE\left(\frac{1}{a}\rvX\right) \\
\\
   \psp\setn{\rvX\ge a}
     &= 1\cdot\psp\setn{\rvX\ge a} + 0\cdot\psp\setn{\rvX<a}
   \\&= \pE I
   \\&\le \pE\left(\frac{1}{a}\rvX \right)
   \\&=   \frac{1}{a}\pE\rvX
\end{align*}
\end{proof}


%---------------------------------------
\begin{theorem}[Chebyshev's inequality]
\index{Chebyshev's inequality}
\index{theorems!Chebyshev's inequality}
\citetbl{
  \citerp{ross}{396}
  }
%---------------------------------------
Let $\rvX$ be a \fncte{random variable} with mean $\mu$ and variance $\sigma^2$.
\thmbox{ \psp\setn{\abs{\rvX-\mu}\ge a} \le \frac{\sigma^2}{a^2}}
\end{theorem}
\begin{proof}
\begin{align*}
  \psp\setn{\abs{\rvX-\mu} \ge a}
    &=   \psp\setn{ (\rvX-\mu)^2 \ge a^2}
  \\&\le \frac{1}{a^2} \pE(\rvX-\mu)^2 
    && \text{by \thme{Markov's inequality}}
    && \text{\xref{thm:markovineq}}
  \\&=   \frac{\sigma^2}{a^2}
\end{align*}
\end{proof}

%=======================================
\section{Joint and conditional probability spaces}
%=======================================
Sometimes the problem of finding the expected value of a \fncte{random variable} $\rvX$
can be simplified by ``conditioning $\rvX$ on $\rvY$".
%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $\rvX$ and $\rvY$ be \fncte{random variable}s. Then
\thmbox{\pEx{\rvX} = \pEy\pEx[x|y](\rvX|\rvY) }
\end{theorem}
\begin{proof}
\begin{align*}
   \pEy\pEx[x|y](\rvX|\rvY)
     &\eqd \pEy \left[ \int_{x\in\R}x \pp(\rvX=x|\rvY) \dx \right]
   \\&\eqd \int_{y\in\R} \left[\int_{x\in\R}x \pp(x|\rvY=y) \dx \right] \pp(y) \dy
   \\&=    \int_{y\in\R} \int_{x\in\R}x \pp(x|y)\pp(y) \dx   \dy
   \\&=    \int_{x\in\R}x \int_{y\in\R} \pp(x,y) \dy   \dx
   \\&=    \int_{x\in\R}x \pp(x) \dx
   \\&\eqd \pEx\rvX
\end{align*}
\end{proof}

When possible, we like to generalize any given mathematical structure
to a more general mathematical structure and then take advantage of
the properties of that more general structure.
Such a generalization can be done with \fncte{random variable}s.
Random variables can be viewed as vectors in a vector space.
Furthermore, the expectation of the product of two \fncte{random variable}s
(e.g. $\pE(\rvX\rvY)$)
can be viewed as an \fncte{inner product} in an \structe{inner product space}.
Since we have an \fncte{inner product} space,
we can then immediately use all the properties of
\structe{inner product space}s, \fncte{norm}ed spaces, vector spaces, metric spaces,
and topological spaces.

%---------------------------------------
\begin{theorem}
\citetbl{
  \citerpp{moon2000}{105}{106}
  }
\label{thm:prb_vspace}
%---------------------------------------
Let $R$ be a ring,
$\ps$ be a \structe{probability space}, $\pE$ the expectation operator, and
$\spV=\set{\rvX}{\rvX:\pso\to R}$ be the set of all random vectors
in \structe{probability space} $\ps$.
\thmbox{\begin{array}{FlM}
  (1). & \spV\eqd\set{\rvX}{\rvX:\pso\to R}        & is a \structe{vector space}. \\
  (2). & \inprod{\rvX}{\rvY}\eqd\pE(\rvX\rvY^\ast) & is an \fncte{inner product}. \\
  (3). & \norm{\rvX}\eqd\sqrt{\pE(\rvX\rvX^\ast)}  & is a \fncte{norm}. \\
  (4). & \opair{\spV}{\inprodn}                    & is an \structe{inner product space}.
\end{array}}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item Proof that $\spV$ is a vector space:
    \[\begin{array}{lll@{\hs{1cm}}D}
   1) & \forall \rvX,\rvY, \rvZ\in\spV
      & (\rvX+\rvY)+ \rvZ = \rvX+(\rvY+ \rvZ)
      & \text{($+$ is associative)}
      \\
   2) & \forall \rvX,\rvY\in\spV
      & \rvX+\rvY = \rvY+\rvX
      & \text{($+$ is commutative)}
      \\
   3) & \exists  0 \in\spV \st \forall \rvX\in\spV
      & \rvX+ 0 = \rvX
      & \text{($+$ identity)}
      \\
   4) & \forall \rvX \in\spV \exists \rvY\in\spV \st
      & \rvX+\rvY =  0
      & \text{($+$ inverse)}
      \\
   5) & \forall \alpha\in S \text{ and } \rvX,\rvY\in\spV
      & \alpha\cdot(\rvX+\rvY) = (\alpha \cdot\rvX)+(\alpha\cdot\rvY)
      & \text{($\cdot$ distributes over $+$)}
      \\
   6) & \forall \alpha,\beta\in S \text{ and } \rvX\in\spV
      & (\alpha+\beta)\cdot\rvX = (\alpha\cdot \rvX)+(\beta\cdot \rvX)
      & \text{($\cdot$ pseudo-distributes over $+$)}
      \\
   7) & \forall \alpha,\beta\in S \text{ and } \rvX\in\spV
      & \alpha(\beta\cdot\rvX) = (\alpha\cdot\beta)\cdot\rvX
      & \text{($\cdot$ associates with $\cdot$)}
      \\
   8) & \forall \rvX\in\spV
      & 1\cdot \rvX = \rvX
      & \text{($\cdot$ identity)}
\end{array}\]

  \item Proof that $\inprod{\rvX}{\rvY}\eqd\pE(\rvX\rvY^\ast)$ is an \fncte{inner product}.
  \[\begin{array}{llllD}
   1) &  \pE(\rvX\rvX^\ast) &\ge 0
      &  \forall \rvX\in\spV
      &  \text{(non-negative)}
      \\
   2) &  \pE(\rvX\rvX^\ast) &= 0 \iff \rvX=0
      &  \forall \rvX\in\spV
      &  \text{(non-degenerate)}
      \\
   3) &  \pE(\alpha\rvX\rvY^\ast)    &= \alpha\pE(\rvX\rvY^\ast)
      &  \forall \rvX,\rvY\in\spV,\;\forall\alpha\in\C
      &  \text{(homogeneous)}
      \\
   4) &  \pE[(\rvX+\rvY)\rvZ^\ast] &= \pE(\rvX\rvZ^\ast) + \pE(Y\rvZ^\ast)
      &  \forall \rvX,\rvY, \rvZ\in\spV
      &  \text{(additive)}
      \\
   5) &  \pE(\rvX\rvY^\ast) &= \pE(\rvY\rvX^\ast)
      &  \forall \rvX,\rvY\in\spV
      &  \text{(conjugate symmetric)}.
  \end{array}\]

  \item Proof that $\norm{\rvX}\eqd\sqrt{\pE(\rvX\rvX^\ast)}$ is a \fncte{norm}:
    This \fncte{norm} is simply induced by the above \fncte{inner product}.
  \item Proof that $\opair{\spV}{\inprodn}$ is an \structe{inner product space}:
    Because $\spV$ is a vector space and $\inprodn$ is
    an \fncte{inner product}, $\opair{\spV}{\inprodn}$ is an \structe{inner product space}.
\end{enumerate}
\end{proof}

The next theorem gives some results that follow directly from vector space
properties:
%---------------------------------------
\begin{theorem}
%---------------------------------------
Let $\ps$ be a \structe{probability space} with expectation functional $\pE$.
\thmbox{\begin{array}{l >{\ds}r c >{\ds}l D}
  1. & \sqrt{\pE\left(\sum_{n=1}^{\xN}\rvX_n\right)}
     &\le& \sum_{n=1}^{\xN} \pE(\rvX_nX_n^\ast)
     & (\thme{Generalized triangle inequality})
     \\
  2. & \abs{\pE(\rvX\rvY^\ast)}^2
     &\le& \pE(\rvX\rvX^\ast)\:\pE(YY^\ast)
     & (\thme{Cauchy-Schwartz inequality})
     \\
  3. & 2\pE(\rvX\rvX^\ast) + 2\pE(YY^\ast)
     &=& \pE[(\rvX+\rvY)(\rvX+\rvY)^\ast] + \pE[(\rvX-Y)(\rvX-Y)^\ast]
     &   (\thme{Parallelogram Law})
  \end{array}}
\end{theorem}
\begin{proof}
\begin{enumerate}
  \item $\opair{\clFor}{\pE(\rvx,\rvy)}$ is an \structe{inner product space}. Proof: \prefpp{thm:prb_vspace}.

  \item Because it is an \structe{inner product space}, the other properties follow:
        \\\indentx\begin{tabular}{llll}
          1. & Generalized triangle inequality:
             & \pref{thm:norm_tri}
             & \prefpo{thm:norm_tri}
             \\
          2. & Cauchy-Schwartz inequality:
             & \pref{thm:cs}
             & \prefpo{thm:cs}
             \\
          3. & Parallelogram Law:
             & \pref{thm:parallelogram}
             & \prefpo{thm:parallelogram}
        \end{tabular}
\end{enumerate}
\end{proof}

