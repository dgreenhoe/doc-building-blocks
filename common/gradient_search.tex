%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================

%======================================
\chapter{Gradient Search Techniques}
%======================================
%======================================
\section{Gradient search techniques}
%======================================
One of the biggest advantages of using a gradient search technique is
that they can be implemented \hie{recursively} as shown in the next equation.
The general form of the gradient search parameter estimation techniques is\footnote{\citerp{nelles2001}{90}}
\thmbox{
   \vp_n = \vp_{n-1} - \eta_{n-1} R \;\left[\gradxy{\vp}{\fCost(\vp_n)}\right]
}
where at time $n$

\begin{tabular}{lll}
   $\vp_n$      & is the \hie{state    }   & (vector)  \\
   $\eta_n$     & is the \hie{step size}   & (scalar)  \\
   $\setY$          & is the \hie{direction}   & (matrix)  \\
   $\gradxy{\vp}{\fCost(\vp_n)}$      & is the \hie{gradient } of the cost function $\fCost(\vp_n)$   & (vector)
\end{tabular}

Two major categories of gradient search techniques are
\begin{liste}
   \item steepest descent (includes LMS)
   \item Newton's method (includes RLS and Kalman filters).
\end{liste}

The key difference between the two is that
{\bf \hie{steepest descent} uses only first derivative information},
while
{\bf \hie{Newton's method} uses both first and second derivative information}
making it converge much faster but with significantly higher
complexity.

%======================================
\subsubsection*{First derivative techniques}
\label{sec:1st-deriv}
%======================================
\paragraph{Steepest descent.}
In this algorithm, $R=I$ (identity matrix).
First derivative information is contained in $\grad\fCost$.
Second derivative information, if present, is contained in $\setY$.
Thus, steepest descent algorithms do not use second derivative information.
\thmbox{
  \vp_n = \vp_{n-1} - \eta_{n-1} \;\left[ \gradxy{\vp}{\fCost(\vp_n)} \right]
}
\paragraph{Least Mean Squares (LMS).}\footnote{\citerp{mik}{526}}
This is a special case of \hie{steepest descent}.
In minimum mean square estimation \xref{sec:est_mms},
the cost function $\fCost(\vp)$ is defined as a
\hie{statistical average} of the error vector such that
$\fCost(\vp) = \Eb{\ve^H\ve}$.
In this case the gradient $\grad\fCost$ is difficult to compute.
However, the LMS algorithm greatly simplifies the problem by
instead defining the cost function as a function of the
\hie{instantaneous error} such that
\begin{align*}
   \vy &= y(n)
\\
   \vye &= \hat{y}(n)
\\
   \fCost(\vp)
   &= \norm{e(n)}^2
 \\&= e^2(n)
 \\&= (\hat{y}(n)-y(n))^2
\end{align*}

Computing the gradient of this cost function is then
just a special case of \hie{least squares estimation} \xref{sec:ls}.
Using LS, we let $U=\vx^T$ and hence
\begin{align*}
   \gradxy{\vp}{\fCost(\vp)}
   &= 2U^TU \vp -2U^T\vy                   && \text{ by \prefp{thm:ls}}
\\ &= 2\vx\vx^T \vp -2\vx y               && \text{ by above definitions}
\\ &= 2\vx\hat{y} -2\vx y                    && \text{ }
\\ &= 2\vx(\hat{y} -y)                      && \text{ }
\\ &= 2\vx e(n)                && \text{ }
\end{align*}

The LMS algorithm uses this instantaneous gradient for $\grad\fCost$,
lets $R=I$, and uses a constant step size $\eta$ to give
\thmbox{
  \vp_n = \vp_{n-1} - 2\eta \vx_n e(n)
}
%--------------------------------------
\subsubsection*{Second derivative techniques}
%--------------------------------------
\paragraph{Newton's Method.}
This algorithm uses the \hie{Hessian} matrix $H$,
which is the second derivative of the cost function $\fCost(\vp)$,
and lets $R=H^{-1}$.
\begin{align*}
   H_n &\eqd& \grad_\vp\grad_\vp \fCost(\vp_n)
\\
   \vp_n &= \vp_{n-1} - \eta_{n-1} H_n^{-1} \left[\grad_\vp\fCost(\vp_n)\right]
\end{align*}


\paragraph{Kalman filtering}\footnote{\citerp{nelles2001}{66}}
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}

\if 0
\paragraph{RLS with forgetting}\footnote{\citerp{nelles2001}{64}}
This algorithm introduces a forgetting factor $\lambda$
to help the algorithm track non-stationary channels.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+\lambda}  P(k-1)x(k) \\
   P(k) &= \frac{1}{\lambda}(I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}
\fi

\paragraph{Recursive Least Squares (RLS)}\footnote{\citerp{nelles2001}{66}}
This algorithm is a special case of either the RLS with forgetting
or the Kalman filter.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}  P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1) \\
   e(k) &= y(k) - x^T(k)\vpe(k-1) \\
   \vpe(k) &= \vpe(k-1) + \gamma(k)e(k)
\end{align*}




%--------------------------------------
\section{Direct search}
%--------------------------------------
A direct search algorithm may be used in cases where the cost
function over $\vp$ has several local minimums, making convergence difficult.
Furthermore, direct search algorithms can be very computationally demanding.

