%============================================================================
% LaTeX File
% Daniel J. Greenhoe
%============================================================================
%======================================
\chapter[Least Squares Algorithms]
        {Least Squares Algorithms when Model is Unknown}
%======================================
\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  %\graphpaper[10](0,0)(700,100)
  \put(-100,  10){\makebox ( 50,  0)[b]{$x$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{operator} }
  \put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vtheta$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn$}           }

  \put( 100,  85){\makebox ( 50,  0)[b]{$y$}               }
  \put( 100,  75){\line    (  1,  0)   { 50}               }
  %\put( 100,- 65){\makebox ( 50,  0)[b]{$U\estn$}            }
  \put( 100,- 75){\line    (  1,  0)   { 50}               }
  \put( 150,  75){\line    (  0, -1)   { 50}               }
  \put( 150,- 75){\line    (  0,  1)   { 50}               }
  \put( 150,  25){\vector  (  1,  0)   { 50}               }
  \put( 150, -25){\vector  (  1,  0)   { 50}               }

  \put( 200,- 50){\framebox(100,100)[c]{cost}    }
  \put( 300,  10){\makebox (100,  0)[b]{$C(\estn,x,y)$}      }
  \put( 300,   0){\vector  (  1,  0)   {100}               }
  \put( 400,- 50){\framebox(100,100)[c]{$\grad_\vtheta$}        }
  \put( 510,  10){\makebox (100,  0)[lb]{$\grad_\vtheta C(\vtheta,x,y)$}      }
  \put( 500,   0){\vector  (  1,  0)   {100}               }

\end{picture}
\caption{
   Estimation using gradient of cost function
   \label{fig:est-grad}
   }
\end{figure}


Norm minimization techniques are useful when the system model is unknown.\footnote{\citerpgc{clarkson1993}{1}{0849386098}{chapter one Introduction}}
%However they are very powerful in that an optimum solution can be computed in one step without iteration or recursion.
In this section we present two types of norm minimization:\footnote{
   The Least Squares algorithm is nothing new to mathematics.
   It was first published by Legendre in 1805, but there is a credible claim by Gauss
   that he had it as far back as 1795.
   Gauss, by the way, was also the first to discover the FFT.
   References:
   \citePp{sorenson1970}{63},
   \citeP{plackett1972},
   \citeP{stigler1981},
   \citeP{dutka1995}
   }

\begin{enume}
  \item minimum mean square estimation (MMSE): \\
        The MMS estimate is a \prope{stochastic} estimate.
        To compute the MMS estimate, the we do not need to know
        the actual data values, but we must know certain system statistics
        which are the
        input data autocorrelation and input/output crosscorrelation.
        The cost function is the expected value of the norm squared error.
   \item least square estimation (LSE):\footnote{\citerp{scargle1979}{5}} \\
        The LS estimate is a \prope{deterministic} estimate.
        To compute the LS estimate, we must know the actual data values
        (although these may be ``noisy" measurements).
        The cost function is the norm squared error.
\end{enume}

Solutions to both are given in terms of two matrices:

\begin{tabular}{lll}
   $\setY$: Autocorrelation matrix \\
   $W$: Crosscorrelation matrix.
\end{tabular}

%--------------------------------------
\section{Minimum mean square estimation}
\label{sec:est_mms}
%--------------------------------------
%--------------------------------------
\begin{definition}
\label{def:est_matrices}
%--------------------------------------
Let the following vectors, matrices, and functions
be defined as follows:
\defbox{\begin{array}{lclM | lclM}
     \vx    &\in& \vCm         & \fncte{data vector}                      & U      &\in& \mCmn        & \ope{regression matrix}         
   \\\vy    &\in& \vCn         & \fncte{processed data vector}            & R      &\in& \mCmm        & \ope{autocorrelation matrix}    
   \\\vye   &\in& \vCn         & \fncte{processed data estimate vector}   & W      &\in& \vCm         & \fncte{cross-correlation vector}
   \\\ve    &\in& \vCn         & \fncte{error vector}                     & \fCost &:  & \vRm\to\Rnn  & \fncte{cost function}           
   \\\vtheta    &\in& \vRm         & \fncte{parameter vector}                 & 
\end{array}}
\end{definition}

%--------------------------------------
\begin{theorem}[\thmd{Minimum mean square estimation}]
\label{thm:est_mms}
%--------------------------------------
Let
\begin{array}[t]{|rcl|}
   \hline
     \vye(\vtheta)   &\eqd& \opU^H\vtheta
   \\\ve(\vtheta)    &\eqd& \vye-\vy
   \\\fCost(\vtheta) &\eqd& \pE\norm{\ve}^2 \eqd \pE\brs{\ve^H\ve}
   \\\estMS      &\eqd& \argmin_\vtheta \fCost(\vtheta)
   \\R           &\eqd& \pE\brs{\opU\opU^H}
   \\W           &\eqd& \pE\brs{\opU\vy}
   \\\hline
\end{array}
Then
\thmbox{\begin{array}{rcl}
     \estMS                          &=& (\Real{\setY})^{-1}(\Real{W})
   \\\fCost(\vtheta)                     &=& \vtheta^H R \vtheta - (W^H\vtheta)^\ast -W^H\vtheta + \pE{\vy^H\vy}
   \\\grad_\vtheta \fCost(\vtheta)           &=& 2\Real\brs{\setY}\vtheta - 2\Real{W}
   \\\fCost(\estMS)                  &=&
     \brbl{\begin{array}{>{\ds}lM}
         \mc{2}{>{\ds}l}{\pE{\vy^H\vy} + (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W})}
       \\\pE{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W}) & if $\opR$ is \propb{real-valued}
     \end{array}}
\end{array}}
\end{theorem}
\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{enumerate}
  \item Proof that cost $\fCost(\vtheta)=\vtheta^H R \vtheta - (W^H)^\ast\vtheta -W^H\vtheta + \pE{\vy^H\vy}$: \label{item:est_mms_cost}
    \begin{align*}
       \boxed{\fCost(\vtheta)}
         &\eqd \pE\norm{\ve}^2
         && \text{by definition of \fncte{cost function} $\fCost$}
       \\&\eqd \pE\brs{\ve^H\ve}
         && \text{by definition of \fncte{norm} $\normn$}
       \\&\eqd \pE\brs{\brp{\vye-\vy}^H\brp{\vye-\vy}}
         && \text{by definition of \fncte{error vector} $\ve$}
       \\&\eqd \pE\brs{\brp{\opU^H\vtheta-\vy}^H\brp{\opU^H\vtheta-\vy}}
         && \text{by definition of estimate $\vye$}
       \\&=\mathrlap{\pE\brs{\brp{\vtheta^H\opU-\vy^H}\brp{\opU^H\vtheta-\vy}}
            \text{by \prope{distributive} prop. of \structe{*-algebra}s 
                  \xref{def:staralg}}}
       \\&=    \pE\brs{ \vtheta^H\opU\opU^H\vtheta - \vtheta^H\opU\vy -\vy^H\opU^H\vtheta + \vy^H\vy }
         && \text{by \structe{matrix algebra} ring property}
       \\&=    \vtheta^H\pE\brs{\opU\opU^H}\vtheta - \vtheta^H\pE\brs{\opU\vy} -\pE\brs{\vy^H\opU^H}\vtheta + \pE{\vy^H\vy}
         && \text{by \prope{linearity} $\pE$ 
                  \xref{thm:pE_linop}}
       \\&=    \vtheta^H\pE\brs{\opU\opU^H}\vtheta - (\pE\brs{\opU\vy}^H\vtheta)^H -\pE\brs{\opU\vy}^H\vtheta + \pE{\vy^H\vy}
       \\&\eqd \vtheta^H \opR \vtheta - (\opW^H\vtheta)^H -\opW^H\vtheta + \pE{\vy^H\vy}
         && \text{by definitions of $\opR$ and $\opW$}
       \\&=    \vtheta^H \opR \vtheta - (\opW^H\vtheta)^\ast -\opW^H\vtheta + \pE{\vy^H\vy}
         && \text{because $\opW^H\vtheta$ is a \prope{scalar}}
       \\&=    \boxed{\vtheta^H \opR \vtheta - (\opW^H)^\ast\vtheta -\opW^H\vtheta + \pE{\vy^H\vy}}
       \\&=    \vtheta^H \opR \vtheta - 2\Real\brs{\opW^H}\vtheta + \pE{\vy^H\vy}
    \end{align*}

  \item Proof that optimal $\vthetao=(\Real{\setY})^{-1}(\Real{W})$: \label{item:est_mms_vpo}
    \begin{align*}
      \grad_\vtheta \fCost(\vtheta)
        &= \grad_\vtheta \brs{\vtheta^H \opR \vtheta - (\opW^H)^\ast\vtheta -\opW^H\vtheta + \pE{\vy^H\vy} }
        && \text{by \pref{item:est_mms_cost}}
      \\&= \opR\vtheta + \opR^T\vtheta - \grad_{\vtheta}\brs{(\opW^H)^\ast\vtheta + \opW^H\vtheta} + 0
        && \text{by \thme{quadratic form} result \xref{thm:mc_xAx}}
      \\&= \opR\vtheta + \opR^T\vtheta - [(\opW^H)^\ast]^T - [\opW^H]^T
        && \text{by \thme{affine equations} result \xref{thm:mc_Ax}}
      \\&= \opR\vtheta + (R^H)^\ast\vtheta - \opW - \opW^\ast
        && \text{by definition of \ope{Hermitian Transpose} $^H$}
      \\&= \opR\vtheta + \opR^\ast\vtheta - \opW - \opW^\ast
        && \text{because $R$ is \prope{Hermitian symmetric}}
      \\&= (R + \opR^\ast)\vtheta - (\opW + \opW^\ast)
        && \text{by \prope{ring} property}
      \\&= 2(\Real{\setY})\vtheta - 2\Real{\opW}
        && \text{by definition of $\Real$ \xref{def:Re}}
      \\
      \implies\vthetao
        &= (\Real{\setY})^{-1}(\Real{\opW})
        && \text{by setting $\grad_\vtheta \fCost(\vtheta)=0$}
    \end{align*}

  \item Cost of optimal $\vthetao$:
    \begin{align*}
      \fCost(\vthetao)
        &=    \vthetao^H \opR \vthetao - 2\Real\brs{\opW^H}\vthetao + \pE{\vy^H\vy}
        &&    \text{by \pref{item:est_mms_cost}}
      \\&=    \brs{(\Real{\setY})^{-1}(\Real{\opW})}^H 
              \opR\brs{(\Real{\setY})^{-1}(\Real{\opW})} 
            - 2\Real\brs{\opW^H}\brs{(\Real{\setY})^{-1}(\Real{\opW})} 
            + \pE{\vy^H\vy}
        &&    \text{by \pref{item:est_mms_vpo}}
      \\&=    (\Real{\opW^H})(\Real{\setY})^{-H} \opR (\Real{\setY})^{-1}(\Real{\opW}) - 2\Real\brs{\opW^H}(\Real{\setY})^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    (\Real{\opW^H})(\Real{\opR^H})^{-1} \opR (\Real{\setY})^{-1}(\Real{\opW}) - 2\Real\brs{\opW^H}(\Real{\setY})^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    (\Real{\opW^H})(\Real{\setY})^{-1} \opR (\Real{\setY})^{-1}(\Real{\opW}) - 2(\Real{\opW^H})(\Real{\setY})^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
        \\
        \\
      \fCost(\vthetao)|_{\opR\mbox{ real}}
        &=    (\Real{\opW^H})(\Real{\setY})^{-1} \opR (\Real{\setY})^{-1}(\Real{\opW}) - 2(\Real{\opW^H})(\Real{\setY})^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    (\Real{\opW^H})\opR^{-1} \opR \opR^{-1}(\Real{\opW}) - 2(\Real{\opW^H})\opR^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    (\Real{\opW^H}) \opR^{-1}(\Real{\opW}) - 2(\Real{\opW^H})\opR^{-1}(\Real{\opW}) + \pE{\vy^H\vy}
      \\&=    \pE{\vy^H\vy} - (\Real{\opW^H})\opR^{-1}(\Real{\opW})
    \end{align*}
\end{enumerate}
\end{proof}

\begin{figure}[ht]
\centering%
\setlength{\unitlength}{0.15mm}
\begin{picture}(700,450)(-100,-300)
  \thicklines
  \put(-100,  10){\makebox ( 50,  0)[b]{$\vx$}               }
  \put(-100,   0){\line    (  1,  0)   { 50}               }
  \put(- 50,- 75){\line    (  0,  1)   {150}               }
  \put(- 50,  75){\vector  (  1,  0)   { 50}               }
  \put(- 50,- 75){\vector  (  1,  0)   { 50}               }
  \put(   0,  25){\framebox(100,100)[c]{process}           }

  \put(   0,-125){\framebox(100,100)[c]{}                  }
  \put(   0,-115){\makebox (100, 80)[t]{estimate} }
  \put(   0,-115){\makebox (100, 80)[c]{process} }
  %\put(   0,-115){\makebox (100, 80)[b]{$U$}            }

  \put(   0,-275){\framebox(100,100)[c]{estimate $\vtheta$}    }
  \put(  50,-175){\vector  (  0,  1)   { 50}               }
  \put(  60,-150){\makebox (  0,  0)[l]{$\estn=R^{-1}W$}           }

  \put( 110,  85){\makebox ( 50,  0)[lb]{$y$}               }
  \put( 110, -65){\makebox ( 50,  0)[lb]{$\hat{y}=\vx^T\vtheta$}               }
  \put( 100,  75){\vector  (  1,  0)   {150}               }
  \put( 100,- 75){\vector  (  1,  0)   {150}               }
  \put( 250,   0){\circle{40} }
  \put( 250,   0){\makebox(0,0)[c]{$+$} }
  \put( 250,  75){\vector  (  0, -1)   { 60}               }
  \put( 250, -75){\vector  (  0,  1)   { 60}               }

  \put( 300,  10){\makebox (100,  0)[b]{$e=\hat{y}-y$}      }
  \put( 270,   0){\vector  (  1,  0)   {80}               }
  \put( 350,   0){\vector  (  0, -1)   {225}               }
  \put( 350,-225){\vector  ( -1,  0)   {250}               }
\end{picture}
\caption{
   Adative filter example
   \label{fig:est_adapt}
   }
\end{figure}

In many adaptive filter and equalization applications,
the autocorrelation matrix $U$ is simply the $m$-element
random data vector $\vx(k)$ at time $k$, as in the \thme{Wiener-Hopf equations} (next).
%--------------------------------------
\begin{corollary}[\thmd{Wiener-Hopf equations}]
\footnote{
  \citerppgc{ifeachor1993}{547}{549}{020154413X}{\textsection ``9.3 Basic Wiener filter theory"},
  \citerppgc{ifeachor2002}{651}{654}{0201596199}{\textsection ``10.3 Basic Wiener filter theory"},
  \citerpgc{kay1988}{51}{9788131733561}{\textsection ``3.3.3 Random Parameters"}
  }
%--------------------------------------
\corbox{
  \brb{
   U \eqd \vx(k) \eqd
   \brs{\begin{array}{l}
      x(k) \\
      x(k-1) \\
      x(k-2) \\
      \vdots \\
      x(k-m+1)
   \end{array}}}
  \implies
  \brb{\begin{array}{rcl}
    \estMS                          &=& R^{-1}W  \\
    \fCost(\estMS)                  &=& W^T R^{-1} R R^{-1}W - 2W^T R^{-1}W + \pE{\vy^T\vy} \\
  \end{array}}
  }
\end{corollary}
\begin{proof}
This is a special case of the more general case discussed
in \prefpp{thm:est_mms}.
Here, the dimension of $U$ is $m\times1$ (n=1).
As a result,
$\vy$, $\vye$, and $\ve$ are simply scalar quantities (not vectors).
In this special case, we have the following results
\xref{fig:est_adapt}:

\begin{align*}
   \hat{y}(\vtheta)   &\eqd \vx^T \vtheta    \\
   e(\vtheta)    &\eqd \hat{y}-y \\
   \fCost(\vtheta) &\eqd \E\norm{\ve}^2 \eqd \pE\brs{e^2} \\
   \estMS      &\eqd \argmin_\vtheta \fCost(\vtheta)  \\
   R           &\eqd \pE\brs{\vx\vx^T}   \\
   W           &\eqd \pE\brs{ \vx y }    \\
    \fCost(\vtheta)                     &= \vtheta^T R \vtheta -2W^T\vtheta  + \pE\brs{\vy^T\vy} \\
    \grad_\vtheta \fCost(\vtheta)           &= 2R\vtheta - 2W  \\
    \fCost(\estMS)|_{R\mbox{ real}} &=    \pE{\vy^T\vy} - W^T R^{-1}W.
\end{align*}
\end{proof}

%======================================
\section{Least squares}
\label{sec:ls}
%======================================
%--------------------------------------
\begin{theorem}[\thmd{Least squares}]
\label{thm:ls}
\index{least squares}
%--------------------------------------
Let
\begin{align*}
   \vye(\vtheta)   &\eqd U^H\vtheta                      \\
   \ve(\vtheta)    &\eqd \vye-\vy                    \\
   \fCost(\vtheta) &\eqd \norm{\ve}^2 \eqd \ve^H\ve  \\
   \estLS      &\eqd \argmin_\vtheta \fCost(\vtheta)    \\
   R           &\eqd UU^H                        \\
   W           &\eqd U\vy.
\end{align*}

Then
\thmbox{\begin{array}{rcl}
   \estLS                          &=& (\Real{\setY})^{-1}(\Real{W})  \\
   \fCost(\vtheta)                     &=& \vtheta^H R \vtheta - (W^H\vtheta)^\ast -W^H\vtheta + \pE{\vy^H\vy} \\
   \grad_\vtheta \fCost(\vtheta)           &=& 2\Real\brs{\setY}\vtheta - 2\Real{W}  \\
   \fCost(\estLS)                  &=& (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \pE{\vy^H\vy} \\
   \fCost(\estLS)|_{R\mbox{ real}} &=& \pE{\vy^H\vy} - (\Real{W^H})R^{-1}(\Real{W}).
\end{array}}
\end{theorem}

\begin{proof}
See \prefpp{app:mc} for a Matrix Calculus reference.

\begin{align*}
   \fCost(\vtheta)
     &\eqd \norm{\ve}^2
   \\&=    e^H\ve
   \\&=    \left(\vye-\vy\right)^H\left(\vye-\vy\right)
   \\&=    \left(U^H\vtheta-\vy\right)^H\left(U^H\vtheta-\vy\right)
   \\&=    \left(\vtheta^HU-\vy^H\right)\left(U^H\vtheta-\vy\right)
   \\&=    \vtheta^HUU^H\vtheta - \vtheta^HU\vy -\vy^HU^H\vtheta + \vy^H\vy
   \\&=    \vtheta^H R \vtheta - (W^H\vtheta)^H -W^H\vtheta + \vy^H\vy
   \\&=    \vtheta^H R \vtheta - (W^H\vtheta)^\ast -W^H\vtheta + \vy^H\vy
   \\&=    \vtheta^H R \vtheta - (W^H)^\ast\vtheta -W^H\vtheta + \vy^H\vy
   \\&=    \vtheta^H R \vtheta - 2\Real\brs{W^H}\vtheta + \vy^H\vy
\\
\\
   \grad_\vtheta \fCost(\vtheta)
     &= \grad_\vtheta \left[ \vtheta^H R \vtheta - (W^H)^\ast\vtheta -W^H\vtheta + \vy^H\vy \right]
   \\&= R\vtheta + R^T\vtheta - [(W^H)^\ast]^T - [W^H]^T + 0
   \\&= R\vtheta + (R^H)^\ast\vtheta - W - W^\ast
   \\&= R\vtheta + R^\ast\vtheta - W - W^\ast
   \\&= (R + R^\ast)\vtheta - (W + W^\ast)
   \\&= 2(\Real{\setY})\vtheta - 2\Real{W}
\\
\\
   \vthetao
     &= (\Real{\setY})^{-1}(\Real{W})
\\
\\
   \fCost(\vthetao)
     &=    \vthetao^H R \vthetao - 2\Real\brs{W^H}\vthetao + \vy^H\vy
   \\&=    [(\Real{\setY})^{-1}(\Real{W})]^H R [(\Real{\setY})^{-1}(\Real{W})] - 2\Real\brs{W^H}[(\Real{\setY})^{-1}(\Real{W})] + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{\setY})^{-H} R (\Real{\setY})^{-1}(\Real{W})     - 2\Real\brs{W^H}(\Real{\setY})^{-1}(\Real{W})       + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{R^H})^{-1} R (\Real{\setY})^{-1}(\Real{W})   - 2\Real\brs{W^H}(\Real{\setY})^{-1}(\Real{W})     + \vy^H\vy
   \\&=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W})     - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W})      + \vy^H\vy
\\
\\
   \fCost(\vthetao)|_{R\mbox{ real}}
     &=    (\Real{W^H})(\Real{\setY})^{-1} R (\Real{\setY})^{-1}(\Real{W}) - 2(\Real{W^H})(\Real{\setY})^{-1}(\Real{W}) + \vy^H\vy
   \\&=    (\Real{W^H})R^{-1} R R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \vy^H\vy
   \\&=    (\Real{W^H}) R^{-1}(\Real{W}) - 2(\Real{W^H})R^{-1}(\Real{W}) + \vy^H\vy
   \\&=    \vy^H\vy - (\Real{W^H})R^{-1}(\Real{W})
\end{align*}
\end{proof}


%---------------------------------------
\begin{example}[Polynomial approximation]
\index{polynomial approximation}
\index{least squares}
\index{Vandermonde matrix}
%---------------------------------------
\mbox{}\\
Suppose we {\bf know} the locations
$\set{(x_n,y_n)}{n=1,2,3,4,5}$ of 5 data points.
Let $\vx$ and $\vy$ represent the locations of these points such that
\[
   \vx \eqd
   \left[\begin{array}{l}
      x_1  \\
      x_2  \\
      x_3  \\
      x_4  \\
      x_5
   \end{array}\right]
   \qquad\qquad
   \vy \eqd
   \left[\begin{array}{l}
      y_1  \\
      y_2  \\
      y_3  \\
      y_4  \\
      y_5
   \end{array}\right]
\]
Suppose we want to find a second order polynomial
  \[ c x^2 + bx + a \]
that best approximates
these 5 points in the least squares sense.
We define the matrix $U$ (known) and vector $\estn$ (to be computed)
as follows:
\[
   U^H \eqd
   \mcom{
   \left[\begin{array}{lll}
      1  & x_1 & x_1^2  \\
      1  & x_2 & x_2^2  \\
      1  & x_3 & x_3^2  \\
      1  & x_4 & x_4^2  \\
      1  & x_5 & x_5^2
   \end{array}\right]
   }{Vandermonde matrix \footnotemark}
   \addtocounter{footnote}{-1}\footnote{\citer{horn}{29}}
   \qquad\qquad
   \estn \eqd
   \left[\begin{array}{l}
      a  \\
      b  \\
      c  \\
   \end{array}\right]
\]
Then, using \prefpp{thm:ls}, the best coefficients $\estn$
for the polynomial are
\begin{align*}
  \estn
    &= \left[\begin{array}{l}
          a  \\
          b  \\
          c  \\
       \end{array}\right]
  \\&= R^{-1}W
  \\&= (UU^H)^{-1}\; (U\vy)
  \\&= \left(
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]^H
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]
       \right)^{-1}
       \left(
       \left[\begin{array}{lll}
          1  & x_1 & x_1^2  \\
          1  & x_2 & x_2^2  \\
          1  & x_3 & x_3^2  \\
          1  & x_4 & x_4^2  \\
          1  & x_5 & x_5^2
       \end{array}\right]^H
       \left[\begin{array}{l}
          y_1  \\
          y_2  \\
          y_3  \\
          y_4  \\
          y_5
       \end{array}\right]
       \right)
\end{align*}
\end{example}

%======================================
%\section{Gradient search techniques}
\section{Recursive forms}
%=======================================
One of the biggest advantages of using a recursive form / gradient search technique is
that it can be implemented \hie{recursively} as shown in the next equation.
The general form of the gradient search parameter estimation techniques is\footnote{\citerp{nelles2001}{90}}
\thmbox{
   \vtheta_n = \vtheta_{n-1} - \eta_{n-1} R \;\left[\gradxy{\vtheta}{\fCost(\vtheta_n)}\right]
}
where at time $n$

\begin{tabular}{lll}
   $\vtheta_n$      & is the \hie{state    }   & (vector)  \\
   $\eta_n$     & is the \hie{step size}   & (scalar)  \\
   $\setY$          & is the \hie{direction}   & (matrix)  \\
   $\gradxy{\vtheta}{\fCost(\vtheta_n)}$      & is the \hie{gradient } of the cost function $\fCost(\vtheta_n)$   & (vector)
\end{tabular}

Two major categories of gradient search techniques are
\begin{liste}
   \item steepest descent (includes LMS)
   \item Newton's method (includes RLS and Kalman filters).
\end{liste}

The key difference between the two is that
{\bf \hie{steepest descent} uses only first derivative information},
while
{\bf \hie{Newton's method} uses both first and second derivative information}
making it converge much faster but with significantly higher
complexity.

%======================================
\subsubsection*{First derivative techniques}
\label{sec:1st-deriv}
%======================================
\paragraph{Steepest descent.}
In this algorithm, $R=I$ (identity matrix).
First derivative information is contained in $\grad\fCost$.
Second derivative information, if present, is contained in $\setY$.
Thus, steepest descent algorithms do not use second derivative information.
\thmbox{
  \vtheta_n = \vtheta_{n-1} - \eta_{n-1} \;\left[ \gradxy{\vtheta}{\fCost(\vtheta_n)} \right]
}
\paragraph{Least Mean Squares (LMS).}\footnote{\citerpg{manolakis2000}{526}{0070400512}}
This is a special case of \hie{steepest descent}.
In minimum mean square estimation \xref{sec:est_mms},
the cost function $\fCost(\vtheta)$ is defined as a
\hie{statistical average} of the error vector such that
$\fCost(\vtheta) = \Eb{\ve^H\ve}$.
In this case the gradient $\grad\fCost$ is difficult to compute.
However, the LMS algorithm greatly simplifies the problem by
instead defining the cost function as a function of the
\hie{instantaneous error} such that
\begin{align*}
   \vy &= y(n)
\\
   \vye &= \hat{y}(n)
\\
   \fCost(\vtheta)
   &= \norm{e(n)}^2
 \\&= e^2(n)
 \\&= (\hat{y}(n)-y(n))^2
\end{align*}

Computing the gradient of this cost function is then
just a special case of \hie{least squares estimation} \xref{sec:ls}.
Using LS, we let $U=\vx^T$ and hence
\begin{align*}
   \gradxy{\vtheta}{\fCost(\vtheta)}
   &= 2U^TU \vtheta -2U^T\vy                   && \text{ by \prefp{thm:ls}}
\\ &= 2\vx\vx^T \vtheta -2\vx y               && \text{ by above definitions}
\\ &= 2\vx\hat{y} -2\vx y                    && \text{ }
\\ &= 2\vx(\hat{y} -y)                      && \text{ }
\\ &= 2\vx e(n)                && \text{ }
\end{align*}

The LMS algorithm uses this instantaneous gradient for $\grad\fCost$,
lets $R=I$, and uses a constant step size $\eta$ to give
\thmbox{
  \vtheta_n = \vtheta_{n-1} - 2\eta \vx_n e(n)
}
%--------------------------------------
\subsubsection*{Second derivative techniques}
%--------------------------------------
\paragraph{Newton's Method.}
This algorithm uses the \hie{Hessian} matrix $H$,
which is the second derivative of the cost function $\fCost(\vtheta)$,
and lets $R=H^{-1}$.
\begin{align*}
   H_n &\eqd& \grad_\vtheta\grad_\vtheta \fCost(\vtheta_n)
\\
   \vtheta_n &= \vtheta_{n-1} - \eta_{n-1} H_n^{-1} \left[\grad_\vtheta\fCost(\vtheta_n)\right]
\end{align*}


\paragraph{Kalman filtering}\footnote{\citerp{nelles2001}{66}}
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vthetae(k-1) \\
   \vthetae(k) &= \vthetae(k-1) + \gamma(k)e(k)
\end{align*}

\if 0
\paragraph{RLS with forgetting}\footnote{\citerp{nelles2001}{64}}
This algorithm introduces a forgetting factor $\lambda$
to help the algorithm track non-stationary channels.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+\lambda}  P(k-1)x(k) \\
   P(k) &= \frac{1}{\lambda}(I-\gamma(k)x^T(k))P(k-1)+V \\
   e(k) &= y(k) - x^T(k)\vthetae(k-1) \\
   \vthetae(k) &= \vthetae(k-1) + \gamma(k)e(k)
\end{align*}
\fi

\paragraph{Recursive Least Squares (RLS)}\footnote{\citerp{nelles2001}{66}}
This algorithm is a special case of either the RLS with forgetting
or the Kalman filter.
\begin{align*}
   \gamma(k) &= \frac{1}{x^T(k)P(k-1)\vx(k)+1}  P(k-1)x(k) \\
   P(k) &= (I-\gamma(k)x^T(k))P(k-1) \\
   e(k) &= y(k) - x^T(k)\vthetae(k-1) \\
   \vthetae(k) &= \vthetae(k-1) + \gamma(k)e(k)
\end{align*}




%--------------------------------------
\section{Direct search}
%--------------------------------------
A direct search algorithm may be used in cases where the cost
function over $\vtheta$ has several local minimums, making convergence difficult.
Furthermore, direct search algorithms can be very computationally demanding.

